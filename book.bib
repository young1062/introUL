@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.org/knitr/},
}

@article{isomap,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},
doi = {10.1126/science.290.5500.2319},
}

@article{lle,
author = {Sam T. Roweis  and Lawrence K. Saul },
title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
journal = {Science},
volume = {290},
number = {5500},
pages = {2323-2326},
year = {2000},
doi = {10.1126/science.290.5500.2323},
url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
}

@article{lle_survey,
author = {Chen, Jing and Liu, Yang},
year = {2011},
month = {06},
pages = {29-48},
title = {Locally linear embedding: A survey},
volume = {36},
journal = {Artif. Intell. Rev.},
doi = {10.1007/s10462-010-9200-z}
}

@article{think_globally,
author = {Saul, Lawrence K. and Roweis, Sam T.},
title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
year = {2003},
publisher = {JMLR.org},
volume = {4},
url = {https://doi.org/10.1162/153244304322972667},
doi = {10.1162/153244304322972667},
pages = {119–155},
numpages = {37}
}

@article{Cox_MDS,
author = {Saul, Lawrence K. and Roweis, Sam T.},
title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
year = {2003},
publisher = {JMLR.org},
volume = {4},
url = {https://doi.org/10.1162/153244304322972667},
doi = {10.1162/153244304322972667},
pages = {119–155},
numpages = {37}
}

@book{cox2000multidimensional,
  title={Multidimensional Scaling, Second Edition},
  author={Cox, T.F. and Cox, M.A.A.},
  isbn={9781420036121},
  lccn={00060180},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.com/books?id=SKZzmEZqvqkC},
  year={2000},
  publisher={CRC Press}
}

@article{Bernstein,
author = {Bernstein, Mira and Silva, Vin and Langford, John and Tenenbaum, Joshua},
year = {2001},
month = {02},
pages = {},
title = {Graph Approximations to Geodesics on Embedded Manifolds}
}


@article{llenoise,
title = {Locally linear embedding with additive noise},
journal = {Pattern Recognition Letters},
volume = {123},
pages = {47-52},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519300820}
}

@article{RobustLLE,
title = {Robust locally linear embedding},
journal = {Pattern Recognition},
volume = {39},
number = {6},
pages = {1053-1065},
year = {2006},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2005.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0031320305003353},
author = {Hong Chang and Dit-Yan Yeung}
}

@article{NN,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647}
}


@article{VAE,
  title={Auto-Encoding Variational Bayes},
  author={Diederik P. Kingma and Max Welling},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6114},
  url={https://api.semanticscholar.org/CorpusID:216078090}
}


@inproceedings{DenoiseAE,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and Composing Robust Features with Denoising Autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
doi = {10.1145/1390156.1390294},
pages = {1096–1103},
series = {ICML '08}
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@book{esl,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@book{izenman,
author = {Izenman, Alan Julian},
title = {Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning},
year = {2008},
isbn = {0387781889},
publisher = {Springer Publishing Company, Incorporated},
edition = {1}
}

@article{pca_pearson,
author = { Karl   Pearson   F.R.S. },
title = {LIII. On lines and planes of closest fit to systems of points in space},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
volume = {2},
number = {11},
pages = {559-572},
year  = {1901},
publisher = {Taylor & Francis},
doi = {10.1080/14786440109462720},
}

@article{pca_hotelling,
  title={Analysis of a complex of statistical variables into principal components.},
  author={Hotelling, Harold},
  journal={Journal of educational psychology},
  volume={24},
  number={6},
  pages={417},
  year={1933},
  publisher={Warwick \& York}
}

@book{trefethen97,
  added-at = {2010-09-19T02:35:23.000+0200},
  author = {Trefethen, Lloyd N. and Bau, David},
  biburl = {https://www.bibsonomy.org/bibtex/2e45a2ed5ccc6dc12721cde613217c222/ytyoun},
  interhash = {1e7e7a44cbff3092be50a71fe056c8ec},
  intrahash = {e45a2ed5ccc6dc12721cde613217c222},
  isbn = {0898713617},
  keywords = {characteristic eigenvalues linear.algebra matrix numerical numerical.analysis polynomial secular.equation textbook},
  publisher = {SIAM},
  timestamp = {2017-11-25T07:18:16.000+0100},
  title = {Numerical Linear Algebra},
  year = 1997
}

@book{strang,
  added-at = {2014-09-12T13:40:38.000+0200},
  address = {Belmont, CA},
  author = {Strang, Gilbert},
  biburl = {https://www.bibsonomy.org/bibtex/2928ae987a1afe04daac3c76587856389/alex_ruff},
  description = {Linear Algebra and Its Applications, 4th Edition: Gilbert Strang: 9780030105678: Amazon.com: Books},
  interhash = {89d8f10be9cdd770dd9639ac4d469e97},
  intrahash = {928ae987a1afe04daac3c76587856389},
  isbn = {0030105676 9780030105678 0534422004 9780534422004},
  keywords = {book math to_READ},
  publisher = {Thomson, Brooks/Cole},
  refid = {61231077},
  timestamp = {2014-09-12T13:40:38.000+0200},
  title = {Linear algebra and its applications},
  url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
  year = 2006
}



@article{lle,
author = {Sam T. Roweis  and Lawrence K. Saul },
title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
journal = {Science},
volume = {290},
number = {5500},
pages = {2323-2326},
year = {2000},
doi = {10.1126/science.290.5500.2323},
url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
}

@article{lle_survey,
author = {Chen, Jing and Liu, Yang},
year = {2011},
month = {06},
pages = {29-48},
title = {Locally linear embedding: A survey},
volume = {36},
journal = {Artif. Intell. Rev.},
doi = {10.1007/s10462-010-9200-z}
}

@article{think_globally,
author = {Saul, Lawrence K. and Roweis, Sam T.},
title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
year = {2003},
publisher = {JMLR.org},
volume = {4},
url = {https://doi.org/10.1162/153244304322972667},
doi = {10.1162/153244304322972667},
pages = {119–155},
numpages = {37}
}

@article{Cox_MDS,
author = {Saul, Lawrence K. and Roweis, Sam T.},
title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
year = {2003},
publisher = {JMLR.org},
volume = {4},
url = {https://doi.org/10.1162/153244304322972667},
doi = {10.1162/153244304322972667},
pages = {119–155},
numpages = {37}
}

@book{cox2000multidimensional,
  title={Multidimensional Scaling, Second Edition},
  author={Cox, T.F. and Cox, M.A.A.},
  isbn={9781420036121},
  lccn={00060180},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.com/books?id=SKZzmEZqvqkC},
  year={2000},
  publisher={CRC Press}
}

@article{Bernstein,
author = {Bernstein, Mira and Silva, Vin and Langford, John and Tenenbaum, Joshua},
year = {2001},
month = {02},
pages = {},
title = {Graph Approximations to Geodesics on Embedded Manifolds}
}


@article{llenoise,
title = {Locally linear embedding with additive noise},
journal = {Pattern Recognition Letters},
volume = {123},
pages = {47-52},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167865519300820}
}

@article{RobustLLE,
title = {Robust locally linear embedding},
journal = {Pattern Recognition},
volume = {39},
number = {6},
pages = {1053-1065},
year = {2006},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2005.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0031320305003353},
author = {Hong Chang and Dit-Yan Yeung}
}

@inproceedings{lee_seung,
 author = {Lee, Daniel and Seung, H. Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Algorithms for Non-negative Matrix Factorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf},
 volume = {13},
 year = {2000}
}

@article{kPCA_tuning,
author = {Alam, Md Ashad and Fukumizu, Kenji},
year = {2014},
month = {01},
pages = {1139-1150},
title = {Hyperparameter selection in kernel principal component analysis},
volume = {10},
journal = {Journal of Computer Science},
doi = {10.3844/jcssp.2014.1139.1150}
}

@inproceedings{kPCA_tuning2,
 author = {Mika, Sebastian and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert and Scholz, Matthias and R\"{a}tsch, Gunnar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Kearns and S. Solla and D. Cohn},
 pages = {},
 publisher = {MIT Press},
 title = {Kernel PCA and De-Noising in Feature Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf},
 volume = {11},
 year = {1998}
}


@inproceedings{laplac_eigen,
 author = {Belkin, Mikhail and Niyogi, Partha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
 volume = {14},
 year = {2001}
}

@article{ScreeNot,
author = {David Donoho and Matan Gavish and Elad Romanov},
title = {{ScreeNOT: Exact MSE-optimal singular value thresholding in correlated noise}},
volume = {51},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {122 -- 148},
keywords = {high-dimensional asymptotics, low-rank matrix denoising, optimal threshold, scree plot, singular value thresholding},
year = {2023},
doi = {10.1214/22-AOS2232},
URL = {https://doi.org/10.1214/22-AOS2232}
}


@article{scree_og,
author = {Raymond B. Cattell},
title = {The Scree Test For The Number Of Factors},
journal = {Multivariate Behavioral Research},
volume = {1},
number = {2},
pages = {245--276},
year = {1966},
publisher = {Routledge},
doi = {10.1207/s15327906mbr0102\_10},

    note ={PMID: 26828106},


URL = { 
    
        https://doi.org/10.1207/s15327906mbr0102_10
    
    

},
eprint = { 
    
        https://doi.org/10.1207/s15327906mbr0102_10
    
    

}

}

@article{arch,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1269949},
 abstract = {Archetypal analysis represents each individual in a data set as a mixture of individuals of pure type or archetypes. The archetypes themselves are restricted to being mixtures of the individuals in the data set. Archetypes are selected by minimizing the squared error in representing each individual as a mixture of archetypes. The usefulness of archetypal analysis is illustrated on several data sets. Computing the archetypes is a nonlinear least squares problem, which is solved using an alternating minimizing algorithm.},
 author = {Adele Cutler and Leo Breiman},
 journal = {Technometrics},
 number = {4},
 pages = {338--347},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Archetypal Analysis},
 urldate = {2024-09-25},
 volume = {36},
 year = {1994}
}

@ARTICLE{is_div,
  author={Févotte, Cédric and Bertin, Nancy and Durrieu, Jean-Louis},
  journal={Neural Computation}, 
  title={Nonnegative Matrix Factorization with the Itakura-Saito Divergence: With Application to Music Analysis}, 
  year={2009},
  volume={21},
  number={3},
  pages={793-830},
  keywords={},
  doi={10.1162/neco.2008.04-08-771}}
  
@article{vrnmf1,
author = {Lin, Chia-Hsiang and Ma, Wing-Kin and Li, Wei-Chiang and Chi, Chong-Yung and Ambikapathi, Arulmurugan},
year = {2014},
month = {06},
pages = {},
title = {Identifiability of the Simplex Volume Minimization Criterion for Blind Hyperspectral Unmixing: The No Pure-Pixel Case},
volume = {53},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
doi = {10.1109/TGRS.2015.2424719}
}

@article{vrnmf2,
author = {Fu, Xiao and Ma, Wing-Kin and Huang, Kejun and Sidiropoulos, N.D.},
year = {2015},
month = {05},
pages = {1-1},
title = {Blind Separation of Quasi-Stationary Sources: Exploiting Convex Geometry in Covariance Domain},
volume = {63},
journal = {IEEE Transactions on Signal Processing},
doi = {10.1109/TSP.2015.2404577}
}

@article{roux_comparative_2018,
	title = {A Comparative Study of Divisive and Agglomerative Hierarchical Clustering Algorithms},
	volume = {35},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/s00357-018-9259-9},
	doi = {10.1007/s00357-018-9259-9},
	abstract = {A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps: first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the node levels of the resulting dendrogram. A set of 12 such algorithms is presented and compared to their agglomerative counterpart (when available). These algorithms are evaluated using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated with the given dissimilarities. Applied to a hundred random data tables and to three real life examples, these comparisons are in favor of methods which are based on unusual ratio-type formulas to evaluate the intermediate bipartitions, namely the Silhouette formula, the Dunn's formula and the Mollineda et al. formula. These formulas take into account both the within cluster and the between cluster mean dissimilarities. Their use in divisive algorithms performs very well and slightly better than in their agglomerative counterpart.},
	pages = {345--366},
	number = {2},
	journaltitle = {Journal of Classification},
	shortjournal = {Journal of Classification},
	author = {Roux, Maurice},
	date = {2018-07-01},
}

@article{szekely2005hierarchical,
  title={Hierarchical clustering via joint between-within distances: Extending Ward's minimum variance method},
  author={Szekely, Gabor J and Rizzo, Maria L and others},
  journal={Journal of classification},
  volume={22},
  number={2},
  pages={151--184},
  year={2005},
  publisher={New York: Published for the Classification Society of North America by~…}
}

@book{EverittBrianS2001Ca,
abstract = {Cluster analysis comprises a range of methods of classifying multivariate data into subgroups and these techniques are widely applicable. This new edition incorporates material covering developing areas such as Bayesian statistics & neural networks.},
publisher = {Arnold ; Oxford University Press},
isbn = {0340761199},
year = {2001},
title = {Cluster analysis},
edition = {4th ed.},
language = {eng},
address = {London : New York},
author = {Everitt, Brian S},
keywords = {Cluster analysis},
}

@article{mojena,
    author = {Mojena, R.},
    title = "{Hierarchical grouping methods and stopping rules: an evaluation*}",
    journal = {The Computer Journal},
    volume = {20},
    number = {4},
    pages = {359-363},
    year = {1977},
    month = {01},
    abstract = "{Hierarchical clustering procedures have received a great deal of emphasis in recent years, yet research has lagged in their empirical evaluation and in objective means to aid the user in selecting good partitions (rather than good hierarchies). The present paper aims to correct both of these deficiencies, first by empirically testing selected methods which have become popular and, second by proposing and evaluating statistical stopping rules. Results indicate: 1. that methods vary widely in their performance and 2. that the proposed stopping rules can aid the user in selecting partitions.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/20.4.359},
    url = {https://doi.org/10.1093/comjnl/20.4.359},
    eprint = {https://academic.oup.com/comjnl/article-pdf/20/4/359/1108679/200359.pdf},
}

@INPROCEEDINGS{nnsvd,
  author={Weixiang Liu and Aifa Tang and Datian Ye and Zhen Ji},
  booktitle={2008 International Conference on Information Technology and Applications in Biomedicine}, 
  title={Nonnegative singular value decomposition for microarray data analysis of spermatogenesis}, 
  year={2008},
  volume={},
  number={},
  pages={225-228},
  keywords={Singular value decomposition;Data analysis;Matrix decomposition;Biomedical engineering;Sorting;Gene expression;Information technology;Data engineering;Biomedical computing;Pattern analysis},
  doi={10.1109/ITAB.2008.4570528}
}

@article{nmf_cophentic_corr,
author = {Jean-Philippe Brunet  and Pablo Tamayo  and Todd R. Golub  and Jill P. Mesirov },
title = {Metagenes and molecular pattern discovery using matrix factorization},
journal = {Proceedings of the National Academy of Sciences},
volume = {101},
number = {12},
pages = {4164-4169},
year = {2004},
doi = {10.1073/pnas.0308531101},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0308531101},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0308531101},
abstract = {We describe here the use of nonnegative matrix factorization (NMF), an algorithm based on decomposition by parts that can reduce the dimension of expression data from thousands of genes to a handful of metagenes. Coupled with a model selection mechanism, adapted to work for any stochastic clustering algorithm, NMF is an efficient method for identification of distinct molecular patterns and provides a powerful method for class discovery. We demonstrate the ability of NMF to recover meaningful biological information from cancer-related microarray data. NMF appears to have advantages over other methods such as hierarchical clustering or self-organizing maps. We found it less sensitive to a priori selection of genes or initial conditions and able to detect alternative or context-dependent patterns of gene expression in complex biological systems. This ability, similar to semantic polysemy in text, provides a general method for robust molecular pattern discovery.}
}

@article{hlles,
author = {David L. Donoho  and Carrie Grimes },
title = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data},
journal = {Proceedings of the National Academy of Sciences},
volume = {100},
number = {10},
pages = {5591-5596},
year = {2003},
doi = {10.1073/pnas.1031596100},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1031596100},
}

@article{dim_est1,
author = {Campadelli, P. and Casiraghi, E. and Ceruti, C. and Rozza, A.},
title = {Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework},
journal = {Mathematical Problems in Engineering},
volume = {2015},
number = {1},
pages = {759567},
doi = {https://doi.org/10.1155/2015/759567},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2015/759567},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2015/759567},
abstract = {When dealing with datasets comprising high-dimensional points, it is usually advantageous to discover some data structure. A fundamental information needed to this aim is the minimum number of parameters required to describe the data while minimizing the information loss. This number, usually called intrinsic dimension, can be interpreted as the dimension of the manifold from which the input data are supposed to be drawn. Due to its usefulness in many theoretical and practical problems, in the last decades the concept of intrinsic dimension has gained considerable attention in the scientific community, motivating the large number of intrinsic dimensionality estimators proposed in the literature. However, the problem is still open since most techniques cannot efficiently deal with datasets drawn from manifolds of high intrinsic dimension and nonlinearly embedded in higher dimensional spaces. This paper surveys some of the most interesting, widespread used, and advanced state-of-the-art methodologies. Unfortunately, since no benchmark database exists in this research field, an objective comparison among different techniques is not possible. Consequently, we suggest a benchmark framework and apply it to comparatively evaluate relevant state-of-the-art estimators.},
year = {2015}
}

@article{Dombowsky2025,
abstract = {Bayesian clustering typically relies on mixture models, with each component interpreted as a different cluster. After defining a prior for the component parameters and weights, Markov chain Monte Carlo (MCMC) algorithms are commonly used to produce samples from the posterior distribution of the component labels. The data are then clustered by minimizing the expectation of a clustering loss function that favors similarity to the component labels. Unfortunately, although these approaches are routinely implemented, clustering results are highly sensitive to kernel misspecification. For example, if Gaussian kernels are used but the true density of data within a cluster is even slightly non-Gaussian, then clusters will be broken into multiple Gaussian components. To address this problem, we develop Fusing of Localized Densities (FOLD), a novel clustering method that melds components together using the posterior of the kernels. FOLD has a fully Bayesian decision theoretic justification, naturally leads to uncertainty quantification, can be easily implemented as an add-on to MCMC algorithms for mixtures, and favors a small number of distinct clusters. We provide theoretical support for FOLD including clustering optimality under kernel misspecification. In simulated experiments and real data, FOLD outperforms competitors by minimizing the number of clusters while inferring meaningful group structure. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
archivePrefix = {arXiv},
arxivId = {2304.00074},
author = {Dombowsky, Alexander and Dunson, David B.},
doi = {10.1080/01621459.2024.2427935;REQUESTEDJOURNAL:JOURNAL:UASA20;WGROUP:STRING:PUBLICATION},
eprint = {2304.00074},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Decision theory,Kernel misspecification,Loss function,Mixture model,Statistical distance},
month = {dec},
publisher = {Taylor and Francis Ltd.},
title = {{Bayesian Clustering via Fusing of Localized Densities}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2024.2427935},
year = {2025}
}
@article{Karlis2009,
abstract = {The majority of the existing literature on model-based clustering deals with symmetric components. In some cases, especially when dealing with skewed subpopulations, the estimate of the number of groups can be misleading; if symmetric components are assumed we need more than one component to describe an asymmetric group. Existing mixture models, based on multivariate normal distributions and multivariate t distributions, try to fit symmetric distributions, i.e. they fit symmetric clusters. In the present paper, we propose the use of finite mixtures of the normal inverse Gaussian distribution (and its multivariate extensions). Such finite mixture models start from a density that allows for skewness and fat tails, generalize the existing models, are tractable and have desirable properties. We examine both the univariate case, to gain insight, and the multivariate case, which is more useful in real applications. EM type algorithms are described for fitting the models. Real data examples are used to demonstrate the potential of the new model in comparison with existing ones. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
author = {Karlis, Dimitris and Santourian, Anais},
doi = {10.1007/S11222-008-9072-0/METRICS},
file = {:Users/alexyoung/Library/Application Support/Mendeley Desktop/Downloaded/Karlis, Santourian - 2009 - Model-based clustering with non-elliptically contoured distributions.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {EM-algorithm,Heavy tailed distributions,Normal inverse Gaussian distribution,Scale normal mixtures},
month = {mar},
number = {1},
pages = {73--83},
publisher = {Springer},
title = {{Model-based clustering with non-elliptically contoured distributions}},
url = {https://link.springer.com/article/10.1007/s11222-008-9072-0},
volume = {19},
year = {2009}
}

@article{OHagan2016,
abstract = {Many model-based clustering methods are based on a finite Gaussian mixture model. The Gaussian mixture model implies that the data scatter within each group is elliptically shaped. Hence non-elliptical groups are often modeled by more than one component, resulting in model over-fitting. An alternative is to use a mean-variance mixture of multivariate normal distributions with an inverse Gaussian mixing distribution (MNIG) in place of the Gaussian distribution, to yield a more flexible family of distributions. Under this model the component distributions may be skewed and have fatter tails than the Gaussian distribution. The MNIG based approach is extended to include a broad range of eigendecomposed covariance structures. Furthermore, MNIG models where the other distributional parameters are constrained is considered. The Bayesian Information Criterion is used to identify the optimal model and number of mixture components. The method is demonstrated on three sample data sets and a novel variation on the univariate Kolmogorov-Smirnov test is used to assess goodness of fit.},
author = {O'Hagan, Adrian and Murphy, Thomas Brendan and Gormley, Isobel Claire and McNicholas, Paul D. and Karlis, Dimitris},
doi = {10.1016/J.CSDA.2014.09.006},
file = {:Users/alexyoung/Library/Application Support/Mendeley Desktop/Downloaded/O'Hagan et al. - 2016 - Clustering with the multivariate normal inverse Gaussian distribution.pdf:pdf},
issn = {0167-9473},
journal = {Computational Statistics & Data Analysis},
keywords = {Information metrics,Kolmogorov-Smirnov goodness of fit,Model-based clustering,Multivariate normal inverse Gaussian distribution,mclust},
month = {jan},
pages = {18--30},
publisher = {North-Holland},
title = {{Clustering with the multivariate normal inverse Gaussian distribution}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S0167947314002667},
volume = {93},
year = {2016}
}
@article{Dang2023,
abstract = {Families of mixtures of multivariate power exponential (MPE) distributions have already been introduced and shown to be competitive for cluster analysis in comparison to other mixtures of elliptical distributions, including mixtures of Gaussian distributions. A family of mixtures of multivariate skewed power exponential distributions is proposed that combines the flexibility of the MPE distribution with the ability to model skewness. These mixtures are more robust to variations from normality and can account for skewness, varying tail weight, and peakedness of data. A generalized expectation-maximization approach, which combines minorization-maximization and optimization based on accelerated line search algorithms on the Stiefel manifold, is used for parameter estimation. These mixtures are implemented both in the unsupervised and semi-supervised classification frameworks. Both simulated and real data are used for illustration and comparison to other mixture families.},
archivePrefix = {arXiv},
arxivId = {1907.01938},
author = {Dang, Utkarsh J. and Gallaugher, Michael P.B. and Browne, Ryan P. and McNicholas, Paul D.},
doi = {10.1007/S00357-022-09427-7/TABLES/5},
eprint = {1907.01938},
file = {:Users/alexyoung/Library/Application Support/Mendeley Desktop/Downloaded/Dang et al. - 2023 - Model-Based Clustering and Classification Using Mixtures of Multivariate Skewed Power Exponential Distributions.pdf:pdf},
issn = {14321343},
journal = {Journal of Classification},
keywords = {Generalized expectation-maximization algorithm,Mixture models,Model-based classification,Model-based clustering,Multivariate skewed power exponential distribution},
month = {apr},
number = {1},
pages = {145--167},
publisher = {Springer},
title = {{Model-Based Clustering and Classification Using Mixtures of Multivariate Skewed Power Exponential Distributions}},
url = {https://link.springer.com/article/10.1007/s00357-022-09427-7},
volume = {40},
year = {2023}
}

@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/J.2517-6161.1977.TB01600.X},
file = {:Users/alexyoung/Library/Application Support/Mendeley Desktop/Downloaded/Dempster, Laird, Rubin - 1977 - Maximum Likelihood from Incomplete Data Via the EM Algorithm.pdf:pdf},
isbn = {0{\textperiodcentered}000000779},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
keywords = {EM ALGORITHM,INCOMPLETE DATA,MAXIMUM LIKELIHOOD,POSTERIOR MODE},
month = {sep},
number = {1},
pages = {1--22},
publisher = {Oxford Academic},
title = {{Maximum Likelihood from Incomplete Data Via the EM Algorithm}},
url = {https://dx.doi.org/10.1111/j.2517-6161.1977.tb01600.x},
volume = {39},
year = {1977}
}

@article{banfield1993,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532201},
 abstract = {The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of Friedman and Rubin (1967, Journal of the American Statistical Association 62, 1159-1178). However, as currently implemented, it does not allow the specification of which features (orientation, size, and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow for noise. We propose ways of overcoming these limitations. A reparameterization of the covariance matrix allows us to specify that some, but not all, features be the same for all clusters. A practical framework for non-Gaussian clustering is outlined, and a means of incorporating noise in the form of a Poisson process is described. An approximate Bayesian method for choosing the number of clusters is given. The performance of the proposed methods is studied by simulation, with encouraging results. The methods are applied to the analysis of a data set arising in the study of diabetes, and the results seem better than those of previous analyses. A magnetic resonance image (MRI) of the brain is also analyzed, and the methods appear successful in extracting the main features of anatomical interest. The methods described here have been implemented in both Fortran and S-PLUS versions, and the software is freely available through StatLib.},
 author = {Jeffrey D. Banfield and Adrian E. Raftery},
 journal = {Biometrics},
 number = {3},
 pages = {803--821},
 publisher = {International Biometric Society},
 title = {Model-Based Gaussian and Non-Gaussian Clustering},
 urldate = {2025-08-28},
 volume = {49},
 year = {1993}
}

@book{spivak,
author = {Spivak, M. D.},
year = {1979},
title = {A comprehensive introduction to differential geometry},
publisher = {Publish or Perish, Inc.}
}

@book{jm_lee,
author = {Lee, J. M.},
year = {2019},
title = {Introduction to Riemannian Manifolds (Second edition)}, 
vol = {176},
publisher = {Springer Nature},
doi = {https://doi.org/10.1007/978-3-319-91755-9}
}

@inproceedings{multiscale_svd,
author = {Little, Anna and Lee, Jason and Jung, Yoon-Mo and Maggioni, Mauro},
year = {2009},
month = {10},
pages = {85 - 88},
title = {Estimation of intrinsic dimensionality of samples from noisy low-dimensional manifolds in high dimensions with multiscale SVD},
journal = {IEEE Workshop on Statistical Signal Processing Proceedings},
doi = {10.1109/SSP.2009.5278634}
}

@inproceedings{adam_sgd,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}


