<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-svd.html"/>
<link rel="next" href="sec-mds.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.4/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
<li class="chapter" data-level="2.4.7" data-path="linear-algebra.html"><a href="linear-algebra.html#hadamard-elementwise-operations"><i class="fa fa-check"></i><b>2.4.7</b> Hadamard (elementwise) operations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="exercises.html"><a href="exercises.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="exercises.html"><a href="exercises.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="exercises.html"><a href="exercises.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="exercises.html"><a href="exercises.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pca.html"><a href="sec-pca.html#pca-in-practice"><i class="fa fa-check"></i><b>4.1.2</b> PCA in Practice</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding an NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
<li class="chapter" data-level="4.3.6" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-and-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.3.6</b> NMF and Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernel-pca.html"><a href="kernel-pca.html"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-on-a-manifold.html"><a href="data-on-a-manifold.html"><i class="fa fa-check"></i><b>6.1</b> Data on a manifold</a></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html"><i class="fa fa-check"></i><b>6.4</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="hessian-eigenmaps-hlles.html"><a href="hessian-eigenmaps-hlles.html"><i class="fa fa-check"></i><b>6.5</b> Hessian Eigenmaps (HLLEs)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="hessian-eigenmaps-hlles.html"><a href="hessian-eigenmaps-hlles.html#introduction-2"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="hessian-eigenmaps-hlles.html"><a href="hessian-eigenmaps-hlles.html#algorithm-3"><i class="fa fa-check"></i><b>6.5.2</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.6</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-3"><i class="fa fa-check"></i><b>6.6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.6.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-4"><i class="fa fa-check"></i><b>6.6.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.6.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.6.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.7</b> Additional methods</a></li>
<li class="chapter" data-level="6.8" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="center-based-clustering.html"><a href="center-based-clustering.html"><i class="fa fa-check"></i><b>7.1</b> Center-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="center-based-clustering.html"><a href="center-based-clustering.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="center-based-clustering.html"><a href="center-based-clustering.html#k-center-and-k-medoids"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(k\)</span>-center and <span class="math inline">\(k\)</span>-medoids</a></li>
<li class="chapter" data-level="7.1.3" data-path="center-based-clustering.html"><a href="center-based-clustering.html#minimizing-clustering-loss-functions"><i class="fa fa-check"></i><b>7.1.3</b> Minimizing clustering loss functions</a></li>
<li class="chapter" data-level="7.1.4" data-path="center-based-clustering.html"><a href="center-based-clustering.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>7.1.4</b> Strengths and Weaknesses</a></li>
<li class="chapter" data-level="7.1.5" data-path="center-based-clustering.html"><a href="center-based-clustering.html#choosing-the-number-of-cluster-k"><i class="fa fa-check"></i><b>7.1.5</b> Choosing the number of cluster <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#dendrograms"><i class="fa fa-check"></i><b>7.2.1</b> Dendrograms</a></li>
<li class="chapter" data-level="7.2.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#building-a-dendrogram"><i class="fa fa-check"></i><b>7.2.2</b> Building a dendrogram</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonnegative-matrix-factorization" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Nonnegative Matrix Factorization<a href="nonnegative-matrix-factorization.html#nonnegative-matrix-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In both PCA and SVD, we learn data-drive orthonormal feature vectors which we can use to decompose our data in an orderly fashion. Nonnegative matrix factorization (NMF) is again focused on learning a set latent vectors which can be used to approximate our data. However, we will add a few restrictions motivated by experimental data and a desire to increase interpretability of the results which will drastically alter the results.</p>
<p>For NMF, we focus on cases where <span class="math inline">\({\bf X}\)</span> is an <span class="math inline">\(N\times d\)</span> data matrix with the added condition that its entries are nonnegative. Notationally, we write <span class="math inline">\({\bf X}\in\mathbb{R}_{\ge 0}^{N\times d}\)</span> to indicate it is composed of nonnegative real values. The nonnegativity condition is a natural constraint for many experimental data sets, but we are also going to impose a similar constraint on our feature vectors and coefficients. More specifically, for a specific rank <span class="math inline">\(k\)</span>, we seek a coefficient matrix <span class="math inline">\({\bf W}\in\mathbb{R}_{\ge 0}^{N\times k}\)</span> and feature matrix <span class="math inline">\({\bf H}\in \mathbb{R}^{k\times d}_{\ge 0}\)</span> such that <span class="math inline">\({\bf WH}\)</span> is as close to <span class="math inline">\({\bf X}\)</span> as possible. The nonnegativity constraint on the elements of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> implies that <span class="math inline">\({\bf WH}\in\mathbb{R}_{\ge 0}^{N\times d}\)</span>. There are many different measures of proximity that one may use in NMF which are greater than zero and equal to zero if and only if <span class="math inline">\({\bf X}={\bf WH}\)</span>. The most common measures are</p>
<ul>
<li><p>Frobenius norm: <span class="math inline">\(\|{\bf X}-{\bf WH}\|_F.\)</span></p></li>
<li><p>Divergence: <span class="math inline">\(D({\bf X} \| {\bf WH}) = \sum_{i=1}^N\sum_{j=1}^d \left[{\bf X}_{ij} \log \frac{{\bf X}_{ij}}{({\bf WH})_{ij}} + ({\bf WH})_{ij} - {\bf X}_{ij}\right]\)</span></p></li>
<li><p>Itakura-Saito Divergence: <span class="math display">\[D_{IS}({\bf X}, {\bf WH}) = \sum_{i=1}^N\sum_{j=1}^d \left[\frac{{\bf X}_{ij}}{({\bf WH})_{ij}} - \log\frac{{\bf X}_{ij}}{({\bf WH})_{ij}} -1  \right]\]</span></p></li>
</ul>
<p>These loss functions emphasize and prioritize different features of the data and are often coupled with additional penalties or assumptions on the elements of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which we will discuss at the end of the section.</p>
<p>For now, let us focus on the primary motivation of NMF, which is to create more interpretable feature vectors (the rows of <span class="math inline">\({\bf H}\)</span>).</p>
<div id="interpretability-superpositions-and-positive-spans" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Interpretability, Superpositions, and Positive Spans<a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the case of SVD where we can approximate a given vector <span class="math inline">\(\vec{x}_i\)</span> using the first <span class="math inline">\(k\)</span> right singular vectors as
<span class="math display">\[\vec{x}_i \approx \sum_{j=1}^k \sigma_j {\bf U}_{ij} \vec{v}_j.\]</span> Let us restrict ourselves to the case of nonnegative entries. Suppose for the moment that the <span class="math inline">\(\ell\)</span>th coordinate of <span class="math inline">\(\vec{x}_i\)</span> is (very close to) zero and this is reflected by our approximation as well so that <span class="math display">\[(\vec{x}_i)_\ell \approx \sum_{j=1}^k \sigma_j {\bf U}_{ij} (\vec{v}_j)_\ell \approx 0.\]</span>
The freedom of the coefficients <span class="math inline">\(\sigma_j {\bf U}_{ij}\)</span> and the features <span class="math inline">\((\vec{v}_j)_\ell\)</span> for <span class="math inline">\(j=1,\dots,k\)</span> to take any value in <span class="math inline">\(\mathbb{R}\)</span> prevents us from concluding that there is a comparable <code>near-zeroness</code> in the features or coefficients. In could be that case that <span class="math inline">\({\bf U}_{ij} (\vec{v}_j)_\ell\)</span> is near zero for all <span class="math inline">\(j=1,\dots,k\)</span> or that some subset are large and positive but are canceled out by a different subset that is large and negative. If, however, we restrict the coefficients and features to be zero this cancellation effect cannot occur. Features can only then be added but never subtracted. Under this restriction, <span class="math inline">\(\sum_{j=1}^k \sigma_j {\bf U}_{ij} (\vec{v}_j)_\ell\)</span> will only be close to zero if the coefficients are (near) zero for any feature vector which has a (large) positive entry in its <span class="math inline">\(\ell\)</span>th coordinate.</p>
<p>Thus, in a factorization of the form <span class="math display">\[\vec{x}_i \approx \sum_{j=1}^k \underbrace{{\bf W}_{ij}}_{\ge 0} \underbrace{\vec{h}_j}_{\in \mathbb{R}^d_{\ge}},\]</span>
we can only superimpose (add) features to approximate our data, we might expect the features themselves to look more like our data. In matrix notation, we have <span class="math display">\[{\bf X} = {\bf WH}\]</span> where the coefficients for each sample are stored in the rows of <span class="math inline">\({\bf W}\in\mathbb{R}^{N\times k}_{\ge 0}\)</span> and the features are transposed and listed as the rows of <span class="math inline">\({\bf H}\in\mathbb{R}^{k\times d}\)</span>.</p>
</div>
<div id="geometric-interpretation" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Geometric Interpretation<a href="nonnegative-matrix-factorization.html#geometric-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
As a motivating example consider the case, where we have data <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\in\mathbb{R}^3_{\ge 0}\)</span> for which there is a exact decomposition <span class="math display">\[{\bf X} = {\bf WH}\]</span> which is to say there are nonnegative coefficients, <span class="math inline">\({\bf W}\)</span> and feature vectors <span class="math inline">\(\vec{h}_1,\vec{h}_2 \in \mathbb{R}_{\ge 0}^3\)</span> such that <span class="math display">\[\vec{x}_i = {\bf W}_{i1}\vec{h}_1 + {\bf W}_{i2}\vec{h}_2 \qquad \text{ for } i =1,\dots,N.\]</span> The nonnegativity condition on data implies that <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\)</span> reside in the positive orthant of <span class="math inline">\(\mathbb{R}^3.\)</span> The exact decomposition assumptions implies <span class="math inline">\(\{\vec{x}_1,\dots,\vec{x}_N\} \in \text{span}\{\vec{h}_1,\vec{h}_2\}\)</span> and furthermore the following more restricted picture holds.
<div class="figure"><span style="display:block;" id="fig:nmf-ex"></span>
<div class="plotly html-widget html-fill-item" id="htmlwidget-061781ecd9e59372869a" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-061781ecd9e59372869a">{"x":{"visdat":{"f02e4fa37361":["function () ","plotlyVisDat"],"f02e73ec0e1f":["function () ","data"]},"cur_data":"f02e73ec0e1f","attrs":{"f02e4fa37361":{"x":{},"y":{},"z":{},"name":"1st Feature","color":["red"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","inherit":true},"f02e73ec0e1f":{"x":{},"y":{},"z":{},"name":"2nd Feature","color":["black"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","inherit":true},"f02e73ec0e1f.1":{"x":[2.2458658555579856,1.4189738247191614,1.7947618282360722,5.1982379050267369,2.0566012901386608,3.4791374787258706,4.7682105035254327,2.4287171183528118,2.3169614643758334,2.8031041612770111,2.8896561609509739,3.3078936796914187,6.0042957908457781,2.6174916620155293,2.8501389193892188,1.9878637902679244,2.209855144722614,3.2323477110359473,2.6457632141130114,3.26479292303766,6.0868278327631105,2.3163721194278879,3.3554910312848918,2.0414525752754398,2.3184927014872896,3.3620000824413538,2.9826122254189773,2.4500607875818154,2.2536290982377833,4.8432302398622076,2.5565641188261345,2.9351195189903487,2.8259701876054271,1.9928345766282269,3.2261986650171695,4.3457550048270663,1.8874079817346658,3.3674540463546867,4.7811618165985674,2.3919384643031671,5.1149217605193016,2.7286357707043107,2.6416571280440806,2.8276991068579114,3.9096775508128352,2.2356580430068385,2.4510207922367289,2.2030990062782805,2.0686249659911278,2.3653257194826325,2.6736741831048128,3.5816308252113687,2.8728979403216663,2.2834706953007222,3.248813537312977,3.1938196967537182,4.6278863241248658,1.5777737310654909,4.7431120993583962,2.5515663208972388,3.2733076957809906,4.484401938245667,3.4076455417369655,2.2762438261257336,1.7091605664102771,3.5942052787671313,3.4489445298459445,3.4992788618351423,3.1294236848314636,3.8270724430309078,2.115477885524919,3.5097185310568952,3.5600896518094203,2.1729300337457063,2.3059397666658361,3.3339710028993137,1.7217715884266791,3.4773244780329824,2.1607511096682512,2.750088807455076,2.4409609964724255,1.4376713378332859,3.125053528773913,3.0297892559815871,1.7720172456196344,3.0986103303892985,2.5937526314762063,2.2223940256122279,2.2213359093531428,2.9489060334279431,1.3643523769044581,3.1727917602834244,3.9608338087092552,3.846296978294089,4.9141624349195059,1.5288500064295778,1.7773322354484371,4.1730075031346008,2.7336318850033066,4.998603379210814],"y":[0.63628817004072147,0.4279935959674514,0.63947459313620991,1.3139508156111221,0.5969502001641257,1.279799422656493,1.5297585341918241,0.68405261584872756,0.61873122995686369,0.94612536732605712,0.82580857374928496,1.0608676075967429,1.887917359886488,0.82298695308754211,0.93808998029175672,0.81393995970092026,0.69077090709347821,1.2541419979359549,0.9655171799805794,0.96814214562736911,2.027384371257178,0.93093471090685609,0.97236214530611553,0.78395799778968067,0.67186840030474193,1.2693386363788368,1.2124575002159594,0.84961041015816685,0.78840979771715802,1.5045101601595514,0.85163194196736069,0.83575771406306865,0.8017127175098635,0.60432952851168364,1.240471929418584,1.6058092424306036,0.65550377934017934,1.1838712300811236,1.3135146024592548,0.747330311472442,1.4802641827293115,0.73254824344741065,0.69046555632450057,1.2210752020842197,1.3964366974858398,0.65155979331105274,0.87255373355690957,0.78763601508423309,0.77233386503503132,0.83487318673701116,0.69887397391785366,1.0858075630344552,1.1243730421488127,0.79926162336617068,0.98719437485141537,0.84118080223935487,1.4883654195149305,0.59649263497436178,1.2954656223887977,0.88365279962912091,1.0407668273615696,1.0979187644774933,1.243923680530032,0.92392397452992381,0.60023816348717618,0.87748208366361646,1.227915219263318,1.030230212824141,0.88340220658643531,1.2544474268832935,0.76298146577393122,0.98793164024875413,0.9341357552997811,0.77786747995663708,0.81990652771841288,0.87841188694441741,0.56722334708330591,1.1660651084482561,0.57997502937013823,0.72604178364651173,0.76077909106487651,0.51807225721344574,0.94517039082836329,0.91614880841531487,0.50999603953950146,0.91934353086045018,0.7362467287120269,0.66425178271776275,0.75517969102393723,0.92856550900540291,0.48589105232701413,1.2674174104105655,1.3981892778153293,0.94491368837028922,1.6725810689018821,0.57727416163558642,0.51355573526065368,1.2067302614701587,0.90379545341875078,1.6265805965982199],"z":[1.4411318117463223,0.95374564495472025,1.3695880204498823,3.0714478961008749,1.3434468184385129,2.7217864184306171,3.3572190130707948,1.551742901914156,1.4240344680032051,2.0515129130422873,1.8660935016524944,2.3283923648154712,4.1636302217433148,1.8150365687561576,2.0461572451258965,1.6927295718263859,1.5256369131974268,2.6370615537251156,2.0569710638946477,2.1667887614509316,4.4097118264754682,1.943327167641669,2.1892595827173293,1.6519792496640116,1.5126962456960682,2.6844293071196246,2.5251716677623492,1.830806744139464,1.695529353606688,3.3279943881591554,1.8523125082601553,1.8903898636747318,1.8151579542147658,1.3448607816832157,2.6132720493029695,3.4117751335947752,1.412036881328709,2.543171687855061,2.9996739593805097,1.6507487992174101,3.3339401069225993,1.6835126531021005,1.5986856895031383,2.5131191917682769,2.9891702457795053,1.4647993566634405,1.8691396286799251,1.6856343412258226,1.6372696913861431,1.7918548802435756,1.6181282930848797,2.4165140636621332,2.3599415009419267,1.7186663141656218,2.1957626821218112,1.9434810674563145,3.2644519214914891,1.2611180966111597,2.9631653103364251,1.9047322077328606,2.289060307576972,2.5904482978979804,2.6499236900735124,1.9248280611836903,1.2897308291525462,2.0720782386124044,2.6303266460356176,2.3100250338740391,2.0027523786819992,2.7388800719779143,1.6296920180045171,2.2414339671207468,2.1605178104583218,1.6642438143928318,1.756839499682999,2.029294605551029,1.2369544440786657,2.5322650249184275,1.3329480585926445,1.6763427594453104,1.6814735882900715,1.1067888810958262,2.1047677329940293,2.0402582557052482,1.150308275166261,2.0572966710789231,1.6666884635759305,1.4836547090851111,1.6347454441095917,2.047136805459103,1.0407612890200062,2.6490017541106714,3.0008002880484432,2.2272033277030236,3.6196864642958873,1.2208114663670706,1.1571357146665247,2.7184255334522893,1.9692568351306201,3.5575427796217571],"name":"Data","color":["blue"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","size":0.10000000000000001,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"camera":{"eye":{"x":-1.5,"y":-1.5,"z":1}},"xaxis":{"title":"x"},"yaxis":{"title":"y"},"zaxis":{"title":"z"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[0,1.5312480579208523],"y":[0,0.31495088346374667],"z":[0,0.78480940089468254],"name":"1st Feature","type":"scatter3d","mode":"lines","marker":{"color":"rgba(255,0,0,1)","line":{"color":"rgba(255,0,0,1)"}},"textfont":{"color":"rgba(255,0,0,1)"},"error_y":{"color":"rgba(255,0,0,1)"},"error_x":{"color":"rgba(255,0,0,1)"},"line":{"color":"rgba(255,0,0,1)"},"frame":null},{"x":[0,0.71630781511966213],"y":[0,0.3991165974471888],"z":[0,0.78600679992932787],"name":"2nd Feature","type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,0,0,1)","line":{"color":"rgba(0,0,0,1)"}},"textfont":{"color":"rgba(0,0,0,1)"},"error_y":{"color":"rgba(0,0,0,1)"},"error_x":{"color":"rgba(0,0,0,1)"},"line":{"color":"rgba(0,0,0,1)"},"frame":null},{"x":[2.2458658555579856,1.4189738247191614,1.7947618282360722,5.1982379050267369,2.0566012901386608,3.4791374787258706,4.7682105035254327,2.4287171183528118,2.3169614643758334,2.8031041612770111,2.8896561609509739,3.3078936796914187,6.0042957908457781,2.6174916620155293,2.8501389193892188,1.9878637902679244,2.209855144722614,3.2323477110359473,2.6457632141130114,3.26479292303766,6.0868278327631105,2.3163721194278879,3.3554910312848918,2.0414525752754398,2.3184927014872896,3.3620000824413538,2.9826122254189773,2.4500607875818154,2.2536290982377833,4.8432302398622076,2.5565641188261345,2.9351195189903487,2.8259701876054271,1.9928345766282269,3.2261986650171695,4.3457550048270663,1.8874079817346658,3.3674540463546867,4.7811618165985674,2.3919384643031671,5.1149217605193016,2.7286357707043107,2.6416571280440806,2.8276991068579114,3.9096775508128352,2.2356580430068385,2.4510207922367289,2.2030990062782805,2.0686249659911278,2.3653257194826325,2.6736741831048128,3.5816308252113687,2.8728979403216663,2.2834706953007222,3.248813537312977,3.1938196967537182,4.6278863241248658,1.5777737310654909,4.7431120993583962,2.5515663208972388,3.2733076957809906,4.484401938245667,3.4076455417369655,2.2762438261257336,1.7091605664102771,3.5942052787671313,3.4489445298459445,3.4992788618351423,3.1294236848314636,3.8270724430309078,2.115477885524919,3.5097185310568952,3.5600896518094203,2.1729300337457063,2.3059397666658361,3.3339710028993137,1.7217715884266791,3.4773244780329824,2.1607511096682512,2.750088807455076,2.4409609964724255,1.4376713378332859,3.125053528773913,3.0297892559815871,1.7720172456196344,3.0986103303892985,2.5937526314762063,2.2223940256122279,2.2213359093531428,2.9489060334279431,1.3643523769044581,3.1727917602834244,3.9608338087092552,3.846296978294089,4.9141624349195059,1.5288500064295778,1.7773322354484371,4.1730075031346008,2.7336318850033066,4.998603379210814],"y":[0.63628817004072147,0.4279935959674514,0.63947459313620991,1.3139508156111221,0.5969502001641257,1.279799422656493,1.5297585341918241,0.68405261584872756,0.61873122995686369,0.94612536732605712,0.82580857374928496,1.0608676075967429,1.887917359886488,0.82298695308754211,0.93808998029175672,0.81393995970092026,0.69077090709347821,1.2541419979359549,0.9655171799805794,0.96814214562736911,2.027384371257178,0.93093471090685609,0.97236214530611553,0.78395799778968067,0.67186840030474193,1.2693386363788368,1.2124575002159594,0.84961041015816685,0.78840979771715802,1.5045101601595514,0.85163194196736069,0.83575771406306865,0.8017127175098635,0.60432952851168364,1.240471929418584,1.6058092424306036,0.65550377934017934,1.1838712300811236,1.3135146024592548,0.747330311472442,1.4802641827293115,0.73254824344741065,0.69046555632450057,1.2210752020842197,1.3964366974858398,0.65155979331105274,0.87255373355690957,0.78763601508423309,0.77233386503503132,0.83487318673701116,0.69887397391785366,1.0858075630344552,1.1243730421488127,0.79926162336617068,0.98719437485141537,0.84118080223935487,1.4883654195149305,0.59649263497436178,1.2954656223887977,0.88365279962912091,1.0407668273615696,1.0979187644774933,1.243923680530032,0.92392397452992381,0.60023816348717618,0.87748208366361646,1.227915219263318,1.030230212824141,0.88340220658643531,1.2544474268832935,0.76298146577393122,0.98793164024875413,0.9341357552997811,0.77786747995663708,0.81990652771841288,0.87841188694441741,0.56722334708330591,1.1660651084482561,0.57997502937013823,0.72604178364651173,0.76077909106487651,0.51807225721344574,0.94517039082836329,0.91614880841531487,0.50999603953950146,0.91934353086045018,0.7362467287120269,0.66425178271776275,0.75517969102393723,0.92856550900540291,0.48589105232701413,1.2674174104105655,1.3981892778153293,0.94491368837028922,1.6725810689018821,0.57727416163558642,0.51355573526065368,1.2067302614701587,0.90379545341875078,1.6265805965982199],"z":[1.4411318117463223,0.95374564495472025,1.3695880204498823,3.0714478961008749,1.3434468184385129,2.7217864184306171,3.3572190130707948,1.551742901914156,1.4240344680032051,2.0515129130422873,1.8660935016524944,2.3283923648154712,4.1636302217433148,1.8150365687561576,2.0461572451258965,1.6927295718263859,1.5256369131974268,2.6370615537251156,2.0569710638946477,2.1667887614509316,4.4097118264754682,1.943327167641669,2.1892595827173293,1.6519792496640116,1.5126962456960682,2.6844293071196246,2.5251716677623492,1.830806744139464,1.695529353606688,3.3279943881591554,1.8523125082601553,1.8903898636747318,1.8151579542147658,1.3448607816832157,2.6132720493029695,3.4117751335947752,1.412036881328709,2.543171687855061,2.9996739593805097,1.6507487992174101,3.3339401069225993,1.6835126531021005,1.5986856895031383,2.5131191917682769,2.9891702457795053,1.4647993566634405,1.8691396286799251,1.6856343412258226,1.6372696913861431,1.7918548802435756,1.6181282930848797,2.4165140636621332,2.3599415009419267,1.7186663141656218,2.1957626821218112,1.9434810674563145,3.2644519214914891,1.2611180966111597,2.9631653103364251,1.9047322077328606,2.289060307576972,2.5904482978979804,2.6499236900735124,1.9248280611836903,1.2897308291525462,2.0720782386124044,2.6303266460356176,2.3100250338740391,2.0027523786819992,2.7388800719779143,1.6296920180045171,2.2414339671207468,2.1605178104583218,1.6642438143928318,1.756839499682999,2.029294605551029,1.2369544440786657,2.5322650249184275,1.3329480585926445,1.6763427594453104,1.6814735882900715,1.1067888810958262,2.1047677329940293,2.0402582557052482,1.150308275166261,2.0572966710789231,1.6666884635759305,1.4836547090851111,1.6347454441095917,2.047136805459103,1.0407612890200062,2.6490017541106714,3.0008002880484432,2.2272033277030236,3.6196864642958873,1.2208114663670706,1.1571357146665247,2.7184255334522893,1.9692568351306201,3.5575427796217571],"name":"Data","type":"scatter3d","mode":"markers","marker":{"color":"rgba(0,0,255,1)","size":[55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55],"sizemode":"area","line":{"color":"rgba(0,0,255,1)"}},"textfont":{"color":"rgba(0,0,255,1)","size":55},"error_y":{"color":"rgba(0,0,255,1)","width":55},"error_x":{"color":"rgba(0,0,255,1)","width":55},"line":{"color":"rgba(0,0,255,1)","width":55},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.8: Data in the positive span of two vectors
</p>
</div>
<p>The data are constrained within the positive span of the vectors, a notion we may now define.</p>
<div class="definition">
<p><span id="def:pos-span" class="definition"><strong>Definition 4.2  (Positive Span) </strong></span>The positive span of a set of vectors <span class="math inline">\(\{\vec{h}_1,\dots,\vec{h}_k\}\in\mathbb{R}^d\)</span> is the set
<span class="math display">\[\Gamma\left(\{\vec{h}_1,\dots,\vec{h}_k\}\right) = \left\{\vec{v}\in\mathbb{R}^d \, \bigg|\, \vec{v} = \sum_{j=1}^k a_j \vec{h}_j, \, a_1,\dots,a_k \ge 0\right\}.\]</span>
This set is also called the simplicial cone or conical hull of <span class="math inline">\(\{\vec{h}_1,\dots,\vec{h}_k\}\)</span>.</p>
</div>
<p>In the motivating example above, our data live in the positive span of the two feature vectors, thus we say the data matrix <span class="math inline">\({\bf X}\)</span> has a nonnegative matrix factorization <span class="math inline">\({\bf WH}\)</span>.</p>
<p>Thus, we may view NMF as a restricted version of PCA or SVD where we have moved from spans to positive spans. This seemingly small change gives rise to some big problems. Even in this simple case above we have a uniqueness problem. Up to sign changes and ordering, the feature vectors in PCA and SVD were unique. However, we can find two alternative vectors <span class="math inline">\(\vec{h}_1&#39;\)</span> and <span class="math inline">\(\vec{h}_2&#39;\)</span> which still give a exact NMF. There are trivial cases. First, one can change ordering (<span class="math inline">\(\vec{h}_1&#39; =\vec{h}_2\)</span> and <span class="math inline">\(\vec{h}_2&#39; = \vec{h}_1\)</span>) which we avoid in PCA and SVD by assuming the corresponding singular values of PC variances are listed in decreasaing order. Secondly, we could rescale by setting <span class="math inline">\(\vec{h}_1&#39; = c\vec{h}_1\)</span> and rescaling the corresponding coefficients by a factor of <span class="math inline">\(1/c\)</span> for some <span class="math inline">\(c &gt; 0\)</span> which PCA and SVD avoid by fixing feature vectors to have unit length. The ordering issue is a labeling concern and may be ignored, whereas the rescaling issue can be addressed by adding constraints on the length of the feature vectors.</p>
<p>A third and far more subtle issue occurs because we do not enforce orthogonality. In @ref[fig:nmf-ex], imagine that the feature vectors are the arms of a folding fan. We could change the angle between our feature vectors by opening or closing the arms of the fan. So long as we do not close the fan too much (and leave our data outside the positive span) or open them too much (so that feature vectors have negative entries), we can still find a perfect reconstruction. This `folding fanâ€™ issue can be addressed through additional penalties which we discuss in @ref{sec-nmf-ext}, but nonuniqueness cannot be eliminated entirely. Thus, we seek <strong>an</strong> NMF for our data rather than <strong>the</strong> NMF.</p>
</div>
<div id="finding-an-nmf-multiplicative-updates" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Finding an NMF: Multiplicative Updates<a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a given choice of error, the lack of a unique solution also means there is no closed form solution for <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>. Thus, we will need to apply numerical optimization to find a <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which minimizes the selected error, <span class="math display">\[\mathop{\mathrm{arg\,min}}_{{\bf W}\in\mathbb{R}^{N\times k}_{\ge 0}, {\bf H}\in\mathbb{R}^{k\times d}_{\ge 0}} D({\bf X},{\bf WH})\]</span>. The loss is a function of all of the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>. To apply any type of gradient based optimization, we need to compute the partial derivative of our loss with respect to each of the entries of these matrices.</p>
<p>As an example, we will focus on the divergence and show the relevant details. For gradient based optimization, we then need to compute <span class="math inline">\(\frac{\partial }{\partial {\bf W}_{ij}} D({\bf X}\| {\bf WH})\)</span> for <span class="math inline">\(`\le i \le N\)</span> and <span class="math inline">\(1\le j\le k\)</span> and <span class="math inline">\(\frac{\partial }{\partial {\bf H}_{j\ell}} D({\bf X}\| {\bf WH})\)</span> for <span class="math inline">\(1\le j \le k\)</span> and <span class="math inline">\(1\le \ell \le d.\)</span> Note that <span class="math display">\[({\bf WH})_{st} = \sum_{j=1}^k{\bf W}_{sk}{\bf H}_{kt}\]</span> so that
<span class="math display">\[\frac{\partial ({\bf WH})_{st}}{\partial ({\bf WH})_{ij}} = \begin{cases}
{\bf H}_{jt} &amp; s = i \\
0 &amp; s\ne i
\end{cases}\]</span>
Thus, we may may make use of the chain rule to conclude that
<span class="math display">\[\begin{align*}
\frac{\partial }{\partial {\bf W}_{ij}} D({bf X}\| {\bf WH}) &amp;= \frac{\partial }{\partial {\bf W}_{ij}}\sum_{st} \left({\bf X}_{st} \log(({\bf X})_{st}) - {\bf X}_{st} \log(({\bf WH})_{st}) + ({\bf WH})_st - ({\bf X})_{st}\right)\\
&amp;=\frac{\partial }{\partial {\bf W}_{ij}}\sum_{st} \left(- {\bf X}_{st} \log(({\bf WH})_{st}) + ({\bf WH})_{st} \right) \\
&amp;=\sum_{st}\left(\frac{{\bf X}_{st}}{({\bf WH}_{st})} + 1\right)\frac{\partial ({\bf WH})_{st}}{\partial {\bf W}_{ij}} \\
&amp;=\sum_{t=1}^d \left(-\frac{{\bf X}_{it}}{({\bf WH}_{it})} + 1\right){\bf H}_{jt}.
\end{align*}\]</span>
A similar calculation gives
<span class="math display">\[\frac{\partial }{\partial {\bf H}_{ij}} D({bf X}\| {\bf WH}) = \sum_{s=1}^N \left(-\frac{{\bf X}_{sj}}{({\bf WH})_{sj}} + 1\right) {\bf W}_{sj}.\]</span>
In a standard implementation of gradient descent, we choose a step size <span class="math inline">\(\epsilon &gt;0\)</span> and apply the updates
<span class="math display">\[\begin{equation}
\begin{split}
{\bf W}_{ij} \leftarrow {\bf W}_{ij} - \epsilon \sum_{t=1}^d \left(-\frac{{\bf X}_{it}}{({\bf WH}_{it})} + 1\right){\bf H}_{jt} \\
{\bf H}_{ij} \leftarrow {\bf H}_{ij} - \epsilon \sum_{s=1}^N \left(-\frac{{\bf X}_{sj}}{({\bf WH})_{sj}} + 1\right) {\bf W}_{sj}
\end{split}
\end{equation}\]</span>
to each entry of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> simultaneously. Alternatively, we could consider coordinate descent where we update each entry of <span class="math inline">\({\bf W}\)</span> (holding all other entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> constant) separately then do the same for <span class="math inline">\({\bf H}\)</span> then repeat. Each approach will converge to a local mode (though possibly different ones) when <span class="math inline">\(\epsilon\)</span> is sufficiently small. However, if <span class="math inline">\(\epsilon\)</span> is too small it may take many iterations to converge. Unfortunately, choosing <span class="math inline">\(\epsilon\)</span> can create a numerically unstable algorithm (that doesnâ€™t converge at all) or one where we lose the nonnegativity condition on the entries of <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span>.</p>
<p>To preserve nonnegativity, <span class="citation">[<a href="#ref-lee_seung">10</a>]</span> developed a set of state dependent step-sizes (one for each entry of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>) which result in multiplicative rather than additive updates. IFor divergence, they take step sizes
<span class="math display">\[\epsilon_{ij}^{W} = \frac{{\bf W}_{ij}}{\sum_t {\bf H}_{jt}} \text{ and } \epsilon_{ij}^H = \frac{{\bf H}_{ij}}{\sum_s {\bf W}_{sj}}.\]</span>
These step size are proportional to the entries we are updating so that we take larger steps for larger entries of <span class="math inline">\({\bf W}\)</span> of <span class="math inline">\({\bf H}\)</span>. If we substitute these step sizes in the updates simplify to</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
{\bf W}_{ij} \leftarrow {\bf W}_{ij} \left[\frac{\sum_t \left({\bf H}_{jt}{\bf X}_{it}/({\bf WH}_{it})\right)}{\sum_t {\bf H}_{jt}} \right] \\
{\bf H}_{ij} \leftarrow {\bf H}_{ij} \left[\frac{\sum_s\left({\bf W}_{si}{\bf X_{sj}}/({\bf WH})_{si}    \right)}{\sum_s {\bf W}_{si}}\right]
\end{split}
\end{equation}\]</span></p>
<p>indicating that we rescale the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> by some nonnegative value which is guaranteed to preserve the nonnegativity of each entry. One can verify that the multiplicative updates simplify to one when <span class="math inline">\({\bf X}= {\bf WH}.\)</span> One can then iteratively update the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> one at a time, in blocks, or all together until a local mode is reached.</p>
<p>Multiplicative update rules for other choices of loss are also available in <span class="citation">[<a href="#ref-lee_seung">10</a>]</span> but are also provided as exercises for the reader.</p>
</div>
<div id="nmf-in-practice" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> NMF in practice<a href="nonnegative-matrix-factorization.html#nmf-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a given choice of loss, standard NMF proceeds in the following manner.</p>
<ol style="list-style-type: decimal">
<li>Choose a rank <span class="math inline">\(k\)</span> and initial <span class="math inline">\({\bf W}_\in\mathbb{R}^{N\times k}_{\ge 0}\)</span> and <span class="math inline">\({\bf H}\in\mathbb{R}^{k\times d}_{\ge 0}\)</span>.</li>
<li>Apply the multiplicative rule until a local minimum is reached.</li>
<li>Repeats steps (1) and (2) for different initial conditions then select the final <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which give the lowest overall loss.</li>
</ol>
<p>Different initial conditions (ICs) will converge to different local modes; reptition over multiple differents (ICs) will help find a better overall minimizer and avoid getting a final NMF which is trapped in a poor local mode. There is no guidance of how many ICs. Many packages default to five. More would be better but computational resources are not infinite, and finding even one mode though coordinate ascent can be slow.</p>
<p>The choice of <span class="math inline">\(k\)</span> is also challenging. In prior information or other project constraints dictate a specific choice of <span class="math inline">\(k\)</span> one should use that value. However, the preceding steps can be repeated over a range of <span class="math inline">\(k\)</span>. One can then compare plot the optimal loss as a function of <span class="math inline">\(k\)</span> and look for a cutoff where the error appears to saturate. Importantly, there are no connections between the rank <span class="math inline">\(k\)</span> NMF and rank <span class="math inline">\(k+1\)</span> NMF. Unlike, PCA or SVD, one cannot truncate the vectors and coefficients and attain an optimal solution for a lower-dimensional representation. Thus, separate fits at each choice of <span class="math inline">\(k\)</span> must be attained through separate runs of the numerical optimization.</p>
<div class="example">
<p><span id="exm:mnist-nmf" class="example"><strong>Example 4.2  (NMF applied to MNIST) </strong></span>As an example, we show NMF applied to the first <span class="math inline">\(100\)</span> eights (to save computation time) from the MNIST dataset <span class="citation">[<a href="#ref-mnist">1</a>]</span> using Divergence with five runs per rank. Below we fit separate NMFs for ranks 1 through 25 with 5 random initializations per fit. Optimal divergence as a function of rank is shown below.</p>
<p><img src="_main_files/figure-html/mnist-w-nmf-1.png" width="672" /></p>
<p>There is no clear elbow like feature in the plot that would clearly suggest an optimal rank between 1 and 25. If we select a rank 4 NMF, we do get interpretable 8-like images as our feature images(vectors) shown below.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>Compare this to the less 8-like images found using the features given by a rank 4 approximation from SVD.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
</div>
</div>
<div id="sec-nmf-ext" class="section level3 hasAnchor" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Regularization and Interpretability<a href="nonnegative-matrix-factorization.html#sec-nmf-ext" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a random initialization, the values of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> obtained when the multiplicative rules are run to convergence not resemble the original data, which is contrary to our original goal of finding features which are more comparable to the data. More specifically, the feature vectors <span class="math inline">\(\vec{h}_1,\dots,\vec{h}_k\)</span> which are superimposed to approximate the observations may appear quite different from the observations themselves. Many extensions of NMF address this issue through the inclusion of additional penalties on the elements of <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span> which induce sparsity in the coefficients <span class="math inline">\({\bf W}\)</span> and/or constrain the features to be more similar to the data. In practice, it is insufficient to apply penalties which depend solely on the scaling of either <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span> as these can typically be made arbitrarily small by increasing corresponding elements of <span class="math inline">\({\bf H}\)</span> or <span class="math inline">\({\bf W}\)</span> respectively. Thus, any penalty which depends on the scaling of one matrix often includes additional constraints or penalties on the other.</p>
<p>Briefly, we discuss two version which are constructed to generate features which strongly mimic the original observations.</p>
<div id="archetypes" class="section level4 hasAnchor" number="4.3.5.1">
<h4><span class="header-section-number">4.3.5.1</span> Archetypes<a href="nonnegative-matrix-factorization.html#archetypes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Before discussing Archetypal Analysis, we need an additional geometric idea.</p>
<div id="conv" class="def" name="Convex Hull">
<p>Given a set of vectors <span class="math inline">\(\mathcal{H}=\{\vec{h}_1,\dots,\vec{h}_k\}\subset\mathbb{R}^d\)</span>, the convex hull of <span class="math inline">\(\mathcal{H}\)</span> is the set
<span class="math display">\[\text{conv}(\mathcal{H}) = \{b_1\vec{h}_1+\cdots + b_k \vec{h}_k: b_1,\dots,b_k \ge 0,\, c_1+\dots+c_k = 1\}\]</span></p>
</div>
<p>The weights in the above definition are constrained to be positive, similar to the definition of positive span, but with the added constraint that they sum to one. Thus, for a set of vectors <span class="math inline">\(\mathcal{H}\)</span></p>
<p><span class="math display">\[\text{conv}(\mathcal{H}) \subset \Gamma(\mathcal{H}) \subset \text{span}(\mathcal{H}).\]</span>
An example of a convex hull (shown in grey) of set of vectors (points in red) in <span class="math inline">\(\mathbb{R}^2\)</span> is shown below. Imagine wrapping a string around the set of points and pulling it tight. Everything on/within the loop is in the convex hull. An analogous interpretation holds in higher dimensions using a â€œsheetâ€ rather than a string.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p>Importantly, we can view the coefficient <span class="math inline">\(c_1,\dots,c_k\)</span> as defining (probability) weights. Thus, every point in the convex hull of <span class="math inline">\({\bf H}\)</span> is a weighted average of the vectors in <span class="math inline">\(\mathcal{H}\)</span> and one then expects any point in the convex hull of <span class="math inline">\(\mathcal{H}\)</span> to resemble the vectors in <span class="math inline">\(\mathcal{H}\)</span>. In archetypal analysis <span class="citation">[<a href="#ref-arch">11</a>]</span>, we add the constraint that the features (row of <span class="math inline">\({\bf H}\)</span>) are themselves elements in the convex hull of the original data so that <span class="math inline">\({\bf H}= {\bf B X}\)</span> where <span class="math inline">\({\bf B}\in\mathbb{R}_{\ge 0}^{k\times N}\)</span> must satisfy the constraint <span class="math inline">\({\bf B}\vec{1}_N = \vec{1}_k\)</span>.</p>
<p>Under this setup, we then want to solve the following constrained optimization problem
<span class="math display">\[\mathop{\mathrm{arg\,min}}_{{\bf W}\in \mathbb{R}_{\ge 0}^{N\times k},\,{\bf B}\in\mathbb{R}_{\ge 0}^{k\times N}, {\bf B}\vec{1}_N = \vec{1}_k}\| {\bf X}- {\bf WBX}\|_F.\]</span></p>
<p>The rows of <span class="math inline">\({\bf BX}\)</span>, called archetypes, reside are typically extremal in the sense that they reside on the boundary of the convex hull of the data. We show them below for a rank four approximation to the eights.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
</div>
<div id="volume-regularized-nmf" class="section level4 hasAnchor" number="4.3.5.2">
<h4><span class="header-section-number">4.3.5.2</span> Volume Regularized NMF<a href="nonnegative-matrix-factorization.html#volume-regularized-nmf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Much like Archetype Analysis, Volume regularized NMF creates feature vectors which resemble the observation through a more geometric approach <span class="citation">[<a href="#ref-vrnmf2">13</a>]</span>. Suppose that we are seeking a rank <span class="math inline">\(k\)</span> approximation of data in <span class="math inline">\(d\)</span> dimensions with <span class="math inline">\(k&lt;d\)</span>. If the vectors <span class="math inline">\(\vec{h}_1,\dots,\vec{h}_k\)</span> are affinely independent, i.e.Â they donâ€™t live on a hyperplane of dimension less than <span class="math inline">\(k\)</span>, then the volume of the convex of hall of the vectors is given by the following formula
<span class="math display">\[\text{vol}\left(\text{conv}(\{\vec{h}_1,\dots,\vec{h}_k\})\right) = \frac{\sqrt{\text{det}(\tilde{\bf H}\tilde{\bf H}^T})}{(k-1)!}\]</span>
where <span class="math display">\[\tilde{\bf H} = \begin{bmatrix} \vec{h}_1^T-\vec{h}_k^T \\ \vdots \\ \vec{h}_{k-1}^T-\vec{h}_k^T\end{bmatrix} \in \mathbb{R}^{(k-1)\times d}.\]</span>
An extension of this result is that for linearly independent vectors <span class="math inline">\(\vec{h}_1,\dots,\vec{h}_k\)</span>, the volume of the convex hull of the vectors <span class="math inline">\(\vec{0},\vec{h}_1,\dots,\vec{h}_k\)</span> can be computed directly from our feature matrix <span class="math inline">\({\bf H}\)</span>, i.e.Â <span class="math display">\[\text{vol}\left(\text{conv}(\{0,\vec{h}_1,\dots,\vec{h}_k\})\right) = \frac{\sqrt{det({\bf H}{\bf H}^T})}{(k)!}.\]</span></p>
<p>One brief aside on the notion of volume here that is not typically made clear in the related literature. When <span class="math inline">\(k &lt; d\)</span>, the convex hull of <span class="math inline">\(\vec{0},\vec{h}_1,\dots,\vec{h}_k\)</span> is contained in a <span class="math inline">\(k\)</span>-dimensional hyperplane. Thus, its <span class="math inline">\(d\)</span>-dimensional volume must be zero. When we talk of the volume of a convex hull, we instead mean its <span class="math inline">\(k\)</span>-dimensional volume. As an example, consider the case of a single vector <span class="math inline">\(\vec{h}_1\)</span> in <span class="math inline">\(\mathbb{R}^3.\)</span> The convex hull of <span class="math inline">\(\vec{0},\vec{h_1}\)</span> is the line segment from the origin to <span class="math inline">\(\vec{h}_1\)</span>. Line segments have zero 3-dimensional volume, but they do have length (1-dimensional volume). In this case, the 1-dimensional volume of the convex hull is the length of the line segment, e.g.Â <span class="math inline">\(\|\vec{h}_1\|\)</span>.</p>
<p>Incorporating a penalty on the volume of the convex hull of the feature vectors gives rise to the aptly named volume-regularized NMF
<span class="math display">\[\mathop{\mathrm{arg\,min}}_{{\bf W}_\in\mathbb{R}_{\ge 0}^{N\times k}, {\bf W}\vec{1}_N = \vec{1}_k, {\bf H}\in \mathbb{R}_{\ge 0}^{k\times d}} \|{\bf X}-{\bf WH}\|_F^2 + \lambda \sqrt{\text{det}({\bf HH}^T)}\]</span>
here the constraint <span class="math inline">\({\bf W}\vec{1}_N = \vec{1}_k\)</span> requiring that the rows of <span class="math inline">\({\bf W}\)</span> sum to one prevents an arbitrarily shrinkage of the feature vectors towards zero. In some numerical implementation, this constraint is relaxed so that the sum of the rows of <span class="math inline">\({\bf W}\)</span> are bounded above by one. The value <span class="math inline">\(\lambda\)</span> is tuning parameter that must be specified by the user to balance goodness of fit vs minimal volume. Below we show the results of volume regularized NMF applied to the subset of the eights data studied above.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
</div>
</div>
<div id="nmf-and-maximum-likelihood-estimation" class="section level3 hasAnchor" number="4.3.6">
<h3><span class="header-section-number">4.3.6</span> NMF and Maximum Likelihood Estimation<a href="nonnegative-matrix-factorization.html#nmf-and-maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this section we have focused on the geometric aspects of NMF, but there are parallel statistical perspectives based on maximum likelihood estimation. For example, consider the model <span class="math display">\[{\bf X}_{ij} \mid ({\bf WH}_{ij}) \sim \mathcal{N}\left(({\bf WH}_{ij},\sigma^2\right), \qquad 1\le i\le N, 1\le j\le d\]</span>
with the additional assumption that the observations in <span class="math inline">\({\bf X}\)</span> are conditionally independent given <span class="math inline">\(({\bf WH})\)</span> and that <span class="math inline">\({\bf WH}\)</span> is low-rank. Under this framework the likelihood of the data is then
<span class="math display">\[\mathcal{L}({\bf X}\mid{\bf WH}) \propto \prod_{i=1}^N\prod_{j=1}^d \exp\left(-\frac{({\bf X}_{ij}-({\bf WH}_{ij}))^2}{\sigma^2}\right)\]</span> which has log-likelihood
<span class="math display">\[\log \mathcal{L}({\bf X}\mid{\bf WH}) = -\frac{1}{\sigma^2}\prod_{i=1}^N\prod_{j=1}^d ({\bf X}_{ij}-({\bf WH}_{ij}))^2 = -\frac{1}{\sigma^2}\|{\bf X}-{\bf WH}\|_F^2.\]</span>
Thus, minimizing the Frobenius norm is equivalent to maximizing the likelihood in this normal model with homoskedastic errors.</p>
<p>Similarly, divergence arises from a Poisson model <span class="math display">\[{\bf X}_{ij} \mid {\bf WH}_{ij} \sim \text{Pois}\left(({\bf WH})_{ij}\right)\]</span> again with a low rank structure for <span class="math inline">\({\bf WH}\)</span> and the conditional independence assumption on the entries of <span class="math inline">\({\bf X}\)</span> given <span class="math inline">\({\bf WH}\)</span>.</p>
<p>The IS divergence again uses conditional independence and a low rank structure for <span class="math inline">\({\bf WH}\)</span> under the assumption of multiplicative Gamma distributed errors with mean one, i.e.Â 
<span class="math display">\[{\bf X}_{ij} \mid {\bf WH}_{ij} = ({\bf WH})_{ij} e_{ij}, \, e_{ij} \sim \text{Gamma}(\alpha,\beta)\]</span>
with <span class="math inline">\(\alpha/\beta = 1\)</span>. See <span class="citation">[<a href="#ref-is_div">14</a>]</span> for additional details.</p>
<!-- MDS -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-mnist" class="csl-entry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Deng</span>, L. (2012). The mnist database of handwritten digit images for machine learning research. <em>IEEE Signal Processing Magazine</em> <strong>29</strong> 141â€“2.</div>
</div>
<div id="ref-lee_seung" class="csl-entry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline"><span class="smallcaps">Lee</span>, D. and <span class="smallcaps">Seung</span>, H. S. (2000). <a href="https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">Algorithms for non-negative matrix factorization</a>. In <em>Advances in neural information processing systems</em> vol 13, (T. Leen, T. Dietterich and V. Tresp, ed). MIT Press.</div>
</div>
<div id="ref-arch" class="csl-entry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline"><span class="smallcaps">Cutler</span>, A. and <span class="smallcaps">Breiman</span>, L. (1994). <a href="http://www.jstor.org/stable/1269949">Archetypal analysis</a>. <em>Technometrics</em> <strong>36</strong> 338â€“47.</div>
</div>
<div id="ref-vrnmf2" class="csl-entry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline"><span class="smallcaps">Fu</span>, X., <span class="smallcaps">Ma</span>, W.-K., <span class="smallcaps">Huang</span>, K. and <span class="smallcaps">Sidiropoulos</span>, N. D. (2015). <a href="https://doi.org/10.1109/TSP.2015.2404577">Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain</a>. <em>IEEE Transactions on Signal Processing</em> <strong>63</strong> 1â€“1.</div>
</div>
<div id="ref-is_div" class="csl-entry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline"><span class="smallcaps">FÃ©votte</span>, C., <span class="smallcaps">Bertin</span>, N. and <span class="smallcaps">Durrieu</span>, J.-L. (2009). <a href="https://doi.org/10.1162/neco.2008.04-08-771">Nonnegative matrix factorization with the itakura-saito divergence: With application to music analysis</a>. <em>Neural Computation</em> <strong>21</strong> 793â€“830.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-svd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-mds.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/young1062/introUL03-linear_methods.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf", "_main.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
