<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Kernels and Nonlinearity | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Kernels and Nonlinearity | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Kernels and Nonlinearity | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-linear.html"/>
<link rel="next" href="ch-nonlinear.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.4/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-prob.html"><a href="ch-prob.html#important-notation"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="ch-prob.html"><a href="ch-prob.html#random-vectors-in-mathbbrd"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="ch-prob.html"><a href="ch-prob.html#expectation-mean-and-covariance"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ch-prob.html"><a href="ch-prob.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="ch-prob.html"><a href="ch-prob.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-prob.html"><a href="ch-prob.html#linear-algebra"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-prob.html"><a href="ch-prob.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-prob.html"><a href="ch-prob.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="ch-prob.html"><a href="ch-prob.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="ch-prob.html"><a href="ch-prob.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="ch-prob.html"><a href="ch-prob.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="ch-prob.html"><a href="ch-prob.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
<li class="chapter" data-level="2.4.7" data-path="ch-prob.html"><a href="ch-prob.html#hadamard-elementwise-operations"><i class="fa fa-check"></i><b>2.4.7</b> Hadamard (elementwise) operations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-prob.html"><a href="ch-prob.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-prob.html"><a href="ch-prob.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-prob.html"><a href="ch-prob.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-prob.html"><a href="ch-prob.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="ch-prob.html"><a href="ch-prob.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#dimension-reduction-and-manifold-learning"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#clustering"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#generating-synthetic-data"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#exercises-1"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-linear.html"><a href="ch-linear.html#sec-pca"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-linear.html"><a href="ch-linear.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-linear.html"><a href="ch-linear.html#pca-in-practice"><i class="fa fa-check"></i><b>4.1.2</b> PCA in Practice</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-linear.html"><a href="ch-linear.html#sec-svd"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-linear.html"><a href="ch-linear.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-linear.html"><a href="ch-linear.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-linear.html"><a href="ch-linear.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-linear.html"><a href="ch-linear.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-linear.html"><a href="ch-linear.html#nonnegative-matrix-factorization"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-linear.html"><a href="ch-linear.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-linear.html"><a href="ch-linear.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-linear.html"><a href="ch-linear.html#finding-a-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding a NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-linear.html"><a href="ch-linear.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-linear.html"><a href="ch-linear.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
<li class="chapter" data-level="4.3.6" data-path="ch-linear.html"><a href="ch-linear.html#nmf-and-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.3.6</b> NMF and Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-linear.html"><a href="ch-linear.html#sec-mds"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-linear.html"><a href="ch-linear.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-linear.html"><a href="ch-linear.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-linear.html"><a href="ch-linear.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="ch-linear.html"><a href="ch-linear.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-linear.html"><a href="ch-linear.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html#kernel-pca"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html#exercises-3"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#the-manifold-hypothesis"><i class="fa fa-check"></i><b>6.1</b> The Manifold Hypothesis</a></li>
<li class="chapter" data-level="6.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#sec-manifolds"><i class="fa fa-check"></i><b>6.2</b> Brief primer on manifolds</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#charts-atlases-tangent-spaces-and-approximating-tangent-planes"><i class="fa fa-check"></i><b>6.2.1</b> Charts, atlases, tangent spaces and approximating tangent planes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#isometric-feature-map-isomap"><i class="fa fa-check"></i><b>6.3</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#key-definitions"><i class="fa fa-check"></i><b>6.3.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm"><i class="fa fa-check"></i><b>6.3.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.3.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#locally-linear-embeddings-lles"><i class="fa fa-check"></i><b>6.4</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction-1"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm-1"><i class="fa fa-check"></i><b>6.4.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.4.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#laplacian-eigenmap"><i class="fa fa-check"></i><b>6.5</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm-2"><i class="fa fa-check"></i><b>6.5.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#hessian-eigenmaps-hlles"><i class="fa fa-check"></i><b>6.6</b> Hessian Eigenmaps (HLLEs)</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction-2"><i class="fa fa-check"></i><b>6.6.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#comparison-of-manifold-learning-methods"><i class="fa fa-check"></i><b>6.7</b> Comparison of Manifold Learning Methods</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#visual-comparison-of-results"><i class="fa fa-check"></i><b>6.7.1</b> Visual comparison of results</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#exercises-4"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-clustering.html"><a href="ch-clustering.html#center-based-clustering"><i class="fa fa-check"></i><b>7.1</b> Center-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-clustering.html"><a href="ch-clustering.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-clustering.html"><a href="ch-clustering.html#k-center-and-k-medoids"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(k\)</span>-center and <span class="math inline">\(k\)</span>-medoids</a></li>
<li class="chapter" data-level="7.1.3" data-path="ch-clustering.html"><a href="ch-clustering.html#minimizing-clustering-loss-functions"><i class="fa fa-check"></i><b>7.1.3</b> Minimizing clustering loss functions</a></li>
<li class="chapter" data-level="7.1.4" data-path="ch-clustering.html"><a href="ch-clustering.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>7.1.4</b> Strengths and Weaknesses</a></li>
<li class="chapter" data-level="7.1.5" data-path="ch-clustering.html"><a href="ch-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>7.1.5</b> Choosing the number of clusters</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-clustering.html"><a href="ch-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-clustering.html"><a href="ch-clustering.html#dendrograms"><i class="fa fa-check"></i><b>7.2.1</b> Dendrograms</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-clustering.html"><a href="ch-clustering.html#building-a-dendrogram"><i class="fa fa-check"></i><b>7.2.2</b> Building a dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-clustering.html"><a href="ch-clustering.html#sec-model-based-clustering"><i class="fa fa-check"></i><b>7.3</b> Model-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ch-clustering.html"><a href="ch-clustering.html#gaussian-mixture-models-gmm"><i class="fa fa-check"></i><b>7.3.1</b> Gaussian Mixture Models (GMM)</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-clustering.html"><a href="ch-clustering.html#expectation-maximization-em-algorithm"><i class="fa fa-check"></i><b>7.3.2</b> Expectation-Maximization (EM) Algorithm</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-clustering.html"><a href="ch-clustering.html#model-selection-and-parameter-constraints"><i class="fa fa-check"></i><b>7.3.3</b> Model Selection and Parameter Constraints</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-clustering.html"><a href="ch-clustering.html#weakneses-and-limitations"><i class="fa fa-check"></i><b>7.3.4</b> Weakneses and Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-clustering.html"><a href="ch-clustering.html#sec-spec-clustering"><i class="fa fa-check"></i><b>7.4</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-clustering.html"><a href="ch-clustering.html#introduction-3"><i class="fa fa-check"></i><b>7.4.1</b> Introduction</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-clustering.html"><a href="ch-clustering.html#algorithm-3"><i class="fa fa-check"></i><b>7.4.2</b> Algorithm</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kernels-and-nonlinearity" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Kernels and Nonlinearity<a href="kernels-and-nonlinearity.html#kernels-and-nonlinearity" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The techniques considered in the previous chapter (PCA, NMF, SVD, and classical Scaling) are ill suited to identify nonlinear structure and dependence in data. If we wish to most efficiently reduce dimensions without loss of information, we will need techniques which incorporate nonlinear structure. One can expand a data matrix by including specific nonlinear relationships then apply PCA or SVD but there are numerous problems with this approach. In particular, which relationships does one choose to include? Even including simple quadratic or cubic terms (features) can result in a data matrix with a massive increase in the number of columns. Even when the original dimensionality of the data is moderate, the including of polynomial terms can quickly result in a data matrix of nonlinear features with an untenable number of columns which can make application of the linear methods we have discussed much more computationally demanding to implement.</p>
<p>Kernels are a important class of functions which can be used to <code>kernelize</code> the methods we have discussed before. In theory, these kernelized versions of the linear methods we have discussed can identify and use nonlinear structure for better dimensionality reduction while circumventing the issue of higher dimensional <code>featurized</code> data. This approach follows from an application of the so called ’kernel trick` which we now discuss.</p>
<p>Briefly, a kernel is a function <span class="math display">\[k:\mathbb{R}^d\times \mathbb{R}^d \to \mathbb{R}\]</span> which has an associated feature space, <span class="math inline">\(\mathcal{H}\)</span> and (implicity defined, possibly nonlinear) feature mapping <span class="math inline">\(\varphi:\mathcal{R}^d \to \mathcal{H}\)</span> such that inner products in the feature space, denoted <span class="math inline">\(\langle \varphi(\vec{x}), \varphi(\vec{y})\rangle_{\mathcal{H}}\)</span> can be obtained through an evaluation of the kernel, namely
<span class="math display">\[\begin{equation}
k(\vec{x},\vec{y}) = \langle \varphi(\vec{x}), \varphi(\vec{y})\rangle_{\mathcal{H}}
\end{equation}\]</span></p>
<p>Any method which can be expressed involving inner products can be kernelized by replacing terms of the form <span class="math inline">\(\vec{x}^T_i\vec{x}_j\)</span> with the quantity <span class="math inline">\(k(\vec{x}_i,\vec{x}_j)\)</span>. Thus, we are replacing inner products of our original <span class="math inline">\(d\)</span>-dimensional data with inner products in the associated feature space <span class="math inline">\(\mathcal{H}\)</span>. Importantly, if we only need inner products, we never need to explicitly compute the feature map <span class="math inline">\(\varphi\)</span> for any of our data! At first glance this connection may seem minor, but by using kernels we can turn many linear techniques into nonlinear methods including PCA, SVD, support vector machines, linear regression, and many others.</p>
<p>There are some limits though. Not every choice of <span class="math inline">\(k\)</span> has an associated feature space. A function is only a kernel if it satisfies Mercer’s Condition.</p>
<div class="theorem">
<p><span id="thm:mercer" class="theorem"><strong>Theorem 5.1  (Mercer's Condition) </strong></span>A function <span class="math display">\[k:\mathbb{R}^d\times \mathbb{R}^d \to \mathbb{R}\]</span> has a an associated feature space <span class="math inline">\(\mathcal{H}\)</span> and feature mapping <span class="math inline">\(\varphi:\mathbb{R}^d \to \mathcal{H}\)</span> such that <span class="math display">\[k(\vec{x},\vec{y}) = \langle \varphi(\vec{x}), \varphi(\vec{y})\rangle_{\mathcal{H}}, \qquad \forall \vec{x},\vec{y}\in\mathbb{R}^d\]</span>
if and only if for any <span class="math inline">\(N \in \{1,2,\dots\}\)</span> and <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\in\mathbb{R}^d\)</span> the kernel matrix <span class="math inline">\({\bf K}\in \mathbb{R}^{N}\)</span> with entries <span class="math inline">\({\bf K}_{ij} = k(\vec{x}_i,\vec{x}_j)\)</span> is positive semidefinite. Equivalently, it must be the case that <span class="math display">\[\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} g(\vec{x})g(\vec{y}) k(\vec{x},\vec{y}) d\vec{x}d\vec{y} \ge 0\]</span> whenever <span class="math inline">\(\int_{\mathbb{R}^2}[g(\vec{x})]d\vec{x}&lt;\infty.\)</span></p>
</div>
<p>We will only consider symmetric functions such that <span class="math inline">\(k(\vec{x},\vec{y}) = k(\vec{y},\vec{x})\)</span> for all <span class="math inline">\(\vec{x},\vec{y}\in\mathbb{R}^d\)</span>. It may not be immediately obvious if a symmetric function satisfies Mercer’s condition, but there are many known examples. A few are shown in the following table.</p>
<table>
<colgroup>
<col width="17%" />
<col width="28%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Equation</th>
<th>Tuning Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Radial Basis Function</td>
<td><span class="math inline">\(k(\vec{x},\vec{y} = \exp\left(-\sigma\|\vec{x}-\vec{y}\|^2\right)\)</span></td>
<td>Scale <span class="math inline">\(\sigma &gt;0\)</span></td>
</tr>
<tr class="even">
<td>Laplace</td>
<td><span class="math inline">\(k(\vec{x},\vec{y} = \exp\left(-\sigma\|\vec{x}-\vec{y}\|\right)\)</span></td>
<td>Scale <span class="math inline">\(\sigma &gt;0\)</span></td>
</tr>
<tr class="odd">
<td>Polynomial</td>
<td><span class="math inline">\(k(\vec{x},\vec{y}) = (c+ \vec{x}^T\vec{y})^d\)</span></td>
<td>Offset <span class="math inline">\(c &gt;0\)</span>, Degree <span class="math inline">\(d \in \mathbb{N}\)</span></td>
</tr>
</tbody>
</table>
<p>The radial basis function (rbf) is the most commonly used kernel and has an associated feature space <span class="math inline">\(\mathcal{H}\)</span> which is infinite dimensional! The associated feature map <span class="math inline">\(\varphi\)</span> for the rbf kernel is
<span class="math display">\[\varphi(\vec{x}) = e^{-\sigma\|\vec{x}\|^2}\left(a_{\ell_0}^{(0)}, a_{1}^{(1)},\dots,a_{\ell_1}^{(1)}, a_{1}^{(2)},\dots, a_{\ell_2}^{(2)},\dots \right)\]</span>
where <span class="math inline">\(\ell_j = \binom{d+j-1}{j}\)</span> and <span class="math inline">\(a_\ell^{(j)} = \frac{(2\sigma)^{j/2}x_1^{\eta_1}\dots x^{\eta_d}}{\sqrt{\eta_1!\dots\eta_d!}}\)</span> when <span class="math inline">\(\eta_1+\dots+\eta_d = j.\)</span> The preceding expression is quite cumbersome, but there is one important point to emphasize. Every possible polynomial combination of the coordinates of <span class="math inline">\(\vec{x}\)</span> appears in some coordinate of <span class="math inline">\(\varphi(\vec{x})\)</span> (though higher order terms are shrunk by the factorial factors in the denominator of <span class="math inline">\(a_\ell^{(j)}\)</span>). Thus, the rbf kernel is associated with a very expressive feature space which makes it a potent but dangerous choice since risks overfitting. To explore these details more, let’s discuss one very important application of kernels in unsupervised learning.</p>
<!-- kernel PCA -->
<div id="kernel-pca" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Kernel PCA<a href="kernels-and-nonlinearity.html#kernel-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have a kernel <span class="math inline">\(k\)</span> and associated feature map <span class="math inline">\(\varphi\)</span>. In kernel PCA, we want to apply to PCA to the featurized data <span class="math inline">\(\varphi(\vec{x}_1),\dots,\varphi(\vec{x}_N)\)</span> rather than the original data. The idea is that by studying the featurized data, we can identify additional nonlinear structure in the features that provides a better lower-dimensional representation of the data. We have discussed three approaches to computing PC scores to data: (i) diagonalization of the sample covariance, (ii) applying SVD to the centered data, and (iii) using the duality of PCA and classical scaling.</p>
<p>For the rbf kernel and its infinite dimensional feature map, approaches (i) and (ii) are impossible. Why? The centered data matrix of features <span class="math display">\[{\bf H}\tilde{\bf X} = {\bf H}\begin{bmatrix} \varphi(\vec{x}_1)^T \\ \vdots \\ \varphi(\vec{x}_N)^T\end{bmatrix} =\begin{bmatrix} \varphi(\vec{x}_1)^T-\bar{\varphi}^T \\ \vdots \\ \varphi(\vec{x}_N)^T-\bar{\varphi}^T\end{bmatrix} \]</span>
has a infinite number of columns so that we cannot compute its SVD. In the above expression, <span class="math inline">\(\bar{\varphi} = \frac{1}{N} \sum_{i=1}^N \varphi(\vec{x}_i)\)</span> is the mean for the feature vectors. The associated sample covariance matrix <span class="math display">\[{\bf \Sigma}_F = \frac{1}{N} \tilde{\bf X}{\bf H}\tilde{\bf X } = \frac{1}{N} \sum_{i=1}^N \left(\varphi(\vec{x}_i) - \bar{\varphi}\right)\left(\varphi(\vec{x}_i) - \bar{\varphi}\right)^T\]</span> will have an infinite number of rows and columns so we cannot hope to diagonalize it either.</p>
<p>Fortunately, the third option, using duality of classical scaling and PC, provides a workaround. Observe that the inner product matrix of the centered feature data <span class="math inline">\({\bf H}\tilde{\bf X} ({\bf H}\tilde{\bf X})^T\)</span> can be written in terms of the kernel since
<span class="math display">\[{\bf H}\tilde{\bf X} ({\bf H}\tilde{\bf X})^T = {\bf H} \begin{bmatrix} \varphi(\vec{x}_1)^T \\ \vdots \\ \varphi(\vec{x}_N)^T\end{bmatrix} \begin{bmatrix} \varphi(\vec{x}_1) &amp; \dots &amp; \varphi(\vec{x}_N)\end{bmatrix} {\bf H} = {\bf H K H}\]</span>
where <span class="math inline">\({\bf K}\)</span> has the inner products in the feature space which we can calculate using the kernel function <span class="math display">\[{\bf K}_{ij} = \varphi(\vec{x}_i)^T\varphi(\vec{x}_j) = k(\vec{x}_i,\vec{x}_j).\]</span></p>
<p>Since <span class="math inline">\(k\)</span> is a symmetric kernel, it follows that <span class="math inline">\({\bf K}\)</span> is positive semidefinite. Using this property, one can argue that <span class="math inline">\({\bf HKH}\)</span> will also be positive semidefinite. We can use the eigendecomposition of the doubly centered kernel to compute the kernel principal component scores. Specifically, if <span class="math inline">\({\bf HKH}\)</span> rank <span class="math inline">\(r\)</span> with eigenvalues <span class="math inline">\(\lambda_1\ge \dots \ge \lambda_r &gt;0\)</span> and corresponding eigenvalues <span class="math inline">\(\vec{u}_1,\dots,\vec{u}_r \in \mathbb{R}^N\)</span>, then <span class="math inline">\({\bf HKH}\)</span> factorizes as <span class="math display">\[{\bf HKH} = \underbrace{\begin{bmatrix}\vec{u}_1 &amp; \dots &amp;\vec{u}_r\end{bmatrix} \begin{bmatrix} \lambda_1^{1/2} &amp;0 &amp;0 \\ 0&amp; \ddots &amp; 0 \\ 0 &amp;0 &amp; \lambda_r^{1/2} \end{bmatrix}}_{{\bf U\Lambda}^{1/2}} \left({\bf U\Lambda}^{1/2}\right)^T. \]</span></p>
<p>The rows of the matrix <span class="math inline">\({\bf U\Lambda}^{1/2}\)</span> are almost the kernel PC scores. The only issue is an additional the identity <span class="math display">\[{\bf HKH} = ({\bf H}\tilde{\bf X})({\bf H}\tilde{\bf X})^T\]</span> is missing the factor of <span class="math inline">\(1/N\)</span> appearing in the covariance calculation. Accounting for this, the first <span class="math inline">\(r\)</span> non-zero kernel PC scores are the rows of the matrix <span class="math display">\[\frac{1}{\sqrt{N}} {\bf U\Lambda}^{1/2}\]</span> and the corresponding nonzero PC variances are <span class="math inline">\(\lambda_1/N,\dots,\lambda_r/N.\)</span></p>
<p>Notably, at no point do we compute the PC loadings! However, similar to standard PCA, we use the scores for dimension reduction and the PC variances for choosing a dimension. Without the loadings, we cannot recompute the original data. Below, we show an application of kernel PCA to the helix and demonstrate its ability to identify the one-dimensional structure of the helix and its sensitivity to kernel selection and tuning.</p>
<div class="example">
<p><span id="exm:kpca-helix" class="example"><strong>Example 5.1  (Kernel PCA applied to the Helix) </strong></span>First, we show the kPCA variances for three different kernels and tuning parameters. The data are regularly spaced points along the helix.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<p>From these graphs, one would infer very different lower dimensional choices depending on the kernel and parameters. The polynomial kernel provides the most robust estimate of the one-dimensional nature of the data.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-139"></span>
<img src="_main_files/figure-html/unnamed-chunk-139-1.png" alt="kPCA Variances for different Kernels" width="672" />
<p class="caption">
Figure 5.1: kPCA Variances for different Kernels
</p>
</div>
<p>Below, we show the recovered one-dimensional coordinates for the polynomial kernel with offset 1 and degree 4 shown below, which is good, but do not quite reflect the equal spaced nature of the points.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-140-1.png" width="672" /></p>
</div>
<p>As the preceding example demonstrates, kernel PCA can identify nonlinear structure, but is quite sensitive to kernel selection and tuning. More advanced implementations make use of cross-validation to aid in the selection and tuning of the kernel <span class="citation">[<a href="#ref-kPCA_tuning2">19</a>]</span>.</p>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Exercises<a href="kernels-and-nonlinearity.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-kPCA_tuning2" class="csl-entry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline"><span class="smallcaps">Mika</span>, S., <span class="smallcaps">Schölkopf</span>, B., <span class="smallcaps">Smola</span>, A., <span class="smallcaps">Müller</span>, K.-R., <span class="smallcaps">Scholz</span>, M. and <span class="smallcaps">Rätsch</span>, G. (1998). <a href="https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf">Kernel PCA and de-noising in feature spaces</a>. In <em>Advances in neural information processing systems</em> vol 11, (M. Kearns, S. Solla and D. Cohn, ed). MIT Press.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-linear.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-nonlinear.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/young1062/introUL04-kernels.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
