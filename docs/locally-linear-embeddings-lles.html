<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.3 Locally Linear Embeddings (LLEs) | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="6.3 Locally Linear Embeddings (LLEs) | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.3 Locally Linear Embeddings (LLEs) | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="isometric-feature-map-isomap.html"/>
<link rel="next" href="autoencoders-aes.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-1-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation 1: Iterative Projections</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="background.html"><a href="background.html#data-on-a-manifold"><i class="fa fa-check"></i><b>6.1.1</b> Data on a manifold</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.4</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-2"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.4.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.5</b> Additional methods</a></li>
<li class="chapter" data-level="6.6" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hierarchical.html"><a href="hierarchical.html"><i class="fa fa-check"></i><b>7.1</b> Hierarchical</a></li>
<li class="chapter" data-level="7.2" data-path="center-based.html"><a href="center-based.html"><i class="fa fa-check"></i><b>7.2</b> Center-based</a></li>
<li class="chapter" data-level="7.3" data-path="model-based.html"><a href="model-based.html"><i class="fa fa-check"></i><b>7.3</b> Model-based</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-based.html"><a href="model-based.html#k-means"><i class="fa fa-check"></i><b>7.3.1</b> k-means</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-based.html"><a href="model-based.html#k-mediods"><i class="fa fa-check"></i><b>7.3.2</b> k-mediods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>7.4</b> Spectral</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="locally-linear-embeddings-lles" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Locally Linear Embeddings (LLEs)<a href="locally-linear-embeddings-lles.html#locally-linear-embeddings-lles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introduction-1" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Introduction<a href="locally-linear-embeddings-lles.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Locally linear embedding (LLE) is an unsupervised learning algorithm first introduced in 2000 by Sam T. Roweis and Lawrence K. Saul <span class="citation">[<a href="#ref-lle">11</a>]</span>. In the original four-page paper, the two authors introduced the LLE algorithm and demonstrated its effectiveness in dimensional reduction, manifold learning, and in handling real-world high-dimensional data. Unlike clustering methods for local dimensional reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text. Thanks to its great mathematical properties and relatively low computing cost (compared to other manifold learning methods, like ISOMAP), LLE quickly became attractive to researchers after its emergence due to its ability to deal with large amounts of high dimensional data and its non-iterative way of finding the embeddings <span class="citation">[<a href="#ref-lle_survey">12</a>]</span>. Compared to ISOMAP and some other previous manifold learning methods, LLE is computationally simpler and can give useful results on a broader range of manifolds <span class="citation">[<a href="#ref-think_globally">13</a>]</span>.</p>
<p>For dimensional reduction, most methods introduced before LLE need to estimate pairwise distances between even two remote data points, no matter it is the simple Euclidean distance (classical MDS) or more sophisticated manifold distance (ISOMAP). The underlying main idea of these methods is actually finding a configuration that recovers all pairwise distances of original data points as much as possible. LLE, however, is quite different from these previous methods as it focuses on preserving <strong>locally linear relationships</strong>.</p>
</div>
<div id="algorithm-1" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Algorithm<a href="locally-linear-embeddings-lles.html#algorithm-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LLE algorithm is actually built on very simple geometric intuitions. As explained in MDS Part, if we consider a small enough region on a manifold in <span class="math inline">\(D\)</span> dimensional space, in most cases, it can be regarded as a <span class="math inline">\(d\)</span> dimensional hyperplane (<span class="math inline">\(d \ll D\)</span>). LLE also makes use of this intuition and assumes that the whole manifold consist of numerous <span class="math inline">\(d\)</span>-dimensional patches that have been stitched together. Assuming that there exists sufficient data (data points are compact), it is reasonable to expect that each data point and its neighbors lie on or close to a locally linear patch of the manifold.</p>
<p>Following this idea, LLE approximates each data point by a weighted linear combination of its neighbors and proceeds to find a lower-dimensional configuration of data points so that the linear approximations of all data points are best preserved.</p>
<p>Specifically speaking, LLE algorithm consists of three steps. The initial step involves selecting a certain number of each data point’s nearest neighbors based on Euclidean distance. Following this, the second step calculates the optimal reconstruction weights for each point using its nearest neighbors. The final step carries out the embedding while maintaining the local geometry depicted by the reconstruction weights.</p>
<div id="construct-neighborhood-graph" class="section level4 hasAnchor" number="6.3.2.1">
<h4><span class="header-section-number">6.3.2.1</span> Construct Neighborhood Graph<a href="locally-linear-embeddings-lles.html#construct-neighborhood-graph" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This step is actually very similar to that of ISOMAP. The process of finding neighbors in LLE is typically conducted using grouping methods like k-nearest neighbors (KNN) or selecting neighbors within a fixed radius ball (<span class="math inline">\(\epsilon\)</span>-neighborhoods), based on the Euclidean distance for each data point, in the provided data set. The KNN method is predominantly utilized for its straightforwardness and ease of implementation. The following explanations are based on KNN method.</p>
<p>Denote <span class="math inline">\(N\)</span> data points in original <span class="math inline">\(D\)</span> dimensional space as <span class="math inline">\(\vec{x}_1, \vec{x}_2, \dots, \vec{x}_N \in \mathbb{R}^D\)</span>. For a point <span class="math inline">\(\vec{x}_i, \quad 1 \leq i \leq N\)</span>, its neighbor set is defined as <span class="math inline">\(N_i^k \subseteq \{1, 2, 3, \dots, i-1, i+1, \dots, N \}\)</span>, where <span class="math inline">\(N_i^k\)</span> can also be called as the indices of <span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(\vec{x}_i\)</span>. The tuning parameter <span class="math inline">\(k\)</span> is chosen small enough so that the patch around <span class="math inline">\(\vec{x}_i\)</span> is flat. However, <span class="math inline">\(k\)</span> should also be strictly larger than <span class="math inline">\(d\)</span> so as to let the algorithm work.</p>
<p>As we can tell from these, LLE works well only if data points are dense and hopefully evenly distributed, which will be explained in detail in later examples. The parameter tuning of the appropriate number of neighbors, <span class="math inline">\(k\)</span>, faces challenges of complexity, non-linearity, and diversity of high-dimensional input samples. A larger <span class="math inline">\(k\)</span> value might cause the algorithm to overlook or even lose the local nonlinear features on the manifold. This issue is exacerbated as neighbor selection, typically based on Euclidean distance, can result in distant neighbors when considering the intrinsic geometry of the data, akin to a short circuit. Conversely, an overly small <span class="math inline">\(k\)</span> value may lead the LLE algorithm to fragment the continuous manifold into isolated local pieces, losing global characteristics.</p>
</div>
<div id="reconsruct-with-linear-weights" class="section level4 hasAnchor" number="6.3.2.2">
<h4><span class="header-section-number">6.3.2.2</span> Reconsruct with Linear Weights<a href="locally-linear-embeddings-lles.html#reconsruct-with-linear-weights" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As put before, we try to reconstruct each <span class="math inline">\(\vec{x}_i\)</span> using an almost convex weighted combination of its neighbors. The respective weights of all its neighbors <span class="math inline">\(\vec{x}_j, \; j \neq i\)</span> for each <span class="math inline">\(\vec{x}_i\)</span> is quite essential in the later reconstruction of the underlying intrinsic configuration, as we consider these weights to remain invariant before and after mapping.</p>
<p>To explain it in mathematical formulas, the approximate of <span class="math inline">\(\vec{x}_i\)</span>: <span class="math inline">\(\tilde{x}_i\)</span> is defined as <span class="math inline">\(\tilde{x}_i = \sum_{j=1}^N w_{ij} \vec{x}_j\)</span>. There are two constraints for this formula: First, <span class="math inline">\(w_{ij} \equiv 0\)</span>, if <span class="math inline">\(j \notin N_i^k\)</span> (consistent with the assumption of <span class="math inline">\(k\)</span> nearest neighbors); Second, the sum of weights for each <span class="math inline">\(\vec{x}_i\)</span> is always zero, i.e., <span class="math inline">\(\sum_{j=1}^N w_{ij}=1\)</span>.</p>
<p>Then, the problem of finding the optimal <span class="math inline">\(w_{ij}, \; 1 \leq i,j \leq N\)</span> is equivalent to solving the following constrained Least Squares problem for <span class="math inline">\(\forall 1 \leq i \leq N\)</span>:</p>
<p><span id="eq:opt_prob"> (1) </span>
<span class="math display">\[
\begin{aligned}
&amp; \min \left\| \vec{x}_i-\sum_{j \in N_i^k} w_{i j} \vec{x}_j\right\|^2 \\
&amp; \text { s.t. } \quad \sum_{j \in N_i^k} w_{i j}=1 .
\end{aligned}
\]</span></p>
<p>It is worth noting that the weights can be negative theoretically, though in practice, we don’t expect that to happen.</p>
<p><strong>Invariance to Rotation, Rescaling and Transaction</strong></p>
<p>Define <span class="math inline">\(\epsilon(w) = \sum_{i=1}^N \left\| \vec{x}_i-\sum_{j \in N_i^k} w_{i j} \vec{x}_j\right\|^2\)</span>, which is the cost function.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\epsilon(w)\)</span> is unchanged by rotation or rescaling by common factor</li>
</ol>
<p>Actually <span class="math inline">\(\sum_{i=1}^N \left\| a \text{U} \vec{x}_i-\sum_{j \in N_i^k} w_{i j} a \text{U} \vec{x}_j\right\|^2 = a^2 \epsilon(w)\)</span>, where <span class="math inline">\(a\)</span> is a non-zero scaler and <span class="math inline">\(\text{U}\)</span> is an orthonormal matrix.</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(\epsilon(w)\)</span> is unchanged by transactions</li>
</ol>
<p>Thanks to the constraint that <span class="math inline">\(\sum_{j=1}^N w_{ij}=1\)</span>, for any transaction <span class="math inline">\(\vec{x}_i \rightarrow \vec{x}_i + \vec{y}\)</span>, the cost function does not change.</p>
<p><span class="math display">\[\sum_{i=1}^N \left\| (\vec{x}_i + \vec{y}) -\sum_{j \in N_i^k} w_{i j} (\vec{x}_j + \vec{y}) \right\|^2 = \sum_{i=1}^N \left\| \vec{x}_i-\sum_{j \in N_i^k} w_{i j} \vec{x}_j\right\|^2 = \epsilon(w)\]</span></p>
<p>From the expressions, we develop a strategy that optimizes one row of matrix <span class="math inline">\(w\)</span> at a time. Now let’s try to rewrite <span class="math inline">\(\epsilon(\vec{w}_i)=\left\| \vec{x}_i-\sum_{j \in N_i^k} w_{i j} \vec{x}_j\right\|^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
\epsilon(\vec{w}_i) &amp;= \left\| \vec{x}_i-\sum_{j \in N_i^k} w_{i j} \vec{x}_j\right\|^2 \\
&amp; = \left[ \sum_{j=1}^N w_{ij} (\vec{x}_i - \vec{w}_j) \right]^T \left[ \sum_{l=1}^N w_{il} (\vec{x}_i - \vec{w}_l) \right]^T \\
&amp; = \sum_{j=1}^N \sum_{l=1}^N w_{ij} w_{il} (\vec{x}_i -\vec{x}_j)^T (\vec{x}_i - \vec{x}_l) \\
&amp; = \vec{w}_i^T G_i \vec{w}_i
\end{align}\]</span></p>
<p><span class="math inline">\(\vec{w}_i^T = (w_{i1}, w_{i2}, \dots w_{iN})\)</span> is the <span class="math inline">\(i^{th}\)</span> row of W. Here <span class="math inline">\(G_i \in \mathbb{R}^{N \times N}\)</span>, where entry <span class="math inline">\(G_{i}(j.l), \; 1 \leq j,l \leq N\)</span> can be represented as:</p>
<p><span class="math display">\[
G_{i}(j,l) =
\begin{cases}
(\vec{x}_i - \vec{x}_j)^T (\vec{x}_i - \vec{x}_l) &amp; j,l \in N_i^k \\
0 &amp; j \; or \; l \notin N_i^k
\end{cases}
\]</span></p>
<p>The <span class="math inline">\((j,l)\)</span> entry of <span class="math inline">\(G_i\)</span> is actually the inner product of <span class="math inline">\(\vec{x}_j\)</span> and <span class="math inline">\(\vec{x}_l\)</span> when centered around <span class="math inline">\(\vec{x}_i\)</span>. From this expression, we know that actually <span class="math inline">\(G_i\)</span> is a sparse matrix and can be reduced to a compact matrix <span class="math inline">\(\tilde{G}_i \in \mathbb{R}^{k \times k}\)</span> that eliminates those empty columns and rows.</p>
<p><span class="math display">\[\begin{align}
\tilde{G}_i &amp; = (\vec{x}_{i[1]} - \vec{x}_i, \dots, \vec{x}_{i[k]} - \vec{x}_i)^T (\vec{x}_{i[1]} - \vec{x}_i, \dots, \vec{x}_{i[k]} - \vec{x}_i) \\
&amp; = Q_i^T Q_i
\end{align}\]</span></p>
<p>where <span class="math inline">\([1]\)</span> denotes the first entry in <span class="math inline">\(N_i^k\)</span>. So <span class="math inline">\(\tilde{G}_i\)</span> is actually a real symmetric and positive semi-definite matrix.</p>
<p>Now let’s go back to deal with the optimization function — Equation <a href="locally-linear-embeddings-lles.html#eq:opt_prob">1</a> can be solved with Lagrange multiplier given that it has only equality constraints. (More details about the use of Lagrange multiplier can be found in [Lagrange multiplier]<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier" class="uri">https://en.wikipedia.org/wiki/Lagrange_multiplier</a>)</p>
<p>Optimizing Equation <a href="locally-linear-embeddings-lles.html#eq:opt_prob">1</a> is equivalent to minimizing (for <span class="math inline">\(\forall 1 \leq i \leq N\)</span>)
<span class="math display">\[
f(\vec{w}_i, \lambda) = \vec{w}^T_i G_i \vec{w}_i - \lambda (\vec{w}^T_i \mathbf{1}_k -1)
\]</span>
which has the result:
<span class="math display">\[
\vec{w}_i^{\star} = \frac{\tilde{G}^{-}_i \mathbf{1}_k}{\mathbf{1}^T_k \tilde{G}^{-}_i \mathbf{1}_k}
\]</span></p>
<p><strong>Complement:</strong></p>
<p>As discussed before, we can only proof that <span class="math inline">\(\tilde{G}_i\)</span> is positive semi-definite, however, we cannot ensure that it is positive definite, which means <span class="math inline">\(\tilde{G}_i\)</span> is not necessarily invertible. That is why we use the generalize inverse sign here. In practice, it can be done through performing SVD on <span class="math inline">\(\tilde{G}_i\)</span> and select the first few large singular values and eliminate the rest. Then when computing <span class="math inline">\(\tilde{G}^{-}_i\)</span>, just do the reciprocal of these retained singular values.</p>
<p>Actually <span class="math inline">\(\tilde{G}_i\)</span> only has <span class="math inline">\(d\)</span> (the intrinsic original dimension if you forget) relatively large eigenvalues. The rest are either very small or zeros. So it is very likely that <span class="math inline">\(\tilde{G}_i\)</span> is singular, making the computation result highly unstable. In {<span class="citation">[<a href="#ref-think_globally">13</a>]</span>}, the authors proposed to address this issue through regularizing <span class="math inline">\(\tilde{G}_i\)</span>.</p>
<p><span class="math display">\[
\tilde{G}_i \leftarrow \tilde{G}_i+ \left(\frac{\Delta^2}{k}\right) \operatorname{Tr}\left(\tilde{G}_i\right) \mathbf{I}
\]</span>
Here <span class="math inline">\(Tr(\tilde{G}_i)\)</span> denotes the trace of <span class="math inline">\(\tilde{G}_i\)</span> and <span class="math inline">\(\Delta \ll 1\)</span>.</p>
</div>
<div id="embedding" class="section level4 hasAnchor" number="6.3.2.3">
<h4><span class="header-section-number">6.3.2.3</span> Embedding<a href="locally-linear-embeddings-lles.html#embedding" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the previous step, we have recovered the optimal weight matrix</p>
<p><span class="math display">\[
\mathbf{W} =
\left(\begin{array}{l}
\vec{w}_1^T \\
\vdots \\
\vec{w}_N^T
\end{array}\right)
\]</span></p>
<p>The optimal weights <span class="math inline">\(\mathbb{W}\)</span> reflects local, linear geometry around each <span class="math inline">\(\vec{x}_i\)</span>, thus if the configuration <span class="math inline">\(\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N \in \mathbb{R}^d\)</span> are the lower dimensional representation, they should also “match” the local geometry.</p>
<p>Following this idea, the objective function is:</p>
<p><span id="eq:embedding"> (2) </span>
<span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{Y}}{\text{argmin}} \sum_{i=1}^N \left\| \vec{y}_i - \sum_{j=1}^{N} w_{ij} \vec{y}_j^T \right\|^2 \\
= &amp; \underset{\mathbf{Y}}{\text{argmin}} \sum_{i=1}^N \left\| \sum_{j=1}^N w_{ij} (\vec{y}_i -  \vec{y}_j)^T \right\|^2 \\
= &amp; \underset{\mathbf{Y}}{\text{argmin}} \left\| \mathbf{Y - WY} \right\|_F^2 \\
= &amp; \underset{\mathbf{Y}}{\text{argmin}} \left\| (\mathbf{I}_N - \mathbf{W}) \mathbf{Y} \right\|_F^2 \\
= &amp; \underset{\mathbf{Y}}{\text{argmin}} \; \text{Tr} \left[ \mathbf{Y}^T (\mathbf{I}_N - \mathbf{W})^T (\mathbf{I}_N - \mathbf{W}) \mathbf{Y} \right]
\end{aligned}
\]</span>
where <span class="math inline">\(\mathbf{Y}=(\vec{y}_1 | \vec{y}_2 | \dots | \vec{y}_N)^T\)</span></p>
<p>There are two constraints:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbf{1}_N^T \mathbf{Y} = \vec{0}\)</span>. This forces <span class="math inline">\(\vec{y}\)</span> to be centered</p></li>
<li><p><span class="math inline">\(\frac{1}{N} \mathbf{Y}^T \mathbf{Y} = \mathbf{I}_d\)</span>. This fixes rotation and scaling.</p></li>
</ol>
<p><strong>Key Observation</strong></p>
<p>Considering the final expression of Equation <a href="locally-linear-embeddings-lles.html#eq:embedding">2</a>, the optimization function is now equivalent to finding <span class="math inline">\(\vec{y}_i\)</span>s that minimizes <span class="math inline">\(\mathbf{Y}^T (\mathbf{I}_N - \mathbf{W})^T (\mathbf{I}_N - \mathbf{W}) \mathbf{Y}\)</span>.</p>
<p>Here we introduce <span class="math inline">\(\mathbf{M} = (\mathbf{I}_N - \mathbf{W})^T (\mathbf{I}_N - \mathbf{W})\)</span>, which is a positive semi-definite matrix. Since <span class="math inline">\(\mathbf{M} \mathbf{1}_N = (\mathbf{I} - \mathbf{W})^T (\mathbf{1}_N - \mathbf{W} \mathbf{1}_N) = \vec{0}\)</span>, <span class="math inline">\(\mathbf{1}_N\)</span> is an eigen-vector of <span class="math inline">\(\mathbf{M}\)</span> with eigenvalue zero.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y}^T (\mathbf{I}_N - \mathbf{W})^T (\mathbf{I}_N - \mathbf{W}) \mathbf{Y}
= \mathbf{Y}^T \mathbf{M} \mathbf{Y}
\end{aligned}
\]</span>
From constraint (b), we know that columns of <span class="math inline">\(\mathbf{Y}\)</span> are orthogonal to each other. As a result, this whole problem can be simplified to finding the eigen-vectors of <span class="math inline">\(\mathbf{M}\)</span> with the smallest eigenvalues.</p>
<p>Compute eigen-vectors with the smallest <span class="math inline">\(d+1\)</span> eigenvalues <span class="math inline">\(0=\lambda_1 \leq \lambda_2 &lt; \dots &lt; \lambda_{d+1}\)</span>, eliminate <span class="math inline">\(\mathbf{1}_N\)</span> (the first one). The remaining <span class="math inline">\(d\)</span> vectors are respectively <span class="math inline">\(\vec{v}_2, \vec{v}_3, \dots \vec{v}_{d+1} \in \mathbb{R}^N\)</span>. So <span class="math inline">\(\mathbf{Y} = (\vec{v}_2 | \vec{v}_2 | \dots | \vec{v}_{d+1})\)</span>, we successfully recover the corresponding <span class="math inline">\(\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N \in \mathbb{R}^d\)</span>.</p>
<p><strong>An illustration of the algorithm</strong></p>
<p>In the original paper <span class="citation">[<a href="#ref-lle">11</a>]</span>, the authors provide a very intuitive plot that summarizes the above three steps.</p>
<p style="text-align:center;">
<div class="float">
<img src="picture/Screenshot%202023-09-27%20at%2000.47.49.png" style="width:40.0%;height:40.0%" alt="LLE_illustration" />
<div class="figcaption">LLE_illustration</div>
</div>
</p>
<p><strong>Parameter Tuning</strong></p>
<p>There are two parameters to tune in LLE, i.e. (the number of neighbors: <span class="math inline">\(k \,\)</span>; the dimension of the recovered configuration: <span class="math inline">\(d \,\)</span>).</p>
<ol style="list-style-type: decimal">
<li><p>For selection of <span class="math inline">\(d\)</span>, we usually use a reverse scree plot and find the elbow point. It is worth noting that we are choosing the smallest <span class="math inline">\(d+1\)</span> eigenvalues and compute their corresponding eigen-vectors here. Since the eigen-vectors and eigenvalues of a particular matrix is super sensitive to any sort of noises or perturbations, especially for those small eigenvalues, it is hard to accurately derive the corresponding eigen-vectors <span class="math inline">\(\vec{v}_2, \dots, \vec{v}_{d+1}\)</span>. This is called ill-conditioned eigen-problem.</p></li>
<li><p>Choose the optimal <span class="math inline">\(k\)</span></p></li>
</ol>
<p>LLE seeks to preserve local structure through nearest neighbor connections. This is the key point to LLE. As a result, we may use the neighbor set of the original <span class="math inline">\(\vec{x}_1, \vec{x}_2, \dots, \vec{x}_N \in \mathbb{R}^D\)</span> and <span class="math inline">\(\vec{y}_1, \vec{y}_2, \dots, \vec{y}_N \in \mathbb{R}^d\)</span> as a criteria.</p>
<p>As explained before, we use <span class="math inline">\(N_i^k\)</span> to denote the indices of k-nearest neighbors to <span class="math inline">\(\vec{x}_i\)</span>. Similarly, we can also use <span class="math inline">\(V_i^k\)</span> to denote the indices of k-nearest neighbors to <span class="math inline">\(\vec{y}_i\)</span>. They should be as close as possible.</p>
<p>So our objective function here is:
<span class="math display">\[
Q(k)= \frac{\sum_{i=1}^N \left| N_i^k \cap V_i^k \right|}{Nk}
\]</span>
Plot <span class="math inline">\(Q(k)\)</span> against <span class="math inline">\(k\)</span>, select <span class="math inline">\(k^{\star}\)</span> where the increase of <span class="math inline">\(Q(k)\)</span> becomes negligible.</p>
</div>
</div>
<div id="strengths-and-weaknesses-of-lle" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Strengths and Weaknesses of LLE<a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="strengths" class="section level4 hasAnchor" number="6.3.3.1">
<h4><span class="header-section-number">6.3.3.1</span> Strengths<a href="locally-linear-embeddings-lles.html#strengths" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>High Computation Efficiency</strong></li>
</ol>
<p>The low computation cost of LLE algorithm may be its most shinning advantage over other manifold learning methods, and it is actually one of its biggest selling point when it was first introduced. The LLE algorithm Involves solving a sparse eigen problem, with computational complexity of roughly <span class="math inline">\(O(N^2 d^2 + N d^3)\)</span> where <span class="math inline">\(N\)</span> is the number of data points and <span class="math inline">\(d\)</span> is the dimension of the recovered configuration.</p>
<p>In comparison, ISOMAP requires computing shortest paths between all pairs of points, which is typically done using Dijkstra’s or Floyd-Warshall algorithm, leading to a complexity of <span class="math inline">\(O(N^2 log N)\)</span> or <span class="math inline">\(O(N^3)\)</span> respectively. Then, it involves eigen decomposition similar to classical MDS which is <span class="math inline">\(O(N^3)\)</span>.</p>
<p>In practice, <span class="math inline">\(d \ll N\)</span>, hence the computation cost of LLE is lower than that of ISOMAP in most cases.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Few parameters to tune</strong></li>
</ol>
<p>There are only two parameters to tune, respectively the number of neighbors included in the map: <span class="math inline">\(k\)</span>, and the dimensional of the original configuration: <span class="math inline">\(d\)</span>. In addition, there exist clear methods to find the optimal <span class="math inline">\(k\)</span> and <span class="math inline">\(d\)</span>, as stated in the previous part. This makes LLE algorithm easy to find the optimal parameters.</p>
</div>
<div id="weaknesses" class="section level4 hasAnchor" number="6.3.3.2">
<h4><span class="header-section-number">6.3.3.2</span> Weaknesses<a href="locally-linear-embeddings-lles.html#weaknesses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Sensitivity to tuning parameters</strong></li>
</ol>
<p>The result of LLE is quite sensitive to its two control parameters: the number of neighbors <span class="math inline">\(k \,\)</span> and the dimensional of the original configuration: <span class="math inline">\(d\)</span>.</p>
<p>Here we use the Swiss Roll example to illustrate this. LLE is optimal at <span class="math inline">\(k=45\)</span>. However, when <span class="math inline">\(k=40\)</span>, the recovered lower-dimensional configuration is wrong (Green points and yellow points overlap, which is not the case in Swiss Roll); and when we slightly increase <span class="math inline">\(k\)</span> to 50, the recovered two-dimensional expression is not necessarily a rectangle.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="locally-linear-embeddings-lles.html#cb26-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb26-2"><a href="locally-linear-embeddings-lles.html#cb26-2" tabindex="-1"></a><span class="co"># Install and load necessary packages</span></span>
<span id="cb26-3"><a href="locally-linear-embeddings-lles.html#cb26-3" tabindex="-1"></a><span class="co"># install.packages(&quot;dimRed&quot;)</span></span>
<span id="cb26-4"><a href="locally-linear-embeddings-lles.html#cb26-4" tabindex="-1"></a><span class="co"># install.packages(&quot;rgl&quot;)</span></span>
<span id="cb26-5"><a href="locally-linear-embeddings-lles.html#cb26-5" tabindex="-1"></a><span class="fu">library</span>(dimRed)</span>
<span id="cb26-6"><a href="locally-linear-embeddings-lles.html#cb26-6" tabindex="-1"></a><span class="fu">library</span>(rgl)</span>
<span id="cb26-7"><a href="locally-linear-embeddings-lles.html#cb26-7" tabindex="-1"></a><span class="fu">library</span>(Rdimtools)</span>
<span id="cb26-8"><a href="locally-linear-embeddings-lles.html#cb26-8" tabindex="-1"></a><span class="fu">library</span>(scatterplot3d)</span>
<span id="cb26-9"><a href="locally-linear-embeddings-lles.html#cb26-9" tabindex="-1"></a></span>
<span id="cb26-10"><a href="locally-linear-embeddings-lles.html#cb26-10" tabindex="-1"></a><span class="co"># Generate Swiss roll-shaped data</span></span>
<span id="cb26-11"><a href="locally-linear-embeddings-lles.html#cb26-11" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb26-12"><a href="locally-linear-embeddings-lles.html#cb26-12" tabindex="-1"></a>Swiss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">2000</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb26-13"><a href="locally-linear-embeddings-lles.html#cb26-13" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>) {</span>
<span id="cb26-14"><a href="locally-linear-embeddings-lles.html#cb26-14" tabindex="-1"></a>  s <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">3</span> <span class="sc">*</span> pi <span class="sc">/</span> <span class="dv">2</span>, <span class="at">max =</span> <span class="dv">9</span> <span class="sc">*</span> pi <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb26-15"><a href="locally-linear-embeddings-lles.html#cb26-15" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">20</span>)</span>
<span id="cb26-16"><a href="locally-linear-embeddings-lles.html#cb26-16" tabindex="-1"></a>  S[n] <span class="ot">&lt;-</span> s</span>
<span id="cb26-17"><a href="locally-linear-embeddings-lles.html#cb26-17" tabindex="-1"></a>  Swiss[n, ] <span class="ot">&lt;-</span> <span class="fu">c</span>(s <span class="sc">*</span> <span class="fu">cos</span>(s), t, s <span class="sc">*</span> <span class="fu">sin</span>(s))</span>
<span id="cb26-18"><a href="locally-linear-embeddings-lles.html#cb26-18" tabindex="-1"></a>}</span>
<span id="cb26-19"><a href="locally-linear-embeddings-lles.html#cb26-19" tabindex="-1"></a></span>
<span id="cb26-20"><a href="locally-linear-embeddings-lles.html#cb26-20" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb26-21"><a href="locally-linear-embeddings-lles.html#cb26-21" tabindex="-1"></a><span class="fu">scatterplot3d</span>(Swiss, <span class="at">color =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S),</span>
<span id="cb26-22"><a href="locally-linear-embeddings-lles.html#cb26-22" tabindex="-1"></a>               <span class="at">xlab =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>]), <span class="at">zlab =</span> <span class="fu">expression</span>(x[<span class="dv">3</span>]))</span>
<span id="cb26-23"><a href="locally-linear-embeddings-lles.html#cb26-23" tabindex="-1"></a></span>
<span id="cb26-24"><a href="locally-linear-embeddings-lles.html#cb26-24" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">40</span>))<span class="sc">$</span>Y,</span>
<span id="cb26-25"><a href="locally-linear-embeddings-lles.html#cb26-25" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=40&quot;</span>,</span>
<span id="cb26-26"><a href="locally-linear-embeddings-lles.html#cb26-26" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb26-27"><a href="locally-linear-embeddings-lles.html#cb26-27" tabindex="-1"></a></span>
<span id="cb26-28"><a href="locally-linear-embeddings-lles.html#cb26-28" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">45</span>))<span class="sc">$</span>Y,</span>
<span id="cb26-29"><a href="locally-linear-embeddings-lles.html#cb26-29" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=45&quot;</span>,</span>
<span id="cb26-30"><a href="locally-linear-embeddings-lles.html#cb26-30" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb26-31"><a href="locally-linear-embeddings-lles.html#cb26-31" tabindex="-1"></a></span>
<span id="cb26-32"><a href="locally-linear-embeddings-lles.html#cb26-32" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">50</span>))<span class="sc">$</span>Y,</span>
<span id="cb26-33"><a href="locally-linear-embeddings-lles.html#cb26-33" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=50&quot;</span>,</span>
<span id="cb26-34"><a href="locally-linear-embeddings-lles.html#cb26-34" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Vulnerable to sparse or unevenly-distributed samples</strong></li>
</ol>
<p>The vulnerability towards sparsity and uneven distribution exists in almost all manifold learning methods, including ISOMAP, as we have illustrated in the previous section. LLE is not immune to this either. When a data set is unevenly distributed, since LLE relies on the original Euclidean distance metric, it tends to select neighbors from a singular direction where these neighbors are densely clustered. Clearly, using these selected neighbors to reconstruct the reference point results in significant redundancy in that specific direction. Concurrently, essential information from other directions or regions is not retained for the reconstruction of the reference point. As a result, these selected neighbors are inadequate for accurately representing and reconstructing the reference point. Consequently, much of the intrinsic structure and internal features will be lost after dimension reduction using LLE.</p>
<p>We borrow the “stretched” Swiss Roll example from ISOMAP section and try LLE this time. LLE also cracks in this scenario, regardless of the chosen parameter <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="locally-linear-embeddings-lles.html#cb27-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb27-2"><a href="locally-linear-embeddings-lles.html#cb27-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2000</span>)</span>
<span id="cb27-3"><a href="locally-linear-embeddings-lles.html#cb27-3" tabindex="-1"></a>Swiss_sparse <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">2000</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb27-4"><a href="locally-linear-embeddings-lles.html#cb27-4" tabindex="-1"></a><span class="cf">for</span>( n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>){</span>
<span id="cb27-5"><a href="locally-linear-embeddings-lles.html#cb27-5" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">3</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">9</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb27-6"><a href="locally-linear-embeddings-lles.html#cb27-6" tabindex="-1"></a>    t <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>,      <span class="at">max =</span> <span class="dv">60</span>)</span>
<span id="cb27-7"><a href="locally-linear-embeddings-lles.html#cb27-7" tabindex="-1"></a>    S[n] <span class="ot">&lt;-</span> s</span>
<span id="cb27-8"><a href="locally-linear-embeddings-lles.html#cb27-8" tabindex="-1"></a>    Swiss_sparse[n, ] <span class="ot">&lt;-</span> <span class="fu">c</span>( s<span class="sc">*</span><span class="fu">cos</span>(s), t, s<span class="sc">*</span><span class="fu">sin</span>(s) )</span>
<span id="cb27-9"><a href="locally-linear-embeddings-lles.html#cb27-9" tabindex="-1"></a>}</span>
<span id="cb27-10"><a href="locally-linear-embeddings-lles.html#cb27-10" tabindex="-1"></a></span>
<span id="cb27-11"><a href="locally-linear-embeddings-lles.html#cb27-11" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb27-12"><a href="locally-linear-embeddings-lles.html#cb27-12" tabindex="-1"></a><span class="fu">scatterplot3d</span>(Swiss_sparse, <span class="at">color =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S), <span class="at">main =</span> <span class="st">&quot;Swiss Roll with Sparse Region&quot;</span>,</span>
<span id="cb27-13"><a href="locally-linear-embeddings-lles.html#cb27-13" tabindex="-1"></a>               <span class="at">xlab =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>]), <span class="at">zlab =</span> <span class="fu">expression</span>(x[<span class="dv">3</span>]))</span>
<span id="cb27-14"><a href="locally-linear-embeddings-lles.html#cb27-14" tabindex="-1"></a></span>
<span id="cb27-15"><a href="locally-linear-embeddings-lles.html#cb27-15" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_sparse, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">35</span>))<span class="sc">$</span>Y,</span>
<span id="cb27-16"><a href="locally-linear-embeddings-lles.html#cb27-16" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=35&quot;</span>,</span>
<span id="cb27-17"><a href="locally-linear-embeddings-lles.html#cb27-17" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb27-18"><a href="locally-linear-embeddings-lles.html#cb27-18" tabindex="-1"></a></span>
<span id="cb27-19"><a href="locally-linear-embeddings-lles.html#cb27-19" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_sparse, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">40</span>))<span class="sc">$</span>Y,</span>
<span id="cb27-20"><a href="locally-linear-embeddings-lles.html#cb27-20" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=40&quot;</span>,</span>
<span id="cb27-21"><a href="locally-linear-embeddings-lles.html#cb27-21" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb27-22"><a href="locally-linear-embeddings-lles.html#cb27-22" tabindex="-1"></a></span>
<span id="cb27-23"><a href="locally-linear-embeddings-lles.html#cb27-23" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_sparse, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">45</span>))<span class="sc">$</span>Y,</span>
<span id="cb27-24"><a href="locally-linear-embeddings-lles.html#cb27-24" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=45&quot;</span>,</span>
<span id="cb27-25"><a href="locally-linear-embeddings-lles.html#cb27-25" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>In addition, we manually create a Swiss Roll with some sparse regions and try LLE on it. From naked eye, these sparse regions won’t affect the overall manifold look. However, since LLE utilizes the local structure, it is greatly affected.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="locally-linear-embeddings-lles.html#cb28-1" tabindex="-1"></a><span class="fu">library</span>(scatterplot3d)</span>
<span id="cb28-2"><a href="locally-linear-embeddings-lles.html#cb28-2" tabindex="-1"></a><span class="fu">library</span>(Rdimtools)</span>
<span id="cb28-3"><a href="locally-linear-embeddings-lles.html#cb28-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb28-4"><a href="locally-linear-embeddings-lles.html#cb28-4" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2000</span>)</span>
<span id="cb28-5"><a href="locally-linear-embeddings-lles.html#cb28-5" tabindex="-1"></a>Swiss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="dv">2000</span>, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb28-6"><a href="locally-linear-embeddings-lles.html#cb28-6" tabindex="-1"></a><span class="cf">for</span>(n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>){</span>
<span id="cb28-7"><a href="locally-linear-embeddings-lles.html#cb28-7" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">3</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">9</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb28-8"><a href="locally-linear-embeddings-lles.html#cb28-8" tabindex="-1"></a>    t <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>,      <span class="at">max =</span> <span class="dv">15</span>)</span>
<span id="cb28-9"><a href="locally-linear-embeddings-lles.html#cb28-9" tabindex="-1"></a>    S[n] <span class="ot">&lt;-</span> s</span>
<span id="cb28-10"><a href="locally-linear-embeddings-lles.html#cb28-10" tabindex="-1"></a>    Swiss[n, ] <span class="ot">&lt;-</span> <span class="fu">c</span>( s<span class="sc">*</span><span class="fu">cos</span>(s), t, s<span class="sc">*</span><span class="fu">sin</span>(s) )</span>
<span id="cb28-11"><a href="locally-linear-embeddings-lles.html#cb28-11" tabindex="-1"></a>}</span>
<span id="cb28-12"><a href="locally-linear-embeddings-lles.html#cb28-12" tabindex="-1"></a></span>
<span id="cb28-13"><a href="locally-linear-embeddings-lles.html#cb28-13" tabindex="-1"></a><span class="co"># Manually create a sparse region</span></span>
<span id="cb28-14"><a href="locally-linear-embeddings-lles.html#cb28-14" tabindex="-1"></a>mask <span class="ot">&lt;-</span> Swiss[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="dv">5</span> <span class="sc">&amp;</span> Swiss[,<span class="dv">2</span>] <span class="sc">&lt;</span> <span class="dv">10</span> <span class="sc">&amp;</span> Swiss[,<span class="dv">3</span>] <span class="sc">&gt;</span> <span class="sc">-</span><span class="dv">5</span> <span class="sc">&amp;</span> Swiss[,<span class="dv">3</span>] <span class="sc">&lt;</span> <span class="dv">5</span></span>
<span id="cb28-15"><a href="locally-linear-embeddings-lles.html#cb28-15" tabindex="-1"></a>Swiss_masked <span class="ot">&lt;-</span> Swiss[<span class="sc">!</span>mask, ]</span>
<span id="cb28-16"><a href="locally-linear-embeddings-lles.html#cb28-16" tabindex="-1"></a>S <span class="ot">&lt;-</span> S[<span class="sc">!</span>mask]</span>
<span id="cb28-17"><a href="locally-linear-embeddings-lles.html#cb28-17" tabindex="-1"></a></span>
<span id="cb28-18"><a href="locally-linear-embeddings-lles.html#cb28-18" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb28-19"><a href="locally-linear-embeddings-lles.html#cb28-19" tabindex="-1"></a><span class="fu">scatterplot3d</span>(Swiss_masked, <span class="at">color =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S), <span class="at">main =</span> <span class="st">&quot;Swiss Roll with Sparse Region&quot;</span>,</span>
<span id="cb28-20"><a href="locally-linear-embeddings-lles.html#cb28-20" tabindex="-1"></a>               <span class="at">xlab =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>]), <span class="at">zlab =</span> <span class="fu">expression</span>(x[<span class="dv">3</span>]))</span>
<span id="cb28-21"><a href="locally-linear-embeddings-lles.html#cb28-21" tabindex="-1"></a></span>
<span id="cb28-22"><a href="locally-linear-embeddings-lles.html#cb28-22" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_masked, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">20</span>))<span class="sc">$</span>Y,</span>
<span id="cb28-23"><a href="locally-linear-embeddings-lles.html#cb28-23" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=20&quot;</span>,</span>
<span id="cb28-24"><a href="locally-linear-embeddings-lles.html#cb28-24" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb28-25"><a href="locally-linear-embeddings-lles.html#cb28-25" tabindex="-1"></a></span>
<span id="cb28-26"><a href="locally-linear-embeddings-lles.html#cb28-26" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_masked, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">30</span>))<span class="sc">$</span>Y,</span>
<span id="cb28-27"><a href="locally-linear-embeddings-lles.html#cb28-27" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=30&quot;</span>,</span>
<span id="cb28-28"><a href="locally-linear-embeddings-lles.html#cb28-28" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span>
<span id="cb28-29"><a href="locally-linear-embeddings-lles.html#cb28-29" tabindex="-1"></a></span>
<span id="cb28-30"><a href="locally-linear-embeddings-lles.html#cb28-30" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">do.lle</span>(Swiss_masked, <span class="at">ndim =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;knn&quot;</span>,<span class="dv">50</span>))<span class="sc">$</span>Y,</span>
<span id="cb28-31"><a href="locally-linear-embeddings-lles.html#cb28-31" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;1st Dimension&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;2nd Dimension&quot;</span>, <span class="at">main =</span> <span class="st">&quot;K=50&quot;</span>,</span>
<span id="cb28-32"><a href="locally-linear-embeddings-lles.html#cb28-32" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">myColorRamp</span>(<span class="fu">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;yellow&quot;</span>), S))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Sensitivity to noise</strong></li>
</ol>
<p>LLE is super sensitive to noise. Even a small noise would cause failure in deriving low dimensional configurations. Justin Wang, et.al utilize various visualization examples to illustrate this drawback in their paper <span class="citation">[<a href="#ref-llenoise">14</a>]</span>, you may take a look if you are interested. Various algorithms have been developed to address this issue, i.e., Robustly Locally Linear Embedding (RLLE) <span class="citation">[<a href="#ref-RobustLLE">15</a>]</span>, and Locally Linear Embedding with Additive Noise (LLEAN) <span class="citation">[<a href="#ref-llenoise">14</a>]</span>. The former works well when outliers exist, while the latter has a satisfactory performance when the original points are distorted with noises.</p>
<!-- AE -->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-lle" class="csl-entry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline"><span class="smallcaps">Roweis</span>, S. T. and <span class="smallcaps">Saul</span>, L. K. (2000). <a href="https://doi.org/10.1126/science.290.5500.2323">Nonlinear dimensionality reduction by locally linear embedding</a>. <em>Science</em> <strong>290</strong> 2323–6.</div>
</div>
<div id="ref-lle_survey" class="csl-entry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline"><span class="smallcaps">Chen</span>, J. and <span class="smallcaps">Liu</span>, Y. (2011). <a href="https://doi.org/10.1007/s10462-010-9200-z">Locally linear embedding: A survey</a>. <em>Artif. Intell. Rev.</em> <strong>36</strong> 29–48.</div>
</div>
<div id="ref-think_globally" class="csl-entry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline"><span class="smallcaps">Saul</span>, L. K. and <span class="smallcaps">Roweis</span>, S. T. (2003). <a href="https://doi.org/10.1162/153244304322972667">Think globally, fit locally: Unsupervised learning of low dimensional manifolds</a>. <strong>4</strong> 119–55.</div>
</div>
<div id="ref-llenoise" class="csl-entry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline"><span class="smallcaps">Anon</span>. (2019). <a href="https://doi.org/10.1016/j.patrec.2019.02.030">Locally linear embedding with additive noise</a>. <em>Pattern Recognition Letters</em> <strong>123</strong> 47–52.</div>
</div>
<div id="ref-RobustLLE" class="csl-entry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline"><span class="smallcaps">Chang</span>, H. and <span class="smallcaps">Yeung</span>, D.-Y. (2006). <a href="https://doi.org/10.1016/j.patcog.2005.07.011">Robust locally linear embedding</a>. <em>Pattern Recognition</em> <strong>39</strong> 1053–65.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="isometric-feature-map-isomap.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="autoencoders-aes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-nonlinear_methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
