[["index.html", "An Introduction to Unsupervised Learning Chapter 1 Introduction 1.1 Prerequisites", " An Introduction to Unsupervised Learning Alex Young Version 0 Chapter 1 Introduction Unsupervised machine learning (UL) is an overarching term for methods designed to understand the patterns and relationships within a set of unlabeled data. UL is often discussed in contrast to (semi-)supervised learning. In the latter setting(s), one is primarly concerned with prediction and classification, and the machine learning algorithms therein focus on learning the relationship between a (often high dimensional) feature vector \\(\\vec{x}\\) and an observable outcome \\(y\\) by training on a labeled dataset \\(\\{(\\vec{x}_i,y_i)\\}_{i=1}^N\\). As a concrete example, consider the MNIST dataset which contains a compendium of labeled digitized grayscale images of handwritten digits [1]. The images themselves are the features, (\\(\\vec{x}\\)), and the identity of the digit (0 to 9) gives the label, \\(y\\) . A supervised learning algorithm trained on the MNIST dataset would be able to classify a handwritten digit in a new grayscale image. Prediction and classification are clearly defined goals which naturally translate to many settings. As such, supervised ML has found numerous applications across diverse fields of research from healthcare and medicine to astronomy and chemistry. Given the clearly translatable goals of supervised learning, most texts on Machine Learning tend to emphasize this setting with much smaller discussion on the un- or semi-supervised setting. For example, both The Elements of Statistical Learning by Hastie et al [2] and Modern Multivariate Statistical Techniques by Izenman [3] are wonderful texts – which were central to the early development of this book – but lean towards supervised problems. Unlike the supervised setting, however, UL algorithms are applied to datasets without (or ignoring) labels. In contrast to the MNIST example above, you can think of have of a case where one has access to a large collection of \\(\\vec{x}_i\\), such as images, without any labels indicating the content(s) of the image. Other examples include, a corpus of emails without any indication of which, if any, are spam genomic data for each individual in a large population of cancer patients collections of consumer data or ratings Without labels, it may be difficult (particularly to students first seeing this branch of machine learning) to grasp the usefulness of UL including its applicability and what one is learning in practice. In this book, we hope to address this difficulty and provide readers with a clear understanding of UL by covering motivating ideas, fundamental techniques, and clear and compelling applications. For now, we’ll discuss the high-altitude view of unsupervised learning. We’ll focus on those cases where we have a collection of independent observations of (preprocessed) features stored in vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N\\in\\mathbb{R}^d\\). This setting will be formalized in 2. Broadly, UL learns patterns and similarities between the vectors which could allow us to find subsets of the data which more similar to each other (clustering) or find simpler representations of the data which preserves important relationships (dimension reduction) identify common relationships between variables in the data (association rules) Each of three cases provide a simplified lens through which we can view our data, and in doing so, can open up a number of interesting possibilities. Clustering different genomic data in cancer patients could provide information to medical practitioners on which cancers exhibit common genetics signatures. Applying dimension reduction to spam emails could allow one to identify odd emails which might be spam (anomaly detection). Learning the common relationships between variables in a consumer data set opens the possibility of matching consumers which items they might enjoy (recommendation systems). The examples above are by no means exhaustive, but they do raise a few critical points. Where Each application above is one step in a larger data science problem. Unsupervised learning is rarely detached from a broader data science pipeline. This is a stark difference from classification and prediction which are often viewed as isolated statistical problems (though careful practitioners recognize that data collection and cleaning and the communication of results are often of equal or greater importance than analysis). Examples are provided throughout the text demonstrating where UL can be useful. What One data set could be approached from one or more different perspectives. For example, one could apply dimension reduction to the cancer data to visualize the potentially complex data in a manner that preserves important relationships. Combining many approaches together makes unsupervised learning a powerful tool to exploratory data analysis and featurization, particularly when combined with expert level content knowledge. Choosing a UL method is linked with what we hope to learn about our data. How If we want an algorithm that clusters similar vectors or provides a visualization that keeps close points together, then we should be mindful of the meaning of similarity or proximity. UL algorithms, sometimes implicitly, prioritize different relationships. We explore how these algorithms work from a geometric perspective which is a helpful intellectual scaffolding. In the remainder of this text, we focus primarily on dimension reduction (4 and 6) and clustering (7). 1.1 Prerequisites This text is targeted at upper level undergraduates with a well rounded background in the following courses and topics Probability: random variables, expectation, variance, and covariance Linear algebra: matrix-vector multiplication, linear spaces, eigendecompositions Multivariable calculus: gradients and basic optimization A brief review of the most important ideas is covered in Chapter 2. Additional tools and techniques needed for specific algorithms are covered at a cursory level as needed. References to more thorough discussions are provided throughout for the interested reader. References "],["ch-prob.html", "Chapter 2 Probability Review 2.1 Important notation 2.2 Random vectors in \\(\\mathbb{R}^d\\) 2.3 Expectation, Mean, and Covariance 2.4 Linear Algebra 2.5 Exercises", " Chapter 2 Probability Review 2.1 Important notation Throughout this text, we will be working with vectors and matrices quite often so we begin with a bit of notation and a few important conventions we will adopt hereafter. We’ll use notation \\(\\vec{x}\\in\\mathbb{R}^d\\) to denote a \\(d\\)-dimensional vector. Importantly, we adopt the convention that vectors are columns vectors by default so that \\[\\vec{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}\\] where \\(x_1,\\dots,x_d\\) are the entries or coordinates of vector \\(\\vec{x}\\). Row vectors are then the transpose of column vectors so that \\(\\vec{x}^T = (x_1,\\dots, x_d)\\). When needed we’ll let \\(\\vec{0}\\) denote a vector of all zeros and \\(\\vec{1}\\) a vector of all ones with the dimensionality defined implicitly, e.g. if \\(\\vec{x}\\in\\mathbb{R}^d\\) then in the expression \\(\\vec{x} + \\vec{1}\\), you may interpret \\(\\vec{1}\\in\\mathbb{R}^d\\) so the summation is well defined. Matrices will be denoted in bold so that \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\) denotes an \\(m\\times n\\) matrix with real entries. Subscripts are read as row,column so that \\({\\bf A}_{ij}\\) is the entry of \\({\\bf A}\\) in the \\(i\\)th row and \\(j\\)th column. A superscript \\(T\\) denotes the transpose of a matrix. For a square matrix \\({\\bf B}\\in \\mathbb{R}^{n\\times n}\\), we use notation \\(Tr({\\bf B})\\) to denote the trace of \\({\\bf B}\\) and \\(det({\\bf B}) = |{\\bf B}|\\) to denotes its determinant. Using this above notation, we may also define the inner product and outer product of two vectors. For vectors \\(\\vec{x}\\) and \\(\\vec{y}\\), the inner product or dot product of \\(\\vec{x}\\) and \\(\\vec{y}\\) is the scalar \\(\\vec{x}^T \\vec{y} = \\sum_{i=1}^d x_i y_i\\). Alternatively, we may also consider the outer product \\(\\vec{x}\\vec{y}^T\\) which is a matrix such that \\((\\vec{x} \\vec{y}^T)_{ij} = x_i y_j.\\) For the inner product to be well defined \\(\\vec{x}\\) and \\(\\vec{y}\\) must have the same dimension. This is not the case for the outer product. If \\(\\vec{x}\\in\\mathbb{R}^m\\) and \\(\\vec{y}\\in\\mathbb{R}^n\\) then \\(\\vec{x} \\vec{y}^T \\in \\mathbb{R}^{m\\times n}.\\) If we view a \\(d\\)-dimensional vector as a \\(d\\times 1\\) matrix, then both of these algebraic computations are completely consistent with standard matrix multiplication which we will revisit near the end of this chapter. Let \\(f:\\mathbb{R}^d\\to \\mathbb{R}\\) be a function of \\(d\\)-dimensional vector \\(\\vec{x}\\). Then we define the gradient of \\(f\\) with respect to \\(\\vec{x}\\), denoted \\(\\nabla f\\), to be the \\(d\\)-dimensional vector of partial deriviates of \\(f\\) with respect to the coordinates of \\(\\vec{x}\\) so that \\[\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{ \\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_d} \\end{bmatrix}.\\] The Hessian matrix of \\(f\\) with respect to \\(\\vec{x}\\), denoted as \\(\\mathcal{H}f\\), is the \\(d\\times d\\) matrix of second order partial derivatives of \\(f\\) with respect to the coordinates of \\(\\vec{x}\\) so that \\((\\mathcal{H}f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\). If we are considering a function of multiple vector valued variables, e.g. \\(f(\\vec{x},\\vec{y},\\vec{z})\\) then we use \\(\\nabla_{\\vec{x}}\\) to denote the gradient of \\(f\\) w.r.t. the vector \\(\\vec{x}\\) only. 2.2 Random vectors in \\(\\mathbb{R}^d\\) Throughout this text, we will consider independent, identically distributed (iid) samples of a \\(d\\)-dimensional random vector \\(\\vec{x} = (x_1,\\dots,x_d)^T\\). Each coordinate \\(x_i\\) is a random variable and we may view the distribution of the random vector \\(\\vec{x}\\) as the joint distribution of all of its coordinates. The cumulative distribution function of \\(\\vec{x}\\) is then \\[F(\\vec{x} \\le \\vec{x}^o\\,) = P(\\vec{x} \\le \\vec{x}^o\\,) = P(x_1\\le x_1^o, \\dots ,x_d \\le x_d^o\\,).\\] We’ll largely consider continuous entries so we can rewrite the above in terms of the joint density, \\(f: \\mathbb{R}^d \\to [0,\\infty)\\), of \\(\\vec{x}\\) such that \\[F(\\vec{x} \\le \\vec{x}^o\\,) = \\int_{-\\infty}^{x_1^o}\\dots\\int_{-\\infty}^{x_d^o} f(x_1,\\dots,x_d) dx_d\\dots dx_1.\\] To simplify notation, we’ll often write the above as \\[\\int_{-\\infty}^{x_1^o}\\dots\\int_{-\\infty}^{x_d^o} f(x_1,\\dots,x_d) dx_d\\dots dx_1 = \\int_{-\\infty}^{\\vec{x}^0} f(\\vec{x})d\\vec{x}.\\] In the case that \\(\\vec{x}\\) may only take one of a countable set of outcomes, one can replace the integral above with a corresponding summation. Generally speaking we will be considering data drawn from an unknown distribution. However, considering known cases which we can analyze and sample from is often helpful to study how different algorithms perform. With this idea in mind, let’s define a few different distributions which we will revisit throughout this chapter as examples. In each case, we will also provide scatterplots of independent samples from these distributions so that you can visualize the distributions more directly. Definition 2.1 (Multivariate Gaussian Distribution) The multivariate Gaussian distribution in \\(\\mathbb{R}^d\\) with mean \\(\\vec{\\mu} \\in \\mathbb{R}^d\\) and symmetric positive definite covariance matrix \\({\\bf \\Sigma} \\in \\mathbb{R}^{d\\times d}\\) is the random \\(d\\)-dimensional vector \\(\\vec{x}\\) with probability density function \\[f(\\vec{x}) = \\frac{1}{(2\\pi)^{d/2} det({\\bf \\Sigma})^{1/2}}\\exp\\left(-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T{\\bf \\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})\\right).\\] We use shorthand \\(\\vec{x}\\sim \\mathcal{N}(\\vec{\\mu},{\\bf \\Sigma})\\) to indicate \\(\\vec{x}\\) follows this distribution. The Multivariate Gaussian distribution is also often called the Multivariate Normal (MVN) distribution. For example of the MVN, first consider the two-dimensional case with \\(\\vec{\\mu} = \\vec{0}\\) and \\[{\\bf \\Sigma} = \\begin{bmatrix}1 &amp; p \\\\ p &amp; 1\\end{bmatrix}.\\] Below, we show scatterplots of \\(1000\\) independent samples from this distribution for three different values of \\(p.\\) We will also refer to a collection of points in \\(\\mathbb{R}^d\\) as a point cloud. For an examples in \\(\\mathbb{R}^3\\) we again consider case where \\(\\vec{\\mu}=0\\) and let \\[{\\bf \\Sigma} = \\begin{bmatrix}1 &amp; p &amp; p^2 \\\\ p &amp; 1 &amp;p \\\\ p^2 &amp;p &amp; 1 \\end{bmatrix}.\\] In the preceding examples, different choices of \\(p\\), hence different covariance matrices, resulted in point clouds with different orientations and shapes. Later, we’ll discuss how we can determine the shape and orientation from the covariance matrix with the aid of linear algebra. What about changes to \\(\\vec{\\mu}\\)? Changing \\(\\vec{\\mu}\\) translates the point cloud. If in the preceding examples, we had taken \\(\\vec{\\mu}=\\vec{1}\\) the scatterplots would have had the same shape and orientation, but they would have been tranlated by a shift of \\(\\vec{1}.\\) Definition 2.2 (Multivariate t Distribution) The multivariate t-distribution on \\(\\mathbb{R}^d\\) with location vector \\(\\vec{\\mu}\\in\\mathbb{R}^d\\), positive definite scale matrix \\({\\bf \\Sigma}\\in \\mathbb{R}^{d\\times d}\\) and degrees of freedom \\(\\nu\\) has density \\[f(\\vec{x}) = \\frac{\\Gamma\\left(\\frac{\\nu+d}{2}\\right)}{\\Gamma(\\nu/2)\\nu^{d/2}\\pi^{d/2}|{\\bf \\Sigma}|^{1/2}}\\left[1 + \\frac{1}{\\nu}(\\vec{x}-\\vec{\\mu})^T{\\bf \\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})\\right]^{-(\\nu +d)/2}.\\] We use shorthand \\(\\vec{x}\\sim t_\\nu(\\vec{\\mu},{\\bf \\Sigma})\\) to indicate \\(\\vec{x}\\) follows this distribution. We’ll only consider a three dimensional case where the location, which determines the mode of the distribution, is \\(\\vec{0}\\) and the scale is the identity matrix. As in the Gaussian case, changing \\(\\vec{\\mu}\\) translates the point cloud and different values of \\({\\bf \\Sigma}\\) give point clouds with different shapes. The remaining parameter to consider here is the degrees of freedom, \\(\\nu\\), which controls how spread out the samples can be. We show results for three different choices of the degrees of freedom. For smaller degrees of freedom, there are more points which are far from the mode at \\(\\vec{0}\\). 2.3 Expectation, Mean, and Covariance As in the one-dimensional case, the cumulative distribution function determines the distribution of the random vector, and using the density we may establish a few important quantities which will appear often throughout this text. The first is the mean or expected value of the random vector which is the vector of expected values of each entry so that \\[\\begin{equation} E[\\vec{x}] = \\int_{\\mathbb{R}^d}\\vec{x} f(\\vec{x})d\\vec{x} = \\begin{bmatrix} E[x_1] \\\\ \\vdots \\\\ E[x_d] \\end{bmatrix} = \\begin{bmatrix} \\int_{\\mathbb{R}^d} x_1 f(\\vec{x})d\\vec{x} \\\\ \\vdots \\\\ \\int_{\\mathbb{R}^d} x_d f(\\vec{x})d\\vec{x} \\end{bmatrix} \\tag{2.1} \\end{equation}\\] where \\[\\int_{\\mathbb{R}^d} x_i f(\\vec{x})d\\vec{x} = \\int_{-\\infty}^\\infty \\dots \\int_{-\\infty}^\\infty x_i f(x_1,\\dots,x_d)dx_1 \\dots dx_d.\\] Note, we are assuming each of the integrals in (2.1) is well defined, which is a convention we adhere to throughout this text. Often, we’ll often use \\(\\vec{\\mu}\\) to denote the mean vector. When we are considering more than multiple random vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) we will add a corresponding subscript \\(\\vec{\\mu}_\\vec{x}\\) to denote the corresponding mean of \\(\\vec{x}\\). The linearity of expectation for univariate random vectors holds here as well. If \\(\\vec{x}\\in\\mathbb{R}^d\\) is a random vector, \\({\\bf A}\\in\\mathbb{R}^{k\\times d}\\) is a matrix of constant entries, and \\(\\vec{b}\\in\\mathbb{R}^k\\) is a vector of constant entries then \\[E[{\\bf A}\\vec{x} + \\vec{b}] = {\\bf A}\\vec{\\mu} + \\vec{b}.\\] Importantly, for non-squared matrices \\({\\bf A}\\) then mean of \\({\\bf A}\\vec{x}\\) will be of a different dimension than \\(\\vec{x}.\\) In general, the coordinates of a random vector will not be independent. To quantify the pairwise dependence, we could consider the covariance \\[Cov(x_i,x_j) = E[(x_i - \\mu_i)(x_j -\\mu_j)] = \\int_\\mathbb{R}^d (x_i-\\mu_i)(x_j-\\mu_j) f(\\vec{x})d\\vec{x}\\] for \\(1\\le i,j \\le d\\). In the case \\(i=j\\), this simplifies to \\(Cov(x_i,x_i) = Var(x_i)\\). Importantly, we do not want to consider each all of the pairwise covariance separately. Instead, we can organize them as a \\(d\\times d\\) matrix \\({\\bf \\Sigma}\\) with entries \\({\\bf \\Sigma}_{ij} = Cov(x_i,x_j)\\). Hereafter, we will refer to \\({\\bf \\Sigma}\\) as the covariance matrix of \\(\\vec{x}.\\) When we are considering multiple random vectors we will use subscripts so that \\({\\bf \\Sigma}_{\\vec{x}}\\) and \\({\\bf \\Sigma}_{\\vec{y}}\\) denote the covariance matrices or random vectors \\({\\vec{x}}\\) and \\({\\vec{y}}\\) respectively. Following the notational conventions, it follows that \\(\\vec{x} - E[\\vec{x}] = \\vec{x} - \\vec{\\mu} \\in \\mathbb{R}^d\\) so that the outer product of \\(\\vec{x} - \\vec{\\mu}\\) with itself is the \\(d\\times d\\) matrix with entries \\[[(\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T]_{ij} = (x_i-\\mu_i)(x_j-\\mu_j)\\] so that we may more compactly write \\[\\begin{equation} \\text{Var}(\\vec{x}) = E\\left[(\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T\\right] \\tag{2.2} \\end{equation}\\] where we interpret the expectation operation as applying to each entry of the matrix \\((\\vec{x} - \\vec{\\mu})(\\vec{x} - \\vec{\\mu})^T\\). This looks very similar to the univariate case save that we must be mindful of the multidimensional nature of our random vector. In fact with some algebra, we have the following alternative formula for the covariance matrix \\[{\\bf \\Sigma} = E[\\vec{x}\\,\\vec{x}^T] -\\vec{\\mu}\\vec{\\mu}^T\\] which is again reminiscent of the univariate case. Showing this result is left as a short exercise. One brief note to avoid confusion. Other texts refer to \\(\\text{Var}(\\vec{x})\\) as the variance matrix or variance-covariance matrix. Herein, we use the term covariance matrix for \\(\\text{Var}(\\vec{x}).\\) Recall the univariate case, \\[\\text{Var}(aX+b) = a^2\\text{Var}(X)\\] for constants \\(a\\) and \\(b\\) and (one-dimensional) random variable \\(X\\). Similar to the univariate case, there is a formula for the covariance of an affine mapping of a random vector, but the specific form requires us to be mindful of the matrix structure of the covariance matrix. For random vector \\(\\vec{x}\\in\\mathbb{R}^d\\), constant matrix \\({\\bf A}\\in\\mathbb{R}^{k\\times d}\\) and constant vector \\(\\vec{b}\\in\\mathbb{R}^k\\), it follows (see exercises) that \\[{\\bf \\Sigma}_{{\\bf A}\\vec{x}+\\vec{b}} = {\\bf A \\Sigma A}^T.\\] Importantly, note that \\({\\bf A \\Sigma A}^T\\) is a \\(k\\times k\\) matrix which is consistent with the fact that \\({\\bf A}\\vec{y}+\\vec{b}\\) is a \\(k\\)-dimensional vector. Example 2.1 (Mean and Covariance of MVN) If \\(\\vec{x} \\sim \\mathcal{N}(\\vec{\\mu}, {\\bf \\Sigma})\\) then \\(E[\\vec{x}] = \\vec{\\mu}\\) and \\(\\text{Var}(\\vec{x}) = {\\bf \\Sigma}\\) Example 2.2 (Mean and Covariance of Multivariate t-distribution) Let \\(\\vec{x} \\sim t_\\nu(\\vec{\\mu}, {\\bf \\Sigma})\\). If \\(\\nu &gt; 1\\) then \\(E[\\vec{x}] = \\vec{\\mu}\\); otherwise the mean does not exist. If \\(\\nu &gt; 2\\), then \\(\\text{Var}(\\vec{x}) = \\frac{\\nu}{\\nu-2}{\\bf \\Sigma}\\); otherwise, the covariance matrix does not exist. Verifying these examples is left to the exercises and rely on multivariate change of variables which are not covered here. 2.3.1 Sample Mean and Sample Covariance In many cases, we’ll consider a collection of \\(N\\) \\(iid\\) vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N \\in \\mathbb{R}\\). Again, subscripts are used here, but importantly when accompanied by the \\(\\vec{\\cdot}\\) sign a subscript does not refer to a specific coordinate of a vector but rather one vector in a set. Given iid observations \\(\\vec{x}_1,\\dots,\\vec{x}_N\\), we will use sample averages to estimate the expectation and covariance of the data generating distribution. We’ll use bars to denote sample averages so that \\(\\bar{x}\\) denotes the sample mean and \\(\\bar{\\bf \\Sigma}\\) the sample covariance. In this case, we have \\[\\begin{equation} \\bar{x} = \\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i. \\tag{2.3} \\end{equation}\\] Similarly, we define the sample covariance matrix to be \\[\\begin{equation} {\\bf \\bar{\\Sigma}} = \\frac{1}{N} \\sum_{i=1}^N (\\vec{x}_i - \\bar{x})(\\vec{x}_i - \\bar{X})^T = \\left(\\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i\\vec{x}_i^T\\right) - \\bar{x}\\bar{x}^T \\tag{2.4} \\end{equation}\\] In (2.4), dividing by \\(N\\) rather than \\(N-1\\) yields biased estimates of the terms of the sample covariance matrix. However, the final formula in (2.4) more directly matches the corresponding term in the definition of the covariance matrix. Had we used a factor of \\(1/(N-1)\\) instead, we would have \\[\\frac{1}{N-1}\\sum_{i=1}^N (\\vec{x}_i - \\bar{X})(\\vec{x}_i - \\bar{X})^T = \\left(\\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i\\vec{x}_i^T\\right) - \\frac{N}{N-1}\\bar{x}\\bar{x}^T\\] which is slightly more cumbersome. In the examples we will consider, \\(N\\) will typically be large enough so that the numerical difference is small. As such, we will opt for algebraically convenient definition form of @(eq:def-sample-covariance) as our definition of the sample covariance matrix. Alternatively, we can view the sample mean and sample covariance as the mean and covariance (using expectation rather than averages) of the empirical distribution from a collection of samples \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) defined below. Definition 2.3 (Empirical Distribution) Given a finite set of points \\(\\mathcal{X}=\\{\\vec{x}_1,\\dots,\\vec{x}_N\\} \\subset \\mathbb{R}^d\\), we say that random vector \\(\\vec{z}\\) follows the empirical distribution from data \\(\\mathcal{X}\\) if \\[P(\\vec{z} = \\vec{x}_i) = \\frac{1}{N}, \\quad i = 1,\\dots, N\\] and is zero otherwise. If \\(\\vec{z}\\) follows the empirical distribution on a set of \\(N\\) points \\(\\mathcal{X}= \\{\\vec{x}_1,\\dots,\\vec{x}_N\\}\\), then the expectation and covariance matrix of \\(\\vec{z}\\) are equivalent to the sample mean and sample covariance for data \\(\\vec{x}_1,\\dots,\\vec{x}_N.\\) 2.3.2 The Data Matrix Both (2.3) and (2.4) involve summations. Working with sums will prove cumbersome, so briefly let us introduce a more compact method for representing these expressions. Hereafter, we will organize the vectors \\(\\vec{x}_1,\\dots, \\vec{x}_N\\) into a data matrix \\[{\\bf X} = \\begin{bmatrix} \\vec{x}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T\\end{bmatrix} \\in \\mathbb{R}^{N\\times d}.\\] In this setup, \\({\\bf X}_{ij}\\) is the \\(j\\)th coordinate of \\(\\vec{x}_i\\), or equivalently, the \\(j\\)th measurement taken from the \\(i\\)th subject. Thus, rows of \\({\\bf X}\\) index subjects (realizations of the random vector) whereas columns index common measurements across all subjects. Using the data matrix, we can forgo the summation notation giving the following formulas for the sample mean \\[\\begin{equation} \\bar{x} = \\frac{1}{N} {\\bf X}^T \\vec{1} \\tag{2.5} \\end{equation}\\] and the sample covariace matrix \\[\\begin{equation} {\\bf \\bar{\\Sigma}} = \\frac{1}{N} ({\\bf HX})^T {\\bf HX} = \\frac{1}{N} {\\bf X}^T {\\bf H X} \\tag{2.6} \\end{equation}\\] where \\({\\bf H} = {\\bf I} - \\frac{1}{N} \\vec{1} \\vec{1}^T \\in \\mathbb{R}^{N\\times N}\\) is known as the centering matrix. We have used the fact that \\({\\bf H}\\) is symmetric and idempotent, e.g. \\({\\bf H}^2 = {\\bf H}\\) which is left as a exercise. The vector \\(\\vec{1}\\) is the \\(N\\)-dimensional vector with 1 in each entry and \\({\\bf I}\\) is the \\(N\\times N\\) identity matrix. One can show (see exercises) that the matrix-vector and matrix-matrix multiplication implicitly handles the summations in (2.3) and (2.4). To conclude this section, we compare the sample mean and covariance matrix computed from random draws from the \\(MVN\\) distribution. Example 2.3 (Draws from the MVN) We draw \\(N=100\\) samples from the \\(\\mathcal{N}(\\vec{0}, {\\bf \\Sigma})\\) distribution where \\[{\\bf \\Sigma}=\\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp;4 &amp; 0 \\\\ 0&amp;0&amp;9\\end{bmatrix}\\]. N &lt;- 100 X &lt;- mvrnorm(n=N, mu = rep(0,3), Sigma = c(1,4,9)*diag(3)) To compute the sample mean and sample covariance, we implement (2.3) and (2.4). xbar &lt;- (1/N) * t(X) %*% rep(1,N) H &lt;- diag(N) - (1/N)*matrix(1,nrow = N, ncol = N) S &lt;- (1/N) * t(H %*% X) %*% (H %*% X) The results are shown below (rounded to three decimal places) \\[\\bar{x} = \\begin{bmatrix} 0.008 \\\\ 0.078 \\\\ -0.239 \\end{bmatrix} \\qquad \\text{and} \\qquad \\bar{\\bf \\Sigma} = \\begin{bmatrix} 1.148 &amp; 0.216 &amp; 0.121 \\\\ 0.216 &amp; 4.181 &amp; 0.087 \\\\ 0.121 &amp; 0.087 &amp; 9.366 \\end{bmatrix}\\] which are closer to the true values. If we increase the sample size to \\(N=10^4\\) samples, we get estimates which are closer to the true values (shown below). \\[\\bar{x} = \\begin{bmatrix} 0.001 \\\\ 0.02 \\\\ 0.051 \\end{bmatrix} \\qquad \\text{and} \\qquad \\bar{\\bf \\Sigma} = \\begin{bmatrix} 0.991 &amp; -0.02 &amp; -0.04 \\\\ -0.02 &amp; 3.997 &amp; 0.095 \\\\ -0.04 &amp; 0.095 &amp; 8.992 \\end{bmatrix}\\] 2.4 Linear Algebra 2.4.1 Assumed Background This text assumes familiarity with definitions from a standard undergraduate course in linear algebra including but not limited to linear spaces, subspaces, spans and bases, and matrix multiplication. However, we have elected to provide review of some of the most commonly used ideas in the methods we’ll cover in the following subsections. For a more thorough treatment of linear algebra, please see REFERENCES 2.4.2 Interpretations of Matrix Multiplication Throughout this text, comfort with common calculations in linear algebra will be very important. Herein, we assume the reader has some exposure to these materials at an undergraduate level including the summation of vectors or matrices. The familiarity with matrix-vector and matrix-matrix multiplication will play a central role as we have already seen in the case of the data matrix formulation of the sample mean and sample covariance. However, rote familiarity with computation will not be sufficient to build intuition for the methods we’ll discuss. As such, we’ll begin with a review of a few important ways one can view matrix-vector (and matrix-matrix) multiplication which will be helpful later. Those who feel comfortable with the myriad interpretations of matrix multiplication in terms of linear combinations of the rows and columns may skip to the next section. Suppose we have matrix \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\) and vector \\(\\vec{x}\\in\\mathbb{R}^n\\). If we let \\(\\vec{a}_1^T,\\dots, \\vec{a}_M^T\\in\\mathbb{R}^n\\) denote the rows of \\({\\bf A}\\), then the most commonly cited formula for computing \\({\\bf A}\\vec{x}\\) is \\[{\\bf A}\\vec{x} = \\begin{bmatrix} \\vec{a}_1^T \\\\ \\vdots \\\\ \\vec{a}_m^T\\end{bmatrix} \\vec{x} = \\begin{bmatrix}\\vec{a}_1^T \\vec{x} \\\\ \\vdots \\\\ \\vec{a}_m^T\\vec{x}\\end{bmatrix}\\] wherein we take the inner product of the rows of \\({\\bf A}\\) with vector \\(\\vec{x}\\). We can expand this definition to matrix-matrix multiplication. If \\({\\bf B}\\in\\mathbb{R}^{n\\times k}\\) has columns \\(\\vec{b}_1,\\dots,\\vec{b}_k\\) then \\[({\\bf AB})_{ij} = \\vec{a}_i^T\\vec{b}_j\\] where we take the inner product of the \\(i\\)th row of \\({\\bf A}\\) with the \\(j\\)th column of \\({\\bf B}\\) to get the \\(ij\\)th entry of \\({\\bf AB}.\\) This is perfectly reasonable method of computation, but alternative perspectives are helpful, particularly when we consider different factorization of the data matrix in later chapters.. Returning to \\({\\bf A}\\vec{x}\\), suppose now that \\({\\bf A}\\) has columns \\(\\vec{\\alpha}_1,\\dots,\\vec{\\alpha}_n\\) and \\(\\vec{x} = (x_1,\\dots,x_n)^T\\), then we may view \\({\\bf A}\\vec{x}\\) as a linear combination of the columns of \\({\\bf A}\\) so that \\[{\\bf A}\\vec{x} = \\begin{bmatrix}\\vec{\\alpha}_1 \\,| &amp; \\cdots &amp;|\\, \\vec{\\alpha}_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1\\vec{\\alpha}_1 + \\dots + x_n \\vec{\\alpha}_n = \\sum_{j=1}^n x_j\\vec{\\alpha}_j.\\] Here, we added vertical columns between the vectors \\(\\vec{\\alpha}_i\\) to make clear that \\(\\begin{bmatrix}\\vec{\\alpha}_1 \\,| &amp; \\cdots &amp;|\\, \\vec{\\alpha}_n \\end{bmatrix}\\) is a matrix. We can extend this perspective to see that the columns of \\({\\bf AB}\\) are comprised of different linear combinations of the columns of \\({\\bf A}\\). Specifically, the \\(j\\) column of \\({\\bf AB}\\) is a linear combination of the columns of \\({\\bf A}\\) using the entries in the \\(j\\)th column of \\({\\bf B}\\). More specifically, the \\(j\\)th column of \\({\\bf AB}\\) is the linear combination \\[\\sum_{i=1}^n {\\bf B}_{ij}\\vec{\\alpha}_{i}.\\] Our final observations follows by taking these insights on linear combinations of columns and transposing the entire operation. What can we say about the rows of \\({\\bf AB}\\)? We can rewrite \\({\\bf AB} = ({\\bf B}^T{\\bf A}^T)^T\\). The columns of \\({\\bf B}^T{\\bf A}^T\\) are linear combinations of the columns of \\({\\bf B}^T\\). Since the columns of \\({\\bf B}^T\\) are the rows of \\({\\bf B}\\), it follows that the rows of \\({\\bf AB} = ({\\bf B}^T{\\bf A}^T)^T\\) are linear combinations of the rows of \\({\\bf B}\\) with weights given by the entries in each row of \\({\\bf A}\\) respectively. In mathematical notation, the \\(i\\)th row of \\({\\bf AB}\\) is \\[\\sum_{j=1}^n {\\bf A}_{ij} \\vec{\\beta}_j^T\\] where \\(\\vec{\\beta}_1^T,\\dots, \\vec{\\beta}_n^T\\) are the rows of \\({\\bf B}.\\) 2.4.3 Norms and Distances Throughout this text, we will use \\(\\| \\cdot \\|\\) to denote the usual Euclidean (or \\(\\ell_2\\)) norm, which for a vector, \\(\\vec{x} = (x_1,\\dots,x_d)\\in\\mathbb{R}^d\\), is \\[\\|\\vec{x}\\| = \\left(\\sum_{j=1}^d x_j^2 \\right)^{1/2}.\\] We may then define the Euclidean distance between two \\(d\\)-dimension vectors \\(\\vec{x}\\) and \\(\\vec{y}\\) to be \\[\\|\\vec{x}-\\vec{y}\\| = \\left(\\sum_{j=1}^d (x_j - y_j)^2\\right)^{1/2}.\\] Euclidean distance is the most commonly used notion of distance (or norm or metric) between two vectors, but it is far from the only option. We can consider the general \\(\\ell_p\\) norm \\[\\|\\vec{x}\\|_p = \\left(\\sum_{j=1}^d x_j^p\\right)^{1/p}\\] which coincides with the Euclidean norm for \\(p=2\\). Two other special cases include \\(p=1\\) also known as the Manhattan distance and \\(p = \\infty\\) also known as the sup-norm \\[\\|\\vec{x}\\|_\\infty = \\max_{j=1,\\dots, d} |x_j|.\\] We can also extend this notions of vector norms to a measure of the norm of a matrix. Two important cases are the \\(\\ell_2\\) norm of a matrix and the Frobenius norm. For matrix \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\), this norm is \\[\\begin{equation} \\|{\\bf A}\\| = \\sup_{\\vec{x}\\in\\mathbb{R}^n \\text{ s.t. } \\vec{x}\\ne \\vec{0}} \\frac{\\|{\\bf A}\\vec{x}\\|}{\\|\\vec{x}\\|} = \\sup_{\\vec{x}\\in\\mathbb{R}^n \\text{ s.t. }\\|\\vec{x}\\|=1} \\|{\\bf A}\\vec{x}\\|. \\end{equation}\\] You can interpret \\(\\|{\\bf A}\\|\\) as the largest relative change in the Euclidean length of a vector after it is multiplied by \\({\\bf A}\\). The Frobenius extends the algebraic definition of a matrix to a matrix. For \\({\\bf A}\\in\\mathbb{R}^{m\\times n}\\), its Frobenius norm is \\[\\begin{equation} \\|{\\bf A}\\|_F = \\left(\\sum_{i=1}^m\\sum_{j=1}^n {\\bf A}_{ij}^2\\right)^{1/2}. \\end{equation}\\] The \\(\\ell_2\\) distance between two matrices is then \\(\\|{\\bf A}-{\\bf B}\\|\\) and the Frobenius distance between two matrices is \\(\\|{\\bf A} - {\\bf B}\\|\\) where both \\({\\bf A}\\) and \\({\\bf B}\\) have the same number of rows and columns. 2.4.4 Important properties A few additional definitions that we will use throughout the text are provided below without examples. Definition 2.4 (Symmetric Matric) A matrix \\({\\bf A}\\in \\mathbb{R}^{d\\times d}\\) is symmetric if \\({\\bf A} = {\\bf A}^T.\\) ::: {.definition #def-eigen name = “Eigenvectors and Eigenvalues”} Let \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). If there is a scalar \\(\\lambda\\) and vector \\(\\vec{x}\\ne \\vec{0}\\) such that \\({\\bf A}\\vec{x} = \\lambda \\vec{x}\\) then we say \\(\\lambda\\) is an eigenvalue of \\({\\bf A}\\) with associated eigenvector \\(\\vec{x}.\\) ::: Definition 2.5 (Positive definite) Let \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). If \\(\\vec{x}^T{\\bf A}\\vec{x} &gt;0\\) for all \\(\\vec{x}\\ne \\vec{0}\\) then we say \\({\\bf A}\\) is positive definite. If instead, \\(\\vec{x}^T{\\bf A}\\vec{x} \\ge 0\\) for all \\(\\vec{x}\\ne \\vec{0}\\) we say that \\({\\bf A}\\) is positive semi-definite. 2.4.5 Matrix Factorizations Two different matrix factorization will arise many times throughout the text. The first, which is commonly presented in linear algebra courses, is the spectral decomposition of a square matrix which is also known as diagonalization or eigenvalue decomposition. Herein, we assume familiarity with eigenvalues and eigenvectors. The second factorization is the singular value decomposition. In the subsequent subsections, we briefly discuss these two factorizations, their geometric interpretation, and some notation that will typically be used in each case. 2.4.5.1 Eigenvalue Decomposition We begin with the eigenvalue decomposition of a square matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\). As you may recall, \\({\\bf A}\\) will have a set of \\(d\\) eigenvalues \\(\\lambda_1,\\dots, \\lambda_d\\) (which may include repeated values) and associated eigenvectors. A number, \\(\\lambda\\), may be repeated in the list of eigenvalues, and the number of times is called the algebraic multiplicity of \\(\\lambda\\). Each eigenvalue has a least one eigenvector. In cases where the eigenvalue has algebraic multiplicity greater than one, we refer to its geometric multiplicity as the number of linearly independent eigenvectors associated with the eigenvalue. The algebraic multiplicity is always greater than or equal to the geometric multiplicity. However, this is not always the case, and when this occurs, the matrix cannot be diagonalized. Fortunately, we will largely be dealing with symmetric matrices for which diagonalization is guaranteed by the following theorem. Theorem 2.1 (Spectral Decomposition Theorem for Symmetric Matrices) Any symmetric matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\) can be written as \\[{\\bf A} = {\\bf U\\Lambda U}^T\\] where \\({\\bf U}\\in\\mathbb{R}^{d\\times d}\\) is an orthonormal matrix and \\({\\bf \\Lambda}\\) is a diagonal matrix \\[{\\bf \\Lambda} = \\begin{bmatrix} \\lambda_1 &amp; 0&amp;0 \\\\ 0&amp; \\ddots &amp;0 \\\\ 0&amp;0 &amp;\\lambda_d\\end{bmatrix}\\] where the scalars \\(\\lambda_1,\\dots,\\lambda_d \\in \\mathbb{R}\\) are the eigenvalues of \\({\\bf A}\\) and the corresponding columns of \\({\\bf U}\\) are their associated eigenvectors. By convention, we will always assume the eigenvalues are in decreasing order so that \\(\\lambda_1\\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d\\). The most common types of symmetric matrices that we will encounter are covariance matrices. In those cases, the spectral decomposition of the covariance can provide some helpful insight about the shape of the associated probability density. We demonstrate this idea graphically in the following examples using the MVN. Example 2.4 (Level curves of MVN in) Consider NOTES: Add example of Gaussian densities in \\(\\mathbb{R}^2\\) with level curves and eigenvectors/values. 2.4.5.2 Singular Value Decomposition The spectral theorem is limited in that it requires a matrix to be both square and symmetric. When focusing on data matrices \\({\\bf X}\\in\\mathbb{R}^{N\\times d}\\) both assumptions are extremely unlikely to be satisfied, and we will need a more flexible class of methods. This idea is explored in much greater detail in Chapter 4. For now, we briefly introduce the Singular Value Decomposition and how this factorization provides some insight on the geometric structure of matrix-vector multiplication. Definition 2.6 (The Singular Value Decomposition (SVD)) Let \\({\\bf A} \\in \\mathbb{R}^{m\\times n}\\) be a rank \\(k\\) matrix. Then \\({\\bf A}\\) may be factored as \\[\\begin{equation} {\\bf A} = \\tilde{\\bf U}\\tilde{\\bf S}\\tilde{\\bf V}^T \\tag{2.7} \\end{equation}\\] where \\(\\tilde{\\bf U}\\in\\mathbb{R}^{m\\times k}\\) and \\(\\tilde{\\bf V}\\in\\mathbb{R}^{k\\times n}\\) have orthonormal columns and \\(\\tilde{\\bf S}\\in\\mathbb{R}^{k\\times k}\\) is a diagonal matrix with real entries \\(\\sigma_1 \\ge \\dots \\sigma_k \\ge 0\\) along the diagonal. We refer to (2.7) as the reduced singular value decomposition of \\({\\bf A}.\\) The columns of \\(\\tilde{\\bf U}\\) (\\(\\tilde{\\bf V}\\)) are called the left (right) singular vectors of \\({\\bf A}\\) and the scalars \\(\\sigma_1\\ge \\dots\\ge \\sigma_k\\) are referred to as the singular values of \\({\\bf A}.\\) Let \\(\\vec{u}_1,\\dots,\\vec{u}_k\\in\\mathbb{R}^m\\) be the columns of \\(\\tilde{\\bf U}\\). When \\(m &gt;k\\), we may find additional vectors \\(\\vec{u}_{k+1},\\dots,\\vec{u}_m\\) such that \\(\\{\\vec{u}_1,\\dots,\\vec{u}_m\\}\\) are an orthonormal basis for \\(\\mathbb{R}^m\\). Similarly, we can apply the same reasoning to \\(\\tilde{\\bf V}\\) to find an orthonormal basis of \\(\\mathbb{R}^n\\) with the first \\(k\\) such vectors corresponding to the right singular vectors of \\({\\bf A}\\). The (full) singular value decomposition of \\({\\bf A}\\) is \\[\\begin{equation} {\\bf A} = {\\bf US\\bf V}^T \\tag{2.8} \\end{equation}\\] where \\[{\\bf U} = \\begin{bmatrix} \\vec{u}_1 &amp; \\dots &amp; \\vec{u}_m\\end{bmatrix}\\in\\mathbb{R}^{m\\times m} \\qquad {\\bf V} = \\begin{bmatrix} \\vec{v}_1 &amp; \\dots &amp; \\vec{v}_n\\end{bmatrix}\\in\\mathbb{R}^{n\\times n}\\] are orthonormal matrices. The matrix \\({\\bf S}\\in\\mathbb{R}^{m\\times n}\\) is formed by taking \\(\\tilde{S}\\) and padding it with zero along the bottom and right so that it is \\(m\\times n\\). When considering matrix muliplication \\({\\bf A}\\vec{x}\\), the full SVD of \\({\\bf A}\\) is helpful for decomposition this matrix multiplication into three more interpretable steps. 2.4.6 Symmetry, Positive Definiteness, and Matrix Powers 2.5 Exercises Let \\(f(\\vec{x}) = \\vec{x}^T {\\bf A}\\vec{x}\\) for a vector \\(\\vec{x}\\in \\mathbb{R}^d\\) and a matrix \\({\\bf A}\\in\\mathbb{R}^{d\\times d}\\) which is constant. Give expressions for \\(\\nabla f\\) and \\(\\mathcal{H} f\\) using only matrices and vectors (no summation notation is allowed). Given a random vector \\(\\vec{x}\\) with mean \\(\\vec{\\mu}\\) and covariance matrix \\({\\bf \\Sigma} = E[(\\vec{x}-\\vec{\\mu})(\\vec{x}-\\vec{\\mu})^T\\,]\\) verify the identity \\({\\bf \\Sigma} = E[\\,\\vec{x}\\,\\vec{x}^T\\,] - \\vec{\\mu} \\,\\vec{\\mu}^T\\). Suppose \\(\\vec{x}\\in \\mathbb{R}^n\\) is a random vector with covariance \\({\\bf \\Sigma}\\in\\mathbb{R}^{n\\times n}\\), and let \\({\\bf A} \\in \\mathbb{R}^{m\\times n}\\) be a matrix with constant entries (non-random). Show that the covariance matrix of \\({\\bf A}\\vec{x}\\) is \\({\\bf A\\Sigma A}^T.\\) Show that the \\(N\\times N\\) centering matrix \\({\\bf H} = {\\bf I} - \\frac{1}{N}\\mathbb{1}\\mathbb{1}^T\\) is idempotent, i.e. \\({\\bf H}^2 = {\\bf H}.\\) Consider the data matrix \\[{\\bf X}=\\begin{bmatrix}\\vec{x}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T\\end{bmatrix}.\\] Show that \\[{\\bf HX} = \\begin{bmatrix} \\vec{x}_1^T -\\bar{x}^T \\\\ \\vdots \\\\ \\vec{x}_N^T - \\bar{x}^T\\end{bmatrix}\\] where \\({\\bf H}\\) is the centering matrix and \\(\\bar{x}\\) is the sample mean of vectors \\(\\vec{x}_1,\\dots, \\vec{x}_N.\\) Let \\({\\bf A}\\in \\mathbb{R}^{m\\times n}\\). Explain why \\({\\bf A A}^T\\) and \\({\\bf A}^T{\\bf A}\\) are diagonalizable and positive semi-definite. Give expressions for the singular vectors and singular values of \\({\\bf A}\\) in terms of the eigenvectors and eigenvalues of \\({\\bf A A}^T\\) and \\({\\bf A}^T{\\bf A}\\). "],["central-goals-and-assumptions.html", "Chapter 3 Central goals and assumptions 3.1 Dimension reduction and manifold learning 3.2 Clustering 3.3 Generating synthetic data 3.4 Exercises", " Chapter 3 Central goals and assumptions In the remainder of this text, we will largely focus on the case where we are given a dataset containing samples \\(\\vec{x}_1,\\dots,\\vec{x}_N \\in \\mathbb{R}^d\\). We will assume that the vectors were drawn independently from some unknown data generating process. As we discussed briefly in Chapter 1, in UL we want to learn important relationships within the dataset that can provide a simplified but meaningful summary of the data. The central assumption to UL is that such structure exists, though the specifics vary depending on the setting. The two largest areas of focus herein are dimension reduction/manifold learning and clustering which can both be used for feature engineering, data compression, and exploratory data analysis. Dimension reduction is also commonly used for visualization. We’ll briefly discuss association rules in Section 4.2. 3.1 Dimension reduction and manifold learning Algorithms for dimension reduction and manifold learning have a number of different applications which are all based on the Manifold Hypothesis Hypothesis 3.1 (Manifold Hypothesis) The points in a high-dimensional dataset live on a latent low-dimension surface (also called a manifold). The manifold hypothesis implies that the dataset can be described by a much smaller number of dimensions. Determining the manifold structure and a simplified set of coordinates for each point on the manifold is the central goal of these algorithms. 3.2 Clustering Hypothesis 3.2 (Clustering Hypothesis) The points in a dataset can be grouped into well defined subsets. Points in each subset are similar and points in different subsets are not. The cluster hypothesis suggests there are non-overlapping regions. Within each region there are (many) similar points, and there are no points whose are comparably similar to those in more than one subset. 3.3 Generating synthetic data 3.3.1 Data on manifolds At the beginning of this chapter, we indicated that our data are drawn from some unknown distribution. This is a practical assumption, but in many cases, it is also helpful to consider examples where we generate the data ourselves. In doing so, we can create whatever complicated structure we would like such as different clustering arrangements or lower dimensional structure. We can test an Unsupervised Learning algorithm of interest on these synthetically generated data to see if important relationships or properties are accurately preserved. This is a helpful method for evaluating how well an algorithm works in a specific case, and importantly, can be used to build intuition on a number of natural complexities such as appropriately choosing tuning parameters, evaluating the effects of noise, and seeing how these algorithms may break when certain assumptions are not met. First, let us consider the case of generating data with a known lower dimensional structure which will be valuable when testing a dimension reduction or manifold learning algorithm. We’ll begin with data on a hyperplane. Later in Chapter 4, we consider data on a hyperplane with additional constraints which can be generated by small changes to the method discussed below. Example 3.1 (Generating data on a hyperplane) Suppose we want to generate a set of \\(d\\)-dimensional data which is on a \\(k&lt;d\\) dimensional hyperplane. The span of \\(k\\) linearly independent vectors \\(\\vec{z}_1,\\dots,\\vec{z}_k \\in \\mathbb{R}^d\\) defines a \\(k\\) dimensional hyperplane. If we then generated random coefficient \\(c_1,\\dots,c_k\\), then the vector \\[\\vec{x} = c_{1}\\vec{z}_1+\\dots +c_{k}\\vec{z}_k\\] would be an element on this hyperplane. To generate a data set we could then Specify \\(\\vec{z}_1,\\dots,\\vec{z}_k\\) or generate them randomly Draw random coefficients \\(c_1,\\dots,c_k\\) and compute the random sample \\(\\vec{x}=c_{1}\\vec{z}_1+\\dots +c_{k}\\vec{z}_k\\). Repeat step 2, \\(N\\) times to generate the \\(N\\) samples. In Figure 3.1, we show an example of data generated to reside on a random plane in \\(\\mathbb{R}^3\\). We first generate \\(\\vec{z}_1,\\dots,\\vec{z}_k\\) randomly by drawing each vector from a \\(\\mathcal{N}(\\vec{0},{\\bf I})\\) distribution. These vectors will be independent with probability 1. When then take coefficients \\(c_1,\\dots,c_k\\) which are iid \\(N(0,1)\\). set.seed(185) N &lt;- 100 # basis zs &lt;- mvrnorm(n=2, mu = rep(0,3), Sigma = diag(1,3)) #coeffs coeffs &lt;- matrix(rnorm(2*N),ncol = 2) # generate data matrix samples &lt;- coeffs %*% zs # plot results scatterplot3js(samples, xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]), angle = 90, pch = &#39;.&#39;, size = 0.2) Figure 3.1: Randomly generated points concentrated on a two-dimensional hyperplane. Generating data on a curved surface is generally more complicated. In some cases, the curved surface is defined implicitly via a constraint such as the unit sphere in \\(d\\)-dimensions \\[S^{d-1} = \\{\\vec{x}\\in\\mathbb{R}^d: \\|\\vec{x}\\| = 1\\}.\\] Generating data on the unit sphere can then be accomplished by drawing a vector from any distribution on \\(\\mathbb{R}^d\\) then rescaling the vector to have unit length. Different choices of the original distribution will result in different distributions over the unit sphere. Alternatively, we could consider a function which parameterizes a curve or surface. We show one such example below. Example 3.2 (Generating data on a Ribbon) The function \\[R(s) = (\\cos(2\\pi s),\\sin(2\\pi s),2s)^T\\] maps the interval \\((0,5)\\) to a helix in \\(\\mathbb{R}^3.\\) For a given choice of \\(s\\), if we let the third coordinate vary from \\(2s\\) to \\(2s+1\\) we would then trace out a ribbon in \\(\\mathbb{R}^3.\\) To do this, let’s add a second coordinate \\(t\\) which ranges from 0 to 1. We then have function \\(R(s,t) = (\\cos(2\\pi s),\\sin(2\\pi s), 2s+t)\\) which maps the rectangle \\((0,5)\\times (0,2)\\) to a curved ribbon shape in \\(\\mathbb{R}^3.\\) To generate data on the ribbon, we draw iid points uniformly from \\((0,5)\\times (0,1)\\) then apply the ribbon function. The results for \\(N=1000\\) samples are shown below. N &lt;- 1e4 s &lt;- runif(N,0,5) t &lt;- runif(N,0,1) ribbon &lt;- cbind(cos(2*pi*s),sin(2*pi*s),2*s+t ) scatterplot3js(ribbon, xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]), angle = 90, pch = &#39;.&#39;, size = 0.1) Figure 3.2: Realizations of points on a ribbon In both the surface is two-dimensional (if you were stand on the surface and look nearby data points, they would appear to be planar. A dimension reduction would be able to recover this two-dimensional structure which in the case of the plane corresponds to find the coefficients used to generate each point. For the ribbons, the coordinates \\((s,t)\\) used in the ribbon mapping would provide the simplified representation. 3.3.2 Clustered data The most straightforward way to generate clustered data is to combine realizations from separate data generating mechanisms that tend to create points in disjoint regions of \\(\\mathbb{R}^d\\). In Figure 3.3, we show two different cases in \\(\\mathbb{R}^2\\). Figure 3.3: In each subfigure below, different subsets of points were generated using different rules and are colored accordingly . Ideally, a clustering algorithm could detect the different cluster shapes (left: ellipsoids, right: concentric rings) and correctly group points depending on how they were generated. If one did not have access to the actual data generating process (depicted by the different colors), it is still likely that they could recover the correct groupings upon visual inspection. In general, this strategy is not tractable. Naturally, we would like an Unsupervised clustering algorithm that can learn these clusters directly from the data automatically. As we shall see in Chapter 7, certain algorithms which excel at grouping the data contained in disjoint ellipsoids will naturally struggle data clustering in concentric rings because the shape(s) of the different clusters matters has a major impact of the accuracy of the clustering. 3.4 Exercises "],["ch-linear.html", "Chapter 4 Linear Methods 4.1 Principal Component Analysis 4.2 Singular Value Decomposition 4.3 Nonnegative Matrix Factorization 4.4 Multidimensional Scaling", " Chapter 4 Linear Methods 4.1 Principal Component Analysis Principal component analysis (PCA) is arguably the first method for dimension reduction, which dates back to papers by some of the earliest contributors to statistical theory including Karl Pearson and Harold Hotelling [4, 5]. Pearson’s original development of PCA borrowed ideas from mechanics which provides a clear geometric/physical interpretation of the resulting PCA loadings, variances, and scores, which we will define later. This interpretability and an implementation that uses scalable linear algebra methods – allowing PCA to be conducted on massive datasets – is one of the reasons PCA is still used prolifically to this day. In fact, many more modern and complex methods still rely on PCA as an internal step in their algorithmic structure. There are number of different derivations of PCA including the construction of minimum least squares error, covariance decompositions, and low rank approximations. We’ll revisit these ideas later, but first, let’s discuss PCA through a geometrically motivated lens via a method called iterative projections. 4.1.1 Derivation 1: Iterative Projections We begin with a data matrix \\[{\\bf X} = \\begin{bmatrix} \\vec{x}_1^T\\\\ \\vdots \\\\\\vec{x}_N^T\\end{bmatrix} \\in\\mathbb{R}^{N\\times d}.\\] Let’s begin with an example of dimension reduction where we’ll seek to replace each vector \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) with corresponding scalars \\(y_1,\\dots,y_N\\) which preserve as much of the variability between these vectors as possible. To formalize this idea, let’s introduce a few assumptions. First, we’ll assume the data \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) are centered. This is not a requirement, but it will simplify the analysis later. We’ll discuss how to account for this centering step later, but for now assume \\(\\bar{x} = \\vec{0}\\) so that \\({\\bf HX} = {\\bf X}\\). More importantly, let’s assume that each \\(y_i\\) is derived in the same way. Specifically, let \\(y_i = \\vec{x}_i^T \\vec{w}\\) for some common vector \\(\\vec{w}\\). Thus, we can view each one-dimensional representation as a dot product of the corresponding observed vector with the same vector \\(\\vec{w}.\\) We can compactly write this expression as \\[\\vec{y} = \\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_n \\end{bmatrix}=\\begin{bmatrix}\\vec{x}_1^T \\vec{w} \\\\ \\vdots \\\\ \\vec{x}_N^T \\vec{w}\\end{bmatrix} = {\\bf X} \\vec{w}.\\] How do we choose \\(\\vec{w}\\)? We would like differences in the scalars \\(y_1,\\dots,y_N\\) to reflect differences in the vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) so having \\(y_1,\\dots,y_N\\) spread out is a natural goal. Thus, if \\(\\vec{x}_i\\) and \\(\\vec{x}_j\\) are far apart then so will \\(y_i\\) and \\(y_j\\). To do this, we’ll try to maximize the sample variance of the \\(y\\)’s. The sample variance \\[\\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y})^2 = \\frac{1}{N}\\sum_{i=1}^N(\\vec{x}_i^T \\vec{w} - \\bar{y})^2\\] will depend on our choice of \\(\\vec{w}\\). In the previous expression, \\[\\bar{y} = \\frac{1}{N} y_i = \\frac{1}{N}\\sum_{i=1}^N \\vec{x}_i^T \\vec{w} = \\frac{1}{N}\\vec{1}^T{\\bf X}\\vec{w}\\] is the sample mean of \\(y_1,\\dots,y_N.\\) Importantly, since we have assumed that \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) are centered, it follows that \\(\\bar{y}=0\\) and the sample variance of \\(y_1,\\dots,y_N\\) simplifies to \\[\\frac{1}{N}\\sum_{i=1}^N(\\vec{x}_i^T \\vec{w})^2 = \\frac{1}{N}\\sum_{i=1}^N y_i^2 = \\frac{1}{N} \\|y\\|^2 = \\frac{1}{N}\\vec{y}^T\\vec{y}.\\] We can write the above expression more compactly. Using the identity \\(\\vec{y} = {\\bf X}\\vec{w}\\), we want to choose \\(\\vec{w}\\) to maximize \\[\\frac{1}{N}\\vec{y}^T\\vec{y} = \\frac{1}{N}({\\bf X}\\vec{w})^T{\\bf X}\\vec{w} = \\frac{1}{N}\\vec{w}^T{\\bf X}^T{\\bf X}\\vec{w} = \\vec{w}^T\\left(\\frac{{\\bf X}^T{\\bf X}}{N}\\right)\\vec{w}.\\] Since we have assumed that \\({\\bf X}\\) is centered it follows that \\({\\bf X}^T{\\bf X}/N\\) is the sample covariance matrix \\(\\hat{\\bf \\Sigma}\\)! Thus, we want to make \\(\\vec{w}^T\\hat{\\bf \\Sigma} \\vec{w}\\) as large as possible. Naturally, we could increase the entries in \\(\\vec{w}\\) and increase the above expression without bound. To make the maximization problem well posed, we will restrict \\(\\vec{w}\\) to be unit-length under the Euclidean norm so that \\(\\|\\vec{w}\\|=1.\\) We now have a constrained optimization problem which gives rise to the first principal component loading. Definition 4.1 (First PCA Loading and Scores) The first principal component loading is the vector \\(\\vec{w}_1\\) solving the constrained optimization problem \\[\\begin{equation} \\begin{split} \\text{Maximize } &amp;\\vec{w}^T \\hat{\\bf \\Sigma}\\vec{w} \\\\ \\text{subject to constraint } &amp;\\|\\vec{w}\\|=1. \\end{split} \\end{equation}\\] The first principal component scores are the scalars \\(y_i = \\vec{x}_i^T\\vec{w}_1\\) for \\(i=1,\\dots, N\\). To find the first PCA loading we can make use of Lagrange multipliers (see exercises) to show that \\(\\vec{w}_1\\) must also satisfy the equation \\[\\hat{\\bf \\Sigma}\\vec{w}_1 = \\lambda \\vec{w}_1\\] where \\(\\lambda\\) is the Lagrange multiplier. From this expression, we can conclude that the first principal component loading is the unit length eigenvector associated with the largest eigenvalue of the sample covariance matrix \\(\\hat{\\bf \\Sigma}\\) and that the Lagrange multiplier \\(\\lambda\\) is the largest eigenvalue of \\(\\hat{\\bf \\Sigma}\\). In this case, we refer to \\(\\lambda\\) as the first principal component variance. 4.1.1.1 Geometric Interpretation of \\(\\vec{w}_1\\) Since \\(\\|\\vec{w}_1\\| = 1\\) we may interpret this vector as specifying a direction in \\(\\mathbb{R}^d\\). Additionally, we can decompose each of our samples into two pieces: one pointing in the direction specified by \\(\\vec{w}_1\\) and a second portion perpendicular to this direction. Thus, we may write \\[\\vec{x}_i = \\underbrace{\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1}_{parallel} + \\underbrace{(\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1)}_{perpendicular}.\\] By the Pythagorean theorem, \\[\\begin{align*} \\|\\vec{x}_i\\|^2 &amp;= \\| \\vec{w}_1 \\vec{x}_i^T\\vec{w}_1 \\|^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\\\ &amp;= (\\vec{w}_1^T\\vec{x}_i)^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\\\ &amp;= y_i^2 + \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2 \\end{align*}\\] for \\(i=1,\\dots,N\\). Averaging over all of samples gives the expression \\[\\frac{1}{N}\\sum_{i=1}^N\\|\\vec{x}_i\\|^2 = \\frac{1}{N}\\sum_{i=1}^N y_i^2 +\\frac{1}{N}\\sum_{i=1}^N \\|\\vec{x}_i -\\vec{w}_1 \\vec{x}_i^T\\vec{w}_1\\|^2.\\] The left-hand side of the above expression is fixed for a given set of data, whereas the first term on the right side is exactly what we sought to maximize when finding the first principal component loading. This quantity is the average squared length of the projection of each sample onto the direction \\(\\vec{w}_1\\). As such, we can view the first principal component loading as the direction in which \\(\\vec{x}_1,\\dots,\\vec{x}_N\\) most greatly varies. Let’s turn to an example in \\(\\mathbb{R}^3\\) to view this. Example 4.1 (Computing the First PCA Loading and Scores) Below, we show a scatterplot of \\(N=1000\\) random points in \\(\\mathbb{R}^3.\\) Notice the oblong shape of the cloud of points. Rotating this image, it is clear that the data varies more in certain directions than in others. We begin by centering the data data &lt;- scale(data, center = TRUE, scale = FALSE) # subtracts mean from each column which appears the same as the previous figure except centered around the origin. To find the principal component scores and loading, let us calculate of the sample covariance matrix. We can use the largest eigenvalue to find the first PCA variance. Its associated eigenvector (unit length) will be the first loading. The sample covariance \\[ \\hat{\\Sigma} = \\begin{bmatrix} 12.13&amp;7.7&amp;3.88 \\\\ 7.7&amp;9.79&amp;9.38 \\\\ 3.88&amp;9.38&amp;15.62 \\\\ \\end{bmatrix} \\] has largest eigenvalue \\(\\lambda = 26.75\\) and associated eigenvector \\((\\)-0.48, -0.58, -0.66\\()^T\\). We can conclude by plotting the first PCA scores below plot(y[,1], rep(0,N), ylab = &#39;&#39;, xlab = &#39;First PCA scores&#39;, main = &#39;&#39;) 4.1.1.2 Additional Principal Components The first PCA loading provides information about the direction in which are data most greatly vary, but it is quite possible that there are still other directions wherein our data still exhibits a lot of variability. In fact, the notion of a first principal component loading, scores, and variance suggests the existence of a second, third, etc. collection of these quantities. To explore these quantities, let’s proceed as follows For each datum, we can remove its component in the direction of \\(\\vec{w}_1\\), and focus on the projection onto the orthogonal complement of \\(\\vec{w}_1\\). Let \\[\\vec{x}_i^{(1)} = \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 = \\vec{x}_i - \\vec{w}_1 y_i\\] denote the portion of \\(\\vec{x}_i\\) which is orthogonal to \\(\\vec{w}_1\\). Here, the superscript \\(^{(1)}\\) indicates we have removed portion of the vector in the direction of the first loading. We can organize the orthogonal components into a new data matrix \\[{\\bf X}^{(1)} = \\begin{bmatrix} \\left(\\vec{x}_1^{(1)}\\right)^T \\\\ \\vdots \\\\ \\left(\\vec{x}_N^{(1)}\\right)^T \\end{bmatrix} = \\begin{bmatrix} \\vec{x}_1^T - \\vec{x}_1^T\\vec{w}_1\\vec{w}_1^T \\\\ \\vdots \\\\ \\vec{x}_N^T - \\vec{x}_N^T\\vec{w}_1\\vec{w}_1^T \\end{bmatrix} = {\\bf X} - {\\bf X}\\vec{w}_1\\vec{w}_1^T.\\] Now let’s apply PCA to the updated data matrix \\({\\bf X}^{(1)}\\) from which we get the second principal component loading, denoted \\(\\vec{w}_2\\), the second principal component scores, and the second principal component variance. One can show that the data matrix \\({\\bf X}^{(1)}\\) is centered so that its sample covariance matrix is \\(\\hat{\\bf \\Sigma}^{(1)} = \\frac{1}{N}({\\bf X}^{(1)})^T{\\bf X}^{(1)}.\\) Thus, the second PCA loading, \\(\\vec{w}_2\\), is a unit eigenvector associated with the largest eigenvalue of \\({\\bf \\Sigma}^{(1)}\\). This eigenvalue is the 2nd PCA variance and the 2nd PCA score of \\(\\vec{x}_i\\) is given by the inner product of \\(\\vec{x}_i^{(1)}\\) with \\(\\vec{w}_2\\). Here is one crucial observation. The vector \\(\\vec{w}_2\\) gives the direction of greatest variability of the vectors \\(\\vec{x}_1^{(1)},\\dots,\\vec{x}_N^{(1)}.\\) For each of these vectors we have removed the component in the direction of \\(\\vec{w}_1\\). Thus, \\(\\vec{x}_1^{(1)},\\dots,\\vec{x}_N^{(1)}\\) do not vary at all in the \\(\\vec{w}_1\\) direction. What can we say about \\(\\vec{w}_2\\)? Naturally, it must be perpendicular to \\(\\vec{w}_1\\)! We need not stop at the second PCA loading, scores, and variance. We could remove components in the direction of \\(\\vec{w}_2\\) and apply PCA to the vectors \\[\\begin{align*} \\vec{x}_i^{(2)} &amp;= \\vec{x}_i^{(1)} - \\vec{w}_2 (\\vec{x}_i^{(1)})^T\\vec{w}_2\\\\ &amp;= \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 - \\vec{w}_2(\\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1)^T\\vec{w}_2\\\\ &amp;= \\vec{x}_i - \\vec{w}_1\\vec{x}_i^T\\vec{w}_1 - \\vec{w}_2\\vec{x}_i^T\\vec{w}_2 + \\vec{w}_2\\vec{w_1}^T\\vec{x}_i\\underbrace{\\vec{w}_1^T\\vec{w}_2}_{=0} \\end{align*}\\] to obtain a third loading, variance, and set of scores. We can continue repeating this argument \\(d\\) times for our \\(d\\)-dimensional data until we arrive at a set of \\(d\\) unit vectors \\(\\vec{w}_1,\\dots,\\vec{w}_d\\) which are the \\(d\\) PCA loadings. To review, we then have \\(d\\) PCA loadings \\(\\vec{w}_1,\\dots,\\vec{w}_d\\) each with an associated PCA variance \\(\\lambda_1,\\dots,\\lambda_d\\). For each sample, we also have an associated set of PCA scores \\(\\vec{x}_i^T\\vec{w}_1,\\dots,\\vec{x}_i^T\\vec{w}_d\\) which we can organize into a large matrix \\[{\\bf Y} = \\begin{bmatrix} \\vec{x}_1^T\\vec{w}_1 &amp; \\dots &amp; \\vec{x}_1^T\\vec{w} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\vec{x}_N^T\\vec{w} &amp; \\dots &amp; \\vec{x}_N^T\\vec{w}_d \\end{bmatrix} = {\\bf X}\\begin{bmatrix} \\vec{w}_1 \\,| \\dots \\,|\\,\\vec{w}_d\\end{bmatrix}.\\] We formalize this idea in the following Lemma. Lemma 4.1 (Eigenvalues of Covariance Matrix of Projected Data) Suppose vectors \\(\\vec{x}_1,\\dots,\\vec{x}_N\\in\\mathbb{R}^d\\) are centered and have sample covariance matrix \\(\\hat{\\bf \\Sigma}\\). Let \\(\\lambda_1 \\ge \\dots\\ge\\lambda_d \\ge 0\\) denote the eigenvalues of \\(\\hat{\\bf \\Sigma}\\) with associated eigenvectors \\(\\vec{w}_1,\\dots,\\vec{w}_d.\\) Not let \\(\\vec{x}^{(k)}_i = \\vec{x}_i - \\sum_{j=1}^k \\vec{w}_j\\vec{x}_i^T\\vec{w}_j\\) denote the portion of vector \\(\\vec{x}_i\\) which is orthogonal to each of the first \\(k\\) PCA loadings. If \\(\\hat{\\bf \\Sigma}^{(k)}\\) denotes the sample covariance matrix of these vectors it follows that \\(\\hat{\\bf \\Sigma}^{(k)}\\) has eigenvalues \\(\\underbrace{0,\\dots,0}_{k}\\), \\(\\lambda_{k+1}\\ge \\dots \\ge \\lambda_d\\ge 0\\) with associated eigenvectors \\[\\underbrace{\\vec{w}_1,\\dots,\\vec{w}_k,}_{\\text{each with eigenvalue 0}} \\vec{w}_{k+1},\\dots,\\vec{w}_d.\\] Verifying this Lemma is left as an exercise. For each loading, we have a corresponding set of PCA scores and PCA variances. Importantly, since are data are \\(d\\)-dimensional and our loadings are \\(d\\)-mutually orthogonal unit vectors, they define a new basis in \\(\\mathbb{R}^d\\). Let’s continue our example above to see this process in the case where \\(d=3\\). Example 4.2 (Computing the Remaining PCA Loadings and Scores) We can continue this process by also removing, from each vector, its component in the direction of \\(\\vec{w}_2.\\) Finally, we can plot the data in the basis defined by the loadings. This is equivalent to a scatterplot of the scores. plot_ly(data.frame( x = y[,1], y= y[,2], z = y[,3]), x = ~x, y = ~y, z = ~z, marker = list(size = 5)) %&gt;% add_markers() %&gt;% layout(scene = list( xaxis = list(title = &quot;y&lt;sub&gt;1&lt;/sub&gt;&quot;), yaxis = list(title = &quot;y&lt;sub&gt;2&lt;/sub&gt;&quot;), zaxis = list(title = &quot;y&lt;sub&gt;3&lt;/sub&gt;&quot;)), title = &quot;Principal Component Scores&quot; ) In this basis, the data forms an ellipsoid with principal axis directed along the \\(y_1,\\,y_2, \\text{ and }y_3\\) axes. This is a result of the uncorrelated nature of principal component scores. 4.1.2 Derivation 2: Optimal Linear Subspace 4.2 Singular Value Decomposition yay 4.3 Nonnegative Matrix Factorization We will continue with the usual setting focusing on an \\(N\\times d\\) data matrix \\({\\bf X}\\). However, we will consider the additional assumption that each entry of the data matrix is non-negative which is a natural feature of many experimental data sets. As before, our goal is to find a low-rank matrix \\(\\hat{\\bf X}\\) which is as close to possible to \\({\\bf X}\\) as possible. How do we measure closness? Here are a few common choices. Frobenius norm \\(\\|{\\bf X}-\\hat{\\bf X}\\|_F.\\) Divergence \\(D({\\bf X} \\| \\hat{\\bf X}) = \\sum_{i=1}^N\\sum_{j=1}^d \\left[{\\bf X}_{ij} \\log \\frac{{\\bf X}_{ij}}{\\hat{\\bf X}_{ij}} + \\hat{\\bf X}_{ij} - {\\bf X}_{ij}\\right]\\) IS Divergence Without any restrictions on \\(\\hat{\\bf X}\\) our previous analysis using SVD provides the answer when we consider the Frobenius norm of the \\(ell_2\\) norm of the difference between \\({\\bf X}\\) and \\(\\hat{\\bf X}.\\) 4.4 Multidimensional Scaling 4.4.1 Classical Scaling 4.4.2 Metric MDS 4.4.3 Nonmetric MDS References "],["kernels-and-nonlinearity.html", "Chapter 5 Kernels and Nonlinearity 5.1 Exercises", " Chapter 5 Kernels and Nonlinearity 5.1 Exercises "],["ch-nonlinear.html", "Chapter 6 Manifold Learning 6.1 Background 6.2 Isometric Feature Map (ISOMAP) 6.3 Local Linear Embeddings (LLEs) 6.4 Autoencoders (AEs) 6.5 Additional methods 6.6 Exercises", " Chapter 6 Manifold Learning 6.1 Background In the previous sections, we have focused on methods which seek to approximate our data through a linear combination of feature vectors. As we have seen, the resulting approximations live on linear (or affine) subspaces in the case of PCA and SVD and positive spans or convex combinations in the case of NMF. While our data may exhibit some low-dimensional structure, there is no practical reason to expect such behavior to be inherently linear. In the resulting sections, we will explore methods which consider nonlinear structure and assume the data reside on or near a manifold. Such methods are referred to as nonlinear dimension reduction or manifold learning. Critical to this discussion is the notion of a manifold. Definition 6.1 (Informal Definition of a Manifold) A manifold is a (topological) space which locally resembles Euclidean space. Each point on a \\(k\\)-dimensional manifold has a neighborhood that can be mapped continuously to \\(\\mathbb{R}^k\\). To guide your intuition, think of a manifold as a smooth, possibly curved surface. Here are a few examples. Example 6.1 (Examples of Manifolds) Add line, sphere, plane, and S And here is an example of something which isn’t a manifold. Example 6.2 (Non-manifold) figure 8 Much more could be said about the mathematical foundations of manifolds which are far beyond the scope of this book. For those interested in the such details consider checking out REFERENCES HERE. We will appeal to a more intuitive understanding of manifolds and when necessary provide informal, descriptive “definitions” of important concepts. For now, let’s turn to the standard manifold assumption which is common to this area of unsupervised learning. 6.1.1 Data on a manifold In the simplest setting, we will assume there are points \\(\\vec{z}_1,\\dots,\\vec{z}_N\\in A \\subset \\mathbb{R}^k\\) which are iid random samples. These points are (nonlinearly) mapped into a higher dimensional space \\(\\mathbb{R}^d\\) by a smooth map \\(\\Psi\\) giving data \\(\\vec{x}_i = \\Psi(\\vec{z}_i)\\) for \\(i=1,\\dots,N.\\) Hereafter, we refer to \\(\\Psi\\) as the manifold map. In this setting, we are only given \\(\\vec{x}_1,\\dots,\\vec{x}_N\\), and we want to recover the lower-dimensional \\(\\vec{z}_1,\\dots,\\vec{z}_N\\). If possible, we would also like recover \\(\\Psi\\) and \\(\\Psi^{-1}\\) and in the most ideal case, the sampling distribution that generated the lower-dimensional coordinates \\(\\vec{z}_1,\\dots,\\vec{z}_N\\). Example 6.3 (Mapping to the Swiss Roll) Let \\(A = (\\pi/2,9\\pi/2)\\times (0,15)\\). We define the map \\(\\Psi:A\\to \\mathbb{R}^3\\) as follows \\[\\Psi(\\vec{z}) = \\Psi(z_1,z_2) = \\begin{bmatrix} z_1\\sin(z_1) \\\\ z_1\\cos(z_1) \\\\ z_2 \\end{bmatrix}\\] Below we show \\(N=10^4\\) samples which are drawn uniformly from \\(A\\). We then show the resulting observations after applying map \\(\\Psi\\) to each sample. We may also consider the more complicated case where the observations are corrupted by additive noise. In this setting, the typical assumption is that the noise follows after the manifold map so that our data are \\[\\vec{x}_i = \\Psi(\\vec{z}_i) + \\vec{\\epsilon}_i, \\qquad i = 1,\\dots, N\\] for some noise vectors \\(\\{\\vec{\\epsilon}_i\\}_{i=1,\\dots,N}.\\) Example 6.4 (Swiss Roll with Additive Gaussian Noise) Here, we perturb the observations in the preceding example with additive \\(\\mathcal{N}(\\vec{0},0.1{\\bf I})\\) noise. In addition to the goals in the noiseless case, we may also add the goal of learning the noiseless version of the data which reside on a manifold. However, there are a number of practical issues to this setup. First, the dimension, \\(k\\), of the original lower-dimensional points is typically unknown. Similar to previous methods, we could pick a value of \\(k\\) with the goal of visualization, base our choice off of prior knowledge, or run our algorithms different choices of \\(k\\) and compare the results. More advanced methods for estimating the true value of \\(k\\) are an open area of research (REFERENCES NEEDED). There is also a issue with the uniqueness problem statement. Given only the high dimensional observations, there is no way we could identify the original lower-dimensional points without more information. In fact, one could find an unlimited sources of equally suitable results. Here is the issue. Let \\(\\Phi:\\mathbb{R}^k\\to\\mathbb{R}^k\\) be some invertible function. As an example, you could think of \\(\\Phi\\) as defining a translation, reflection, rotation, or some composition of these operations. If our original observed data are \\(\\vec{x}_i = \\Psi(\\vec{z}_i)\\), our manifold learning algorithm could instead infer that the manifold map is \\(\\Psi \\circ \\Phi^{-1}\\) and the lower-dimensional points are \\(\\Phi(\\vec{z}_i)\\). This is a perfectly reasonable result since \\((\\Psi\\circ \\Phi^{-1}\\circ)\\Phi(\\vec{z}_i) = \\Psi(\\vec{z}_i)= \\vec{x}_i\\) for \\(i=1,\\dots,N\\), which is the only result we require. Without additional information, there is little we could do to address this issue. For the purposes of visualization, however, we will typically be most interested in the relationship between the lower-dimensional points rather than their specific location or orientation. As such, we need not be concerned about a manifold learning algorithm that provides a translated or rotated representation of \\(\\vec{z}_1,\\dots,\\vec{z}_N.\\) More complicated transformations of the lower-dimensional coordinates are of greater concern and may be addressed through additional assumptions about the manifold map \\(\\Psi.\\) In the following sections, we will review a small collection of different methods which address the manifold learning problem. This collection is by no means exhaustive so we provide a small list with associated references to conclude the chapter. 6.2 Isometric Feature Map (ISOMAP) The first method we cover is the Isometric Feature Map (ISOMAP), originally published by Tenenbaum, de Silva, and Langford in 2000 [6]. As suggested by the name, we will see that an assumption of isometry is central to this method. Prior to discussing the ISOMAP algorithm, let’s briefly discuss the notion of isometry through an example which motivates different notions of distance between two points. Example 6.5 (Distance between points on a Helix) Consider the helix map \\(\\Psi:\\mathbb{R}\\to\\mathbb{R}^3\\) given by the formula \\[\\begin{equation} \\Psi(t) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}}\\cos(t) \\\\ \\frac{1}{\\sqrt{2}}\\sin(t) \\\\ \\frac{1}{\\sqrt{2}}t \\\\ \\end{bmatrix} \\end{equation}\\] Below, we show the result of applying the Helix map to each point in the interval \\((0,25)\\). Let’s focus on two points \\(\\vec{x}_1 = \\Psi(2\\pi)= (1/\\sqrt{2},0,\\sqrt{2}\\pi)^T\\) and \\(\\vec{x}_2 = \\Psi(4\\pi)=(1/\\sqrt{2},0,2\\sqrt{2}\\pi)^T\\) in particular which are shown as large black dots in the figure below. There are a few different ways we could measure the distance between the two black points. The first approach would be to ignore the helix (manifold) structure viewing them as vectors in \\(\\mathbb{R}^3\\) and directly measure their Euclidean distance which gives \\[\\|\\vec{x}_1 - \\vec{x}_2\\| = \\sqrt{2}\\pi.\\] However, we also know that these points are images of the one-dimensional coordinate \\(z_1 = 2\\pi\\) and \\(z_2 = 4\\pi\\) respectively. Thus, we could also consider the Euclidean distance of the lower-dimemsional coordiates which is \\(|2\\pi - 4\\pi| = 2\\pi\\) which notably differs from the Euclidean distance. A third option is to return to the three-dimensional representation but to also account for the manifold structure when considering distance. Recall Euclidean distance gives the length of the shortest, straightline path connecting the two points. Instead, let’s restrict ourselves to only those paths which stay on the helix (manifold). You may correctly conclude that the curve starting at \\(\\Psi(2\\pi)\\), rotating up the helix one rotation, and ending at \\(\\Psi(4\\pi)\\) is the shortest such path. Fortunately, computing arclength is relatively friendly in this example since \\(\\Psi\\) already parameterizes the path connecting these two points. The arclength is then \\[\\int_{2\\pi}^{4\\pi} \\left\\|\\frac{d\\Psi}{dt}\\right\\| dt = \\int_{2\\pi}^{4\\pi} dt = 2\\pi.\\] Jumping slightly ahead, we then say the manifold distance between \\(\\Psi(2\\pi)\\) and \\(\\Psi(4\\pi)\\) is \\(2\\pi\\). Importantly, the manifold distance coincides exactly with the Euclidean distance between the lower-dimensional coordinates. In fact, for any two points, \\(s\\) and \\(t\\), on the real line their Euclidean distance, \\(|s-t|\\) will be the same as the manifold distance between \\(\\Psi(s)\\) and \\(\\Psi(t)\\). Thus, the helix map \\(\\Psi\\) above serves as our first example of an isometric (distance preserving) map. We may generalize this idea to any smooth manifold to define a new notion of distance. Given a manifold \\(\\mathcal{M}\\), we define the manifold distance function \\(d_\\mathcal{M} : \\mathcal{M} \\times \\mathcal{M} \\to [0,\\infty)\\) as follows Definition 6.2 (Manifold Distance Function) Given two points \\(\\vec{x}\\) and \\(\\vec{y}\\) on a smooth manifold, \\(\\mathcal{M}\\), let \\(\\Gamma(\\vec{x},\\vec{y})\\) be the set of all piecewise smooth curves connecting \\(\\vec{x}\\) and \\(\\vec{y}\\) constrained to stay on \\(\\mathcal{M}\\). Then, we define the manifold distance to be \\[\\begin{equation} d_\\mathcal{M}(\\vec{x},\\vec{y}) = \\inf_{\\gamma \\in \\Gamma(\\vec{x},\\vec{y})} L(\\gamma) \\tag{6.1} \\end{equation}\\] where \\(L(\\gamma)\\) is the arclength of \\(\\gamma.\\) As we reviewed above, the helix example with the arclength formula is one example of a manifold and distance function. Additional examples of a manifold and manifold distance include, Euclidean space \\(\\mathbb{R}^d\\) where standard Euclidean distance gives the manifold distance. The sphere in \\(\\mathbb{R}^3\\) which is a two-dimensional manifold. Its manifold distance is also called the Great Circle Distance. We may now define the notion of isometry which is a central assumption of ISOMAP. Definition 6.3 (Isometry) Let \\(\\mathcal{M}_1\\) be a manifold with distance function \\(d_{\\mathcal{M}_1}\\) and let \\(\\mathcal{M}_2\\) be a second manifold with distance function \\(d_{\\mathcal{M}_2}\\). The mapping \\(\\Psi:\\mathcal{M}_1 \\mapsto \\mathcal{M}_2\\) is an isometry if \\[d_{\\mathcal{M}_1}(x,y) = d_{\\mathcal{M}_2}\\left(\\Psi(x),\\Psi(y)\\right) \\qquad \\text{ for all } x,y\\in \\mathcal{M}_1.\\] For the purposes of ISOMAP, we will think of \\(\\mathcal{M}_1\\) as some subset of a \\(\\mathbb{R}^k\\) for \\(k\\) small where we measure distances using the Euclidean norm. Then \\(\\mathcal{M}_2\\) will be a \\(k\\)-dimensional manifold in \\(\\mathbb{R}^d\\) containing our data. Our first assumption is that the manifold mapping \\(\\Psi\\) is an isometry. Unfortunately, in practice we do not know the manifold nor will we have a method for parameterizing curves on the manifold to compute distances. Instead, ISOMAP makes use of a data-driven approach to estimate the manifold distance between points following a three step procedure. Fix tuning parameters \\(k &gt;0\\) or \\(\\epsilon &gt;0\\). Calculate all pairwise Euclidean distance \\(d_{ij} = \\|\\vec{x}_i - \\vec{x}_j \\|.\\) Define a (directed) neighborhood graph \\(G\\) by connecting nodes \\(i\\) and \\(j\\) {}, connect \\(\\vec{x}_i\\) to \\(\\vec{x}_j\\) if \\(\\vec{x}_j\\) is one of the \\(k\\) nearest points to \\(\\vec{x}_i\\) as measured by \\(d_{ij}\\). {}, connect \\(\\vec{x}_i\\) and _j$ if \\(d_ij &lt; \\epsilon.\\) The distances found in step 2 provide a distance matrix in which each entry approximates the manifold distance. Under the assumption that the unknown manifold map is an isometry, we can seek a lower-dimensinoal collection of points which have these pairwise distance. By default, we use classical scaling to find such a configuration. As discussed in Chapter 4.4, we have a f For visualization, we may typically choose a dimension (usually one, two, or three) and find the corresponding classical MDS solution. Alternatively, we use the eigenvalues of the inner product matrix based on the graph distances to choose a suitable cutoff for the appropriate 6.3 Local Linear Embeddings (LLEs) 6.4 Autoencoders (AEs) 6.5 Additional methods 6.6 Exercises References "],["ch-clustering.html", "Chapter 7 Clustering 7.1 Hierarchical 7.2 Center-based 7.3 Model-based 7.4 Spectral", " Chapter 7 Clustering 7.1 Hierarchical 7.2 Center-based 7.3 Model-based 7.3.1 k-means 7.3.2 k-mediods 7.4 Spectral "]]
