<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.4 Autoencoders (AEs) | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="6.4 Autoencoders (AEs) | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.4 Autoencoders (AEs) | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="locally-linear-embeddings-lles.html"/>
<link rel="next" href="additional-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-1-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation 1: Iterative Projections</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>5.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>6.1</b> Background</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="background.html"><a href="background.html#data-on-a-manifold"><i class="fa fa-check"></i><b>6.1.1</b> Data on a manifold</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.4</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-2"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.4.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.5</b> Additional methods</a></li>
<li class="chapter" data-level="6.6" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hierarchical.html"><a href="hierarchical.html"><i class="fa fa-check"></i><b>7.1</b> Hierarchical</a></li>
<li class="chapter" data-level="7.2" data-path="center-based.html"><a href="center-based.html"><i class="fa fa-check"></i><b>7.2</b> Center-based</a></li>
<li class="chapter" data-level="7.3" data-path="model-based.html"><a href="model-based.html"><i class="fa fa-check"></i><b>7.3</b> Model-based</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-based.html"><a href="model-based.html#k-means"><i class="fa fa-check"></i><b>7.3.1</b> k-means</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-based.html"><a href="model-based.html#k-mediods"><i class="fa fa-check"></i><b>7.3.2</b> k-mediods</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>7.4</b> Spectral</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="autoencoders-aes" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Autoencoders (AEs)<a href="autoencoders-aes.html#autoencoders-aes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introduction-2" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Introduction<a href="autoencoders-aes.html#introduction-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Autoencoders, which originated from the domain of neural network research, represent a class of unsupervised deep learning models. At its core, autoencoders seek to learn a compressed, efficient representation of input data by leveraging a network structure that nonlinearly encodes the data into a lower dimension space and subsequently decodes it to reconstruct the original data. <span class="citation">[<a href="#ref-NN">16</a>]</span></p>
<p>The typical architecture of an autoencoder comprises three main components:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Encoder</strong>: A function <span class="math inline">\(f(\vec{x})\)</span> that compresses the input <span class="math inline">\(\vec{x}\)</span> into a latent representation.</p></li>
<li><p><strong>Latent Space</strong>: The reduced dimensionality representation, often denoted as <span class="math inline">\(\vec{z}\)</span>, where <span class="math inline">\(\vec{z} = f(\vec{x})\)</span>.</p></li>
<li><p><strong>Decoder</strong>: A function <span class="math inline">\(g(\vec{z})\)</span> that aims to reconstruct the original input from the latent representation.</p></li>
</ol>
<p>The primary objective during the training phase of an autoencoder is to minimize the reconstruction error, often quantified using metrics such as Mean Squared Error (MSE) between the input data and its reconstructed counterpart. The minimization forces the model to capture salient features of the data in the latent space, thereby enabling efficient data compression, noise reduction, and feature extraction.</p>
<p>The utility of Autoencoders has been demonstrated in a wide array of applications, from dimensionality reduction, anomaly detection, denoising, to more complex tasks such as generating new data instances. Variations and extensions of the basic Autoencoder model, including Variational Autoencoders (VAEs) <span class="citation">[<a href="#ref-VAE">17</a>]</span> and Denoising Autoencoders <span class="citation">[<a href="#ref-DenoiseAE">18</a>]</span>, have further broadened their applicability by introducing probabilistic interpretations and noise robustness, respectively.</p>
<p>In the broader context of machine learning and artificial intelligence, Autoencoders exemplify the power of unsupervised learning paradigms, emphasizing the capability of neural networks to derive meaningful representations from data without explicit labeling.</p>
</div>
<div id="algorithm-2" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Algorithm<a href="autoencoders-aes.html#algorithm-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="notations" class="section level4 hasAnchor" number="6.4.2.1">
<h4><span class="header-section-number">6.4.2.1</span> Notations<a href="autoencoders-aes.html#notations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Input</strong>
- Dataset <span class="math inline">\(X = \{\vec{x}_1, \vec{x}_2, \dots \vec{x}_n\}\)</span> where <span class="math inline">\(\vec{x}_i\)</span> is a vectorized representation of each data sample.
- Encoder neural network <span class="math inline">\(E\)</span> with parameters <span class="math inline">\(\theta_e\)</span> mapping inputs to latent space.
- Decoder neural network <span class="math inline">\(D\)</span> with parameters <span class="math inline">\(\theta_d\)</span> mapping latent space back to the original space.
- Reconstruction loss function <span class="math inline">\(L\)</span>, e.g., Mean Squared Error (MSE).</p>
<p><strong>Output</strong>:
- Optimized parameters <span class="math inline">\(\theta_e^*\)</span> for <span class="math inline">\(E\)</span> and <span class="math inline">\(\theta_d^*\)</span> for <span class="math inline">\(D\)</span>.</p>
</div>
<div id="steps-breakdown" class="section level4 hasAnchor" number="6.4.2.2">
<h4><span class="header-section-number">6.4.2.2</span> Steps Breakdown<a href="autoencoders-aes.html#steps-breakdown" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Initialization</strong>:
<ul>
<li>Initialize encoder <span class="math inline">\(E\)</span> and decoder <span class="math inline">\(D\)</span> neural network weights using techniques like Xavier or He initialization to ensure weight variance remains controlled during training.</li>
</ul></li>
<li><strong>Forward Propagation</strong>:
For each data sample <span class="math inline">\(\vec{x}_i\)</span>:
<ul>
<li>Use encoder <span class="math inline">\(E\)</span> to transform <span class="math inline">\(\vec{x}_i\)</span> to latent space:
<span class="math display">\[ \vec{z}_i = E_{\theta_e}(\vec{x}_i) \]</span></li>
<li>Use decoder <span class="math inline">\(D\)</span> to reconstruct <span class="math inline">\(\vec{x}_i\)</span> from <span class="math inline">\(\vec{z}_i\)</span>:
<span class="math display">\[ \vec{x}^{\prime}_i = D_{\theta_d}(\vec{z}_i) \]</span></li>
</ul></li>
<li><strong>Loss Computation</strong>:
<ul>
<li>Calculate reconstruction loss for the given sample using <span class="math inline">\(L\)</span>:
<span class="math display">\[ L_i = L(\vec{x}_i, \vec{x}^{\prime}_i) \]</span></li>
<li>Compute average loss for the batch:
<span class="math display">\[ L_{batch} = \frac{1}{N} \sum_{i=1}^{N} L_i \]</span>
where <span class="math inline">\(N\)</span> is the batch size.</li>
</ul></li>
<li><strong>Backward Propagation</strong>:
<ul>
<li>Compute the gradient of <span class="math inline">\(L_{batch}\)</span> with respect to encoder and decoder parameters using back-propagation.</li>
<li>Use gradient descent (or its variants) to update both <span class="math inline">\(\theta_e\)</span> and <span class="math inline">\(\theta_d\)</span>:
<span class="math display">\[ \theta_e = \theta_e - \alpha \frac{\partial L_{batch}}{\partial \theta_e} \]</span>
<span class="math display">\[ \theta_d = \theta_d - \alpha \frac{\partial L_{batch}}{\partial \theta_d} \]</span>
where <span class="math inline">\(\alpha\)</span> is the learning rate.</li>
</ul></li>
<li><strong>Training Loop</strong>:
<ul>
<li>Execute steps 2-4 for multiple iterations (epochs) over the entire data set. Regularly shuffle the data set before each epoch to ensure IID (independent and identically distributed) property.</li>
</ul></li>
<li><strong>Model Retrieval</strong>:
<ul>
<li>Once convergence is achieved, i.e., when the reduction in <span class="math inline">\(L_{batch}\)</span> between subsequent epochs is negligible or after a pre-defined number of epochs, extract the optimized parameters <span class="math inline">\(\theta_e^*\)</span> and <span class="math inline">\(\theta_d^*\)</span>. These define the trained Autoencoder.</li>
</ul></li>
<li><strong>Optional Fine-tuning</strong>:
<ul>
<li>For better performance, further fine-tune the trained Autoencoder using techniques like early stopping based on validation loss, or introduce regularization techniques like dropout to prevent overfitting.</li>
</ul></li>
</ol>
</div>
</div>
<div id="example" class="section level3 hasAnchor" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Example<a href="autoencoders-aes.html#example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compared to other manifold learning methods we have introduced (ISOMAP and LLE), the implementation and parameter tuning of Autoencoders is much more complicated. We’ll explore how to implement an autoencoder in R using the <strong>Keras</strong> library with the famous MNIST dataset. As a Neuro Network method, Autoencoders have a number of parameters, thus requiring a large dataset to train. This explains why we choose MNIST dataset here.</p>
<ol style="list-style-type: decimal">
<li>Load and Preprocess the Data</li>
</ol>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="autoencoders-aes.html#cb29-1" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span>
<span id="cb29-2"><a href="autoencoders-aes.html#cb29-2" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span></code></pre></div>
<p>The above code loads the MNIST dataset from <strong>keras</strong> package. mnist variable is divided into a training set and a test set, both containing the image information and label information. Since we are using Autoencoders to conduct dimensio reduction here (unsupervised learning), we only require the image information from the traing set. The train_images contains the images from the training set.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="autoencoders-aes.html#cb30-1" tabindex="-1"></a><span class="co"># Flatten and normalize the images</span></span>
<span id="cb30-2"><a href="autoencoders-aes.html#cb30-2" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(train_images, <span class="fu">c</span>(<span class="fu">nrow</span>(train_images), <span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>))</span>
<span id="cb30-3"><a href="autoencoders-aes.html#cb30-3" tabindex="-1"></a>train_images <span class="ot">&lt;-</span> train_images <span class="sc">/</span> <span class="dv">255</span></span></code></pre></div>
<p>For an Autoencoder to process these images, they need to be reshaped into a flat vector. This step reshapes each 28x28 image into a vector of length 784. Furthermore, to facilitate the neural network’s training, the pixel values (originally between 0 to 255) are normalized to fall between 0 and 1.</p>
<ol start="2" style="list-style-type: decimal">
<li>Build the Autoencoders</li>
</ol>
<p>The autoencoder comprises two main parts: Encoder and Decoder, as we have stated before.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="autoencoders-aes.html#cb31-1" tabindex="-1"></a><span class="co"># initiates the input layer for the autoencoder which expects input vectors of length 784</span></span>
<span id="cb31-2"><a href="autoencoders-aes.html#cb31-2" tabindex="-1"></a>input_img <span class="ot">&lt;-</span> <span class="fu">layer_input</span>(<span class="at">shape =</span> <span class="fu">c</span>(<span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>)) </span>
<span id="cb31-3"><a href="autoencoders-aes.html#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a href="autoencoders-aes.html#cb31-4" tabindex="-1"></a><span class="co"># Encoder</span></span>
<span id="cb31-5"><a href="autoencoders-aes.html#cb31-5" tabindex="-1"></a>encoded <span class="ot">&lt;-</span> input_img <span class="sc">%&gt;%</span> </span>
<span id="cb31-6"><a href="autoencoders-aes.html#cb31-6" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb31-7"><a href="autoencoders-aes.html#cb31-7" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate=</span><span class="fl">0.8</span>) <span class="sc">%&gt;%</span></span>
<span id="cb31-8"><a href="autoencoders-aes.html#cb31-8" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">2</span>, <span class="at">activation =</span> <span class="st">&quot;tanh&quot;</span>)</span></code></pre></div>
<p>The Encoder starts with the input image and passes it through a dense layer with 128 neurons and ReLU activation. A dropout layer is added for regularization, reducing the risk of over-fitting. The last layer of the Encoder further compresses the data into a 2D representation.</p>
<p><strong>Dropout Layer</strong>
In the context of Keras, layer_dropout(rate=0.8) adds a dropout layer where there’s an 80% chance that any given neuron in the previous layer will be turned off during training for a particular update.</p>
<p>In practice, it means that during training step, for each training iteration, <span class="math inline">\(80\%\)</span> of the neurons of the preceding layer will be randomly selected and turned off. These turned-off neurons won’t have any influence on the computation for that specific iteration. Dropout is only active during training, during testing step, all neurons will be active, and dropout won’t be applied.</p>
<p>This step is super important, if dropout layer is not introduced, the model will be largely over-fitted. You may try this yourself.</p>
<p><strong>Activation Function</strong></p>
<p>Activation functions play a vital role in neural networks, determining the output of a neuron based on its input. They introduce non-linear properties into the network, enabling it to learn from the error and make adjustments, which is essential for learning complex patterns.</p>
<p>There are several common activation functions we can choose from.</p>
<ol style="list-style-type: decimal">
<li><strong>Sigmoid (Logistic) Function:</strong></li>
</ol>
<p>Equation: <span class="math inline">\(f(x) = \frac{1}{1 + e^{-x}}\)</span></p>
<p>Range: Between 0 and 1</p>
<p>Pros: Smooth gradient, preventing “jumps” in output values</p>
<p>Cons: Can cause vanishing gradient problems in deep networks</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Hyperbolic Tangent (tanh) Function:</strong></li>
</ol>
<p>Equation: <span class="math inline">\(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span></p>
<p>Range: Between -1 and 1</p>
<p>Pros: Zero-centered, making it easier for the model to generalize</p>
<p>Cons: Like the sigmoid, it can also lead to vanishing gradient issues</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Rectified Linear Unit (ReLU) Function:</strong></li>
</ol>
<p>Equation: <span class="math inline">\(f(x) = max(0, x)\)</span></p>
<p>Range: From 0 to infinity</p>
<p>Pros: Helps mitigate the vanishing gradient problem, leading to faster convergence</p>
<p>Cons: Neurons can sometimes “die”, especially with a large learning rate</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Softmax Function:</strong></li>
</ol>
<p>Equation: <span class="math inline">\(\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}\)</span> for <span class="math inline">\(j = 1, ..., K\)</span></p>
<p>Used primarily in the output layer of a classifier, where it turns logits into probabilities by distributing them over several classes</p>
<p>Range: Between 0 to 1</p>
<p>Each activation function has its unique characteristics and is suitable for different kinds of tasks. The choice of an activation function depends on the specific problem at hand, the nature of the data, and the architecture of the neural network.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="autoencoders-aes.html#cb32-1" tabindex="-1"></a><span class="co"># Decoder</span></span>
<span id="cb32-2"><a href="autoencoders-aes.html#cb32-2" tabindex="-1"></a>decoded <span class="ot">&lt;-</span> encoded <span class="sc">%&gt;%</span> </span>
<span id="cb32-3"><a href="autoencoders-aes.html#cb32-3" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb32-4"><a href="autoencoders-aes.html#cb32-4" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate=</span><span class="fl">0.8</span>) <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="autoencoders-aes.html#cb32-5" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">28</span><span class="sc">*</span><span class="dv">28</span>, <span class="at">activation =</span> <span class="st">&quot;tanh&quot;</span>)</span>
<span id="cb32-6"><a href="autoencoders-aes.html#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a href="autoencoders-aes.html#cb32-7" tabindex="-1"></a><span class="co"># combines the encoder and decoder into the Autoencoder model</span></span>
<span id="cb32-8"><a href="autoencoders-aes.html#cb32-8" tabindex="-1"></a>autoencoder <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(<span class="at">input =</span> input_img, <span class="at">output =</span> decoded)</span></code></pre></div>
<p>The Decoder takes the encoded 2D representation and attempts to reconstruct the original image. It uses a dense layer with 128 neurons followed by another dropout layer. The final dense layer outputs vectors of length <span class="math inline">\(784 \, (28 \times 28)\)</span>, matching the original image’s shape.</p>
<ol start="3" style="list-style-type: decimal">
<li>Compile and Train Autoencoders</li>
</ol>
<p>Before training, the model needs to be compiled. The ‘adam’ optimizer and the ‘mean_squared_error’ loss function are chosen for this task. You may choose different loss functions according to your specific data type and objective, here MSE might be the most suitable.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="autoencoders-aes.html#cb33-1" tabindex="-1"></a>autoencoder <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb33-2"><a href="autoencoders-aes.html#cb33-2" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&#39;adam&#39;</span>, </span>
<span id="cb33-3"><a href="autoencoders-aes.html#cb33-3" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span></span>
<span id="cb33-4"><a href="autoencoders-aes.html#cb33-4" tabindex="-1"></a>)</span>
<span id="cb33-5"><a href="autoencoders-aes.html#cb33-5" tabindex="-1"></a></span>
<span id="cb33-6"><a href="autoencoders-aes.html#cb33-6" tabindex="-1"></a><span class="fu">summary</span>(autoencoder)</span></code></pre></div>
<pre><code>## Model: &quot;model&quot;
## ______________________________________________________________________________________________________________________________________________________________________________
##  Layer (type)                                                                 Output Shape                                                         Param #                    
## ==============================================================================================================================================================================
##  input_1 (InputLayer)                                                         [(None, 784)]                                                        0                          
##  dense_1 (Dense)                                                              (None, 128)                                                          100480                     
##  dropout (Dropout)                                                            (None, 128)                                                          0                          
##  dense (Dense)                                                                (None, 2)                                                            258                        
##  dense_3 (Dense)                                                              (None, 128)                                                          384                        
##  dropout_1 (Dropout)                                                          (None, 128)                                                          0                          
##  dense_2 (Dense)                                                              (None, 784)                                                          101136                     
## ==============================================================================================================================================================================
## Total params: 202,258
## Trainable params: 202,258
## Non-trainable params: 0
## ______________________________________________________________________________________________________________________________________________________________________________</code></pre>
<p>The Autoencoder is trained using the training images as both inputs and targets. This is because the Autoencoder’s goal is to reconstruct its input as closely as possible.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="autoencoders-aes.html#cb35-1" tabindex="-1"></a>history <span class="ot">&lt;-</span> autoencoder <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb35-2"><a href="autoencoders-aes.html#cb35-2" tabindex="-1"></a>  train_images, train_images, </span>
<span id="cb35-3"><a href="autoencoders-aes.html#cb35-3" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">40</span>, </span>
<span id="cb35-4"><a href="autoencoders-aes.html#cb35-4" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">256</span>, </span>
<span id="cb35-5"><a href="autoencoders-aes.html#cb35-5" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span>,</span>
<span id="cb35-6"><a href="autoencoders-aes.html#cb35-6" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span>,</span>
<span id="cb35-7"><a href="autoencoders-aes.html#cb35-7" tabindex="-1"></a>  <span class="fu">getOption</span>(<span class="st">&quot;keras.fit_verbose&quot;</span>, <span class="at">default =</span> <span class="dv">0</span>)</span>
<span id="cb35-8"><a href="autoencoders-aes.html#cb35-8" tabindex="-1"></a>)</span>
<span id="cb35-9"><a href="autoencoders-aes.html#cb35-9" tabindex="-1"></a></span>
<span id="cb35-10"><a href="autoencoders-aes.html#cb35-10" tabindex="-1"></a><span class="fu">plot</span>(history)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Visualize the Encoded Representations</li>
</ol>
<p>To visualize the 2D representations learned by the Encoder, a separate Encoder model is created. This model shares its weights with the Autoencoders’ Encoder portion. It’s then used to predict or transform the training images into their 2D encoded form.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="autoencoders-aes.html#cb36-1" tabindex="-1"></a>encoder <span class="ot">&lt;-</span> <span class="fu">keras_model</span>(<span class="at">inputs =</span> input_img, <span class="at">outputs =</span> encoded)</span>
<span id="cb36-2"><a href="autoencoders-aes.html#cb36-2" tabindex="-1"></a>encoded_images <span class="ot">&lt;-</span> <span class="fu">predict</span>(encoder, train_images)</span></code></pre></div>
<pre><code>## 1875/1875 - 1s - 600ms/epoch - 320us/step</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="autoencoders-aes.html#cb38-1" tabindex="-1"></a><span class="co"># Convert to a data frame for easier viewing</span></span>
<span id="cb38-2"><a href="autoencoders-aes.html#cb38-2" tabindex="-1"></a>encoded_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(encoded_images)</span>
<span id="cb38-3"><a href="autoencoders-aes.html#cb38-3" tabindex="-1"></a><span class="fu">head</span>(encoded_df)</span></code></pre></div>
<pre><code>##             V1         V2
## 1  0.784608781  0.2318616
## 2  0.958282411  0.9580116
## 3 -0.375284046  0.6486747
## 4  0.003874707 -0.9999937
## 5 -0.999934554  0.3062524
## 6  0.099140599  0.8832416</code></pre>
<p>Using the ggplot2 package, the #D encoded images are visualized, color-coded by their true labels. This visualization can provide insights into how well the encoder has learned to cluster different digits in this reduced-dimensional space. We also compare the result of AutoEncoder with that of PCA, as shown below.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="autoencoders-aes.html#cb40-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb40-2"><a href="autoencoders-aes.html#cb40-2" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb40-3"><a href="autoencoders-aes.html#cb40-3" tabindex="-1"></a>labels_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb40-4"><a href="autoencoders-aes.html#cb40-4" tabindex="-1"></a>labels_train <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(labels_train)</span>
<span id="cb40-5"><a href="autoencoders-aes.html#cb40-5" tabindex="-1"></a></span>
<span id="cb40-6"><a href="autoencoders-aes.html#cb40-6" tabindex="-1"></a><span class="co"># Plotting the data</span></span>
<span id="cb40-7"><a href="autoencoders-aes.html#cb40-7" tabindex="-1"></a>ae <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(encoded_df, <span class="fu">aes</span>(<span class="at">x =</span> V1, <span class="at">y =</span> V2, <span class="at">color =</span> labels_train)) <span class="sc">+</span></span>
<span id="cb40-8"><a href="autoencoders-aes.html#cb40-8" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb40-9"><a href="autoencoders-aes.html#cb40-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Autoencoder projection&quot;</span>,</span>
<span id="cb40-10"><a href="autoencoders-aes.html#cb40-10" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;DF1&quot;</span>,</span>
<span id="cb40-11"><a href="autoencoders-aes.html#cb40-11" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;DF2&quot;</span>,</span>
<span id="cb40-12"><a href="autoencoders-aes.html#cb40-12" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">&quot;Response&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-13"><a href="autoencoders-aes.html#cb40-13" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb40-14"><a href="autoencoders-aes.html#cb40-14" tabindex="-1"></a><span class="co"># plotting 3d results</span></span>
<span id="cb40-15"><a href="autoencoders-aes.html#cb40-15" tabindex="-1"></a><span class="co"># ae3 &lt;- plot_ly(encoded_df, x=~V1, y=~V2, z =~V3, </span></span>
<span id="cb40-16"><a href="autoencoders-aes.html#cb40-16" tabindex="-1"></a><span class="co">#                 color = labels_train,</span></span>
<span id="cb40-17"><a href="autoencoders-aes.html#cb40-17" tabindex="-1"></a><span class="co">#                 size = 0.1)</span></span></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="autoencoders-aes.html#cb41-1" tabindex="-1"></a>pca_result <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(train_images)</span>
<span id="cb41-2"><a href="autoencoders-aes.html#cb41-2" tabindex="-1"></a></span>
<span id="cb41-3"><a href="autoencoders-aes.html#cb41-3" tabindex="-1"></a><span class="co"># Extract the scores of the first two principal components</span></span>
<span id="cb41-4"><a href="autoencoders-aes.html#cb41-4" tabindex="-1"></a>scores <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(pca_result<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span>
<span id="cb41-5"><a href="autoencoders-aes.html#cb41-5" tabindex="-1"></a><span class="fu">colnames</span>(scores) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb41-6"><a href="autoencoders-aes.html#cb41-6" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(scores, <span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2, <span class="at">color =</span> labels_train)) <span class="sc">+</span></span>
<span id="cb41-7"><a href="autoencoders-aes.html#cb41-7" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb41-8"><a href="autoencoders-aes.html#cb41-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;PCA&quot;</span>, <span class="at">x =</span> <span class="st">&quot;PC1&quot;</span>, <span class="at">y =</span> <span class="st">&quot;PC2&quot;</span>, <span class="at">color=</span><span class="st">&quot;response&quot;</span>) <span class="sc">+</span></span>
<span id="cb41-9"><a href="autoencoders-aes.html#cb41-9" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb41-10"><a href="autoencoders-aes.html#cb41-10" tabindex="-1"></a><span class="co"># # 3D </span></span>
<span id="cb41-11"><a href="autoencoders-aes.html#cb41-11" tabindex="-1"></a><span class="co"># scores &lt;- as.data.frame(pca_result$x[, 1:3])</span></span>
<span id="cb41-12"><a href="autoencoders-aes.html#cb41-12" tabindex="-1"></a><span class="co"># colnames(scores) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;)</span></span>
<span id="cb41-13"><a href="autoencoders-aes.html#cb41-13" tabindex="-1"></a><span class="co"># p3 &lt;- plot_ly(scores, x = ~PC1, y =~PC2, z=~PC3, </span></span>
<span id="cb41-14"><a href="autoencoders-aes.html#cb41-14" tabindex="-1"></a><span class="co">#               color = labels_train,</span></span>
<span id="cb41-15"><a href="autoencoders-aes.html#cb41-15" tabindex="-1"></a><span class="co">#               size = 0.1)</span></span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="autoencoders-aes.html#cb42-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb42-2"><a href="autoencoders-aes.html#cb42-2" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="autoencoders-aes.html#cb43-1" tabindex="-1"></a>ae</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-56-2.png" width="672" /></p>
<p><strong>Alternative Choices for the use of R packages for Autoencoders</strong></p>
<p>Besides package <strong>keras</strong> we have used as an example, <strong>h2o</strong> package is also a popular package for implementing Autoencoders. Unlike h2o package has an automated machine learning (AutoML) feature, making it easy for users to train and compare multiple models. In addition, it requires less manual tuning and configuration for deep learning, making it more accessible for beginners or those not primarily focused on deep learning. You may have a try yourself if you are interested after class. <a href="https://cran.r-project.org/web/packages/h2o/index.html" class="uri">https://cran.r-project.org/web/packages/h2o/index.html</a> (Keep in mind that h2o is Java-based, so having Java installed is a prerequisite of using h2o package)</p>
<p>We choose keras here because it allows the development of custom layers, loss functions, and metrics, enabling you to have a clearer understanding of the mechanism and implementation of Autoencoders.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-NN" class="csl-entry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline"><span class="smallcaps">Hinton</span>, G. E. and <span class="smallcaps">Salakhutdinov</span>, R. R. (2006). <a href="https://doi.org/10.1126/science.1127647">Reducing the dimensionality of data with neural networks</a>. <em>Science</em> <strong>313</strong> 504–7.</div>
</div>
<div id="ref-VAE" class="csl-entry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline"><span class="smallcaps">Kingma</span>, D. P. and <span class="smallcaps">Welling</span>, M. (2013). <a href="https://api.semanticscholar.org/CorpusID:216078090">Auto-encoding variational bayes</a>. <em>CoRR</em> <strong>abs/1312.6114</strong>.</div>
</div>
<div id="ref-DenoiseAE" class="csl-entry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline"><span class="smallcaps">Vincent</span>, P., <span class="smallcaps">Larochelle</span>, H., <span class="smallcaps">Bengio</span>, Y. and <span class="smallcaps">Manzagol</span>, P.-A. (2008). <a href="https://doi.org/10.1145/1390156.1390294">Extracting and composing robust features with denoising autoencoders</a>. In ICML ’08 pp 1096–103. Association for Computing Machinery.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="locally-linear-embeddings-lles.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="additional-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-nonlinear_methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
