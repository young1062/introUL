<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="ch-prob.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.4/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-prob.html"><a href="ch-prob.html#important-notation"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="ch-prob.html"><a href="ch-prob.html#random-vectors-in-mathbbrd"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="ch-prob.html"><a href="ch-prob.html#expectation-mean-and-covariance"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ch-prob.html"><a href="ch-prob.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="ch-prob.html"><a href="ch-prob.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-prob.html"><a href="ch-prob.html#linear-algebra"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch-prob.html"><a href="ch-prob.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-prob.html"><a href="ch-prob.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="ch-prob.html"><a href="ch-prob.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="ch-prob.html"><a href="ch-prob.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="ch-prob.html"><a href="ch-prob.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="ch-prob.html"><a href="ch-prob.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
<li class="chapter" data-level="2.4.7" data-path="ch-prob.html"><a href="ch-prob.html#hadamard-elementwise-operations"><i class="fa fa-check"></i><b>2.4.7</b> Hadamard (elementwise) operations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-prob.html"><a href="ch-prob.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ch-prob.html"><a href="ch-prob.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-prob.html"><a href="ch-prob.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="ch-prob.html"><a href="ch-prob.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="ch-prob.html"><a href="ch-prob.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#dimension-reduction-and-manifold-learning"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#clustering"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#generating-synthetic-data"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html#exercises-1"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ch-linear.html"><a href="ch-linear.html#sec-pca"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ch-linear.html"><a href="ch-linear.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
<li class="chapter" data-level="4.1.2" data-path="ch-linear.html"><a href="ch-linear.html#pca-in-practice"><i class="fa fa-check"></i><b>4.1.2</b> PCA in Practice</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-linear.html"><a href="ch-linear.html#sec-svd"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ch-linear.html"><a href="ch-linear.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="ch-linear.html"><a href="ch-linear.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="ch-linear.html"><a href="ch-linear.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="ch-linear.html"><a href="ch-linear.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-linear.html"><a href="ch-linear.html#nonnegative-matrix-factorization"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ch-linear.html"><a href="ch-linear.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-linear.html"><a href="ch-linear.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="ch-linear.html"><a href="ch-linear.html#finding-a-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding a NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="ch-linear.html"><a href="ch-linear.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="ch-linear.html"><a href="ch-linear.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
<li class="chapter" data-level="4.3.6" data-path="ch-linear.html"><a href="ch-linear.html#nmf-and-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.3.6</b> NMF and Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-linear.html"><a href="ch-linear.html#sec-mds"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ch-linear.html"><a href="ch-linear.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-linear.html"><a href="ch-linear.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-linear.html"><a href="ch-linear.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="ch-linear.html"><a href="ch-linear.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-linear.html"><a href="ch-linear.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html#kernel-pca"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html#exercises-3"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#the-manifold-hypothesis"><i class="fa fa-check"></i><b>6.1</b> The Manifold Hypothesis</a></li>
<li class="chapter" data-level="6.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#sec-manifolds"><i class="fa fa-check"></i><b>6.2</b> Brief primer on manifolds</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#charts-atlases-tangent-spaces-and-approximating-tangent-planes"><i class="fa fa-check"></i><b>6.2.1</b> Charts, atlases, tangent spaces and approximating tangent planes</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#isometric-feature-map-isomap"><i class="fa fa-check"></i><b>6.3</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#key-definitions"><i class="fa fa-check"></i><b>6.3.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.3.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm"><i class="fa fa-check"></i><b>6.3.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.4" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.3.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#locally-linear-embeddings-lles"><i class="fa fa-check"></i><b>6.4</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction-1"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm-1"><i class="fa fa-check"></i><b>6.4.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.4.3" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.4.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#laplacian-eigenmap"><i class="fa fa-check"></i><b>6.5</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#algorithm-2"><i class="fa fa-check"></i><b>6.5.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#hessian-eigenmaps-hlles"><i class="fa fa-check"></i><b>6.6</b> Hessian Eigenmaps (HLLEs)</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#introduction-2"><i class="fa fa-check"></i><b>6.6.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#comparison-of-manifold-learning-methods"><i class="fa fa-check"></i><b>6.7</b> Comparison of Manifold Learning Methods</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#visual-comparison-of-results"><i class="fa fa-check"></i><b>6.7.1</b> Visual comparison of results</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html#exercises-4"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ch-clustering.html"><a href="ch-clustering.html#center-based-clustering"><i class="fa fa-check"></i><b>7.1</b> Center-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ch-clustering.html"><a href="ch-clustering.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="ch-clustering.html"><a href="ch-clustering.html#k-center-and-k-medoids"><i class="fa fa-check"></i><b>7.1.2</b> <span class="math inline">\(k\)</span>-center and <span class="math inline">\(k\)</span>-medoids</a></li>
<li class="chapter" data-level="7.1.3" data-path="ch-clustering.html"><a href="ch-clustering.html#minimizing-clustering-loss-functions"><i class="fa fa-check"></i><b>7.1.3</b> Minimizing clustering loss functions</a></li>
<li class="chapter" data-level="7.1.4" data-path="ch-clustering.html"><a href="ch-clustering.html#strengths-and-weaknesses"><i class="fa fa-check"></i><b>7.1.4</b> Strengths and Weaknesses</a></li>
<li class="chapter" data-level="7.1.5" data-path="ch-clustering.html"><a href="ch-clustering.html#choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>7.1.5</b> Choosing the number of clusters</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ch-clustering.html"><a href="ch-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>7.2</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ch-clustering.html"><a href="ch-clustering.html#dendrograms"><i class="fa fa-check"></i><b>7.2.1</b> Dendrograms</a></li>
<li class="chapter" data-level="7.2.2" data-path="ch-clustering.html"><a href="ch-clustering.html#building-a-dendrogram"><i class="fa fa-check"></i><b>7.2.2</b> Building a dendrogram</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ch-clustering.html"><a href="ch-clustering.html#sec-model-based-clustering"><i class="fa fa-check"></i><b>7.3</b> Model-Based Clustering</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ch-clustering.html"><a href="ch-clustering.html#gaussian-mixture-models-gmm"><i class="fa fa-check"></i><b>7.3.1</b> Gaussian Mixture Models (GMM)</a></li>
<li class="chapter" data-level="7.3.2" data-path="ch-clustering.html"><a href="ch-clustering.html#expectation-maximization-em-algorithm"><i class="fa fa-check"></i><b>7.3.2</b> Expectation-Maximization (EM) Algorithm</a></li>
<li class="chapter" data-level="7.3.3" data-path="ch-clustering.html"><a href="ch-clustering.html#model-selection-and-parameter-constraints"><i class="fa fa-check"></i><b>7.3.3</b> Model Selection and Parameter Constraints</a></li>
<li class="chapter" data-level="7.3.4" data-path="ch-clustering.html"><a href="ch-clustering.html#weakneses-and-limitations"><i class="fa fa-check"></i><b>7.3.4</b> Weakneses and Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch-clustering.html"><a href="ch-clustering.html#sec-spec-clustering"><i class="fa fa-check"></i><b>7.4</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch-clustering.html"><a href="ch-clustering.html#introduction-3"><i class="fa fa-check"></i><b>7.4.1</b> Introduction</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch-clustering.html"><a href="ch-clustering.html#algorithm-3"><i class="fa fa-check"></i><b>7.4.2</b> Algorithm</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">An Introduction to Unsupervised Learning</h1>
<p class="author"><em>Alex Young and Cenhao Zhu</em></p>
<p class="date"><em>Version 0</em></p>
</div>
<div id="intro" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction<a href="index.html#intro" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Unsupervised machine learning (UL) is an overarching term for methods designed to understand the patterns and relationships within a set of <strong>unlabeled data</strong>. UL is often discussed in contrast to (semi-)supervised learning. In the latter setting(s), one is primarly concerned with prediction and classification, and the machine learning algorithms therein focus on learning the relationship between a (often high dimensional) feature vector <span class="math inline">\(\vec{x}\)</span> and an observable outcome <span class="math inline">\(y\)</span> by training on a labeled dataset <span class="math inline">\(\{(\vec{x}_i,y_i)\}_{i=1}^N\)</span>. As a concrete example, consider the MNIST dataset which contains a compendium of labeled digitized grayscale images of handwritten digits <span class="citation">[<a href="#ref-mnist">1</a>]</span>. The images themselves are the features, (<span class="math inline">\(\vec{x}\)</span>), and the identity of the digit (0 to 9) gives the label, <span class="math inline">\(y\)</span> . A supervised learning algorithm trained on the MNIST dataset would be able to classify a handwritten digit in a new grayscale image.</p>
<p>Prediction and classification are clearly defined goals which naturally translate to many settings. As such, supervised ML has found numerous applications across diverse fields of research from healthcare and medicine to astronomy and chemistry. Given the clearly translatable goals of supervised learning, most texts on Machine Learning tend to emphasize this setting with much smaller discussion on the un- or semi-supervised setting. For example, both <em>The Elements of Statistical Learning</em> by Hastie et al <span class="citation">[<a href="#ref-esl">2</a>]</span> and <em>Modern Multivariate Statistical Techniques</em> by Izenman <span class="citation">[<a href="#ref-izenman">3</a>]</span> are wonderful texts – which were central to the early development of this book – but lean towards supervised problems.</p>
<p>Unlike the supervised setting, however, UL algorithms are applied to datasets without (or ignoring) labels. In contrast to the MNIST example above, you can think of have of a case where one has access to a large collection of <span class="math inline">\(\vec{x}_i\)</span>, such as images, without any labels indicating the content(s) of the image. Other examples include,</p>
<ul>
<li>a corpus of emails without any indication of which, if any, are spam</li>
<li>genomic data for each individual in a large population of cancer patients</li>
<li>collections of consumer data or ratings</li>
</ul>
<p>Without labels, it may be difficult (particularly to students first seeing this branch of machine learning) to grasp the usefulness of UL including its applicability and what one is <em>learning</em> in practice. In this book, we hope to address this difficulty and provide readers with a clear understanding of UL by covering motivating ideas, fundamental techniques, and clear and compelling applications. For now, we’ll discuss the high-altitude view of unsupervised learning.</p>
<p>We’ll focus on those cases where we have a collection of independent observations of (preprocessed) features stored in vectors <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\in\mathbb{R}^d\)</span>. This setting will be formalized in <a href="ch-prob.html#ch-prob">2</a>. Broadly, UL learns patterns and similarities between the vectors which could allow us to</p>
<ol style="list-style-type: decimal">
<li>find subsets of the data which more similar to each other (clustering) or</li>
<li>find simpler representations of the data which preserves important relationships (dimension reduction)</li>
<li>identify common relationships between variables in the data (association rules)</li>
</ol>
<p>Each of three cases provide a simplified lens through which we can view our data, and in doing so, can open up a number of interesting possibilities.</p>
<p>Clustering different genomic data in cancer patients could provide information to medical practitioners on which cancers exhibit common genetics signatures. Applying dimension reduction to spam emails could allow one to identify odd emails which might be spam (anomaly detection). Learning the common relationships between variables in a consumer data set opens the possibility of matching consumers which items they might enjoy (recommendation systems).</p>
<p>The examples above are by no means exhaustive, but they do raise a few critical points.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Where</strong> Each application above is one step in a larger data science problem. Unsupervised learning is rarely detached from a broader data science pipeline. This is a stark difference from classification and prediction which are often viewed as isolated statistical problems (though careful practitioners recognize that data collection and cleaning and the communication of results are often of equal or greater importance than analysis). Examples are provided throughout the text demonstrating where UL can be useful.</p></li>
<li><p><strong>What</strong> One data set could be approached from one or more different perspectives. For example, one could apply dimension reduction to the cancer data to <em>visualize</em> the potentially complex data in a manner that preserves important relationships. Combining many approaches together makes unsupervised learning a powerful tool to exploratory data analysis and featurization, particularly when combined with expert level content knowledge. Choosing a UL method is linked with what we hope to learn about our data.</p></li>
<li><p><strong>How</strong> If we want an algorithm that clusters similar vectors or provides a visualization that keeps close points together, then we should be mindful of the meaning of similarity or proximity. UL algorithms, sometimes implicitly, prioritize different relationships. We explore how these algorithms work from a geometric perspective which is a helpful intellectual scaffolding.</p></li>
</ol>
<p>In the remainder of this text, we focus primarily on dimension reduction (<a href="ch-linear.html#ch-linear">4</a> and <a href="ch-nonlinear.html#ch-nonlinear">6</a>) and clustering (<a href="ch-clustering.html#ch-clustering">7</a>).</p>
<div id="prerequisites" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Prerequisites<a href="index.html#prerequisites" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This text is targeted at upper level undergraduates with a well rounded background in the following courses and topics</p>
<ol style="list-style-type: decimal">
<li>Probability: random variables, expectation, variance, and covariance</li>
<li>Linear algebra: matrix-vector multiplication, linear spaces, eigendecompositions</li>
<li>Multivariable calculus: gradients and basic optimization</li>
</ol>
<p>A brief review of the most important ideas is covered in Chapter <a href="ch-prob.html#ch-prob">2</a>. Additional tools and techniques needed for specific algorithms are covered at a cursory level as needed. References to more thorough discussions are provided throughout for the interested reader.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-mnist" class="csl-entry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Deng</span>, L. (2012). The mnist database of handwritten digit images for machine learning research. <em>IEEE Signal Processing Magazine</em> <strong>29</strong> 141–2.</div>
</div>
<div id="ref-esl" class="csl-entry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline"><span class="smallcaps">Hastie</span>, T., <span class="smallcaps">Tibshirani</span>, R. and <span class="smallcaps">Friedman</span>, J. (2001). <em>The elements of statistical learning</em>. Springer New York Inc., New York, NY, USA.</div>
</div>
<div id="ref-izenman" class="csl-entry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline"><span class="smallcaps">Izenman</span>, A. J. (2008). <em>Modern multivariate statistical techniques: Regression, classification, and manifold learning</em>. Springer Publishing Company, Incorporated.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="ch-prob.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/young1062/introULindex.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
